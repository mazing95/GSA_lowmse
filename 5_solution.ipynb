{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2521599-8fe9-4bf0-9410-40ab8b9a0b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------->Start Estimation....\n",
      "loading train file...\n",
      "Creating all features (FIXED - no data leakage)...\n",
      "Creating opening/closing difference features...\n",
      "Creating log features...\n",
      "Creating expanding mean difference (FIXED)...\n",
      "Creating time-based rolling features (FIXED)...\n",
      "Creating intraday EWMA features...\n",
      "Creating lag features...\n",
      "Creating RSI features...\n",
      "Creating EWMA on time means...\n",
      "Creating XY combined features (FIXED)...\n",
      "Creating YX spread features (FIXED)...\n",
      "Feature engineering completed (NO DATA LEAKAGE)!\n",
      "----->Normalization....\n",
      "----->Validation....\n",
      "     train_50_percent  train_60_percent  train_70_percent  train_80_percent  \\\n",
      "mse          0.006176          0.003709          0.003094          0.002837   \n",
      "r2          79.553134         82.748923         81.979486         83.755175   \n",
      "\n",
      "     train_90_percent  min_stats  max_stats        avg  \n",
      "mse          0.003857   0.002837   0.006176   0.004098  \n",
      "r2          81.263896  79.553131  83.755173  81.801277  \n",
      "----->Fitting....\n",
      "----->Calculation feature importances...\n",
      "yprice_time_mean_10min                   28.59%           -0.21\n",
      "yprice_dayly_ewma_10min                  19.82%            0.15\n",
      "yprice_time_mean_10min_dayly_ewma_4hour  17.94%            0.13\n",
      "yprice_ewma_difpair_10min_4hour          9.76%          -0.073\n",
      "yprice_time_mean_1hour                   4.67%          -0.035\n",
      "yprice_time_zscore_1hour                 3.83%          -0.029\n",
      "xy_square_time_zscore_10min              3.59%           0.027\n",
      "yprice_time_mean_10min_rsi_1hour         2.48%          -0.019\n",
      "xprice_time_mean_10min_dayly_ewma_10min  2.21%          -0.017\n",
      "yprice_time_mean_2hours                  1.65%           0.012\n",
      "xprice_dayly_ewma_4hour                  1.51%          -0.011\n",
      "yprice_time_zscore_2hours                1.29%          0.0097\n",
      "xprice_lag_4hours                        0.73%         -0.0054\n",
      "xlog                                     0.55%          0.0041\n",
      "xy_geom_time_mean_2hours_dayly_ewma_20min 0.46%          0.0034\n",
      "ylog_dayly_ewma_1hour                    0.24%          0.0018\n",
      "yx_spread_time_mean_10min_lag_1hour      0.13%        -0.00094\n",
      "xy_garmonic_time_std_4hours              0.11%        -0.00085\n",
      "yx_spread_time_zscore_4hours             0.08%         0.00063\n",
      "xprice_time_mean_1hour_lag_4hours        0.08%        -0.00058\n",
      "yprice_expanding_mean_diff               0.08%        -0.00057\n",
      "xlog_dayly_ewma_10min                    0.07%        -0.00051\n",
      "yx_spread_ewma_prodpair_1hour_10min      0.05%         0.00035\n",
      "yprice_time_mean_10min_lag_10min         0.04%         0.00033\n",
      "xdiff_from_opening                       0.03%        -0.00026\n",
      "yprice_time_mean_1hour_lag_1workweek     0.01%          0.0001\n",
      "--------->Starting Forecasting....\n",
      "loading train file...\n",
      "loading test file...\n",
      "Creating all features (FIXED - no data leakage)...\n",
      "Creating opening/closing difference features...\n",
      "Creating log features...\n",
      "Creating expanding mean difference (FIXED)...\n",
      "Creating time-based rolling features (FIXED)...\n",
      "Creating intraday EWMA features...\n",
      "Creating lag features...\n",
      "Creating RSI features...\n",
      "Creating EWMA on time means...\n",
      "Creating XY combined features (FIXED)...\n",
      "Creating YX spread features (FIXED)...\n",
      "Feature engineering completed (NO DATA LEAKAGE)!\n",
      "----->Normalization....\n",
      "----->Prediction....\n",
      "----->Evaluation....\n",
      "Test MSE: 0.003064\n",
      "Test R-squared: 0.819881\n",
      "Test R-squared (x100): 81.99\n",
      "\n",
      "First 10 predictions: [0.10498988 0.12215311 0.12198458 0.1214245  0.10266273 0.10150809\n",
      " 0.10043223 0.06352505 0.06262206 0.06143697]\n",
      "Total predictions: 35000\n",
      "\n",
      "Evaluation Metrics:\n",
      "MSE: 0.003064\n",
      "R-squared: 0.819881\n",
      "R-squared (x100): 81.99\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from copy import copy\n",
    "import scipy\n",
    "import pickle\n",
    "\n",
    "# %% Configuration\n",
    "# Configuration parameters\n",
    "droprows = 7050\n",
    "std_reg_const = 0.1\n",
    "normalization_std_reg = 0.0001\n",
    "ridge_alpha = 1333\n",
    "\n",
    "selected_cols = [\n",
    "    # Opening/Closing difference features\n",
    "    'xdiff_from_opening',\n",
    "    \n",
    "    # X-price features\n",
    "    'xlog',\n",
    "    'xlog_dayly_ewma_10min',\n",
    "    'xprice_dayly_ewma_4hour',\n",
    "    'xprice_lag_4hours',\n",
    "    'xprice_time_mean_1hour_lag_4hours',\n",
    "    'xprice_time_mean_10min_dayly_ewma_10min',\n",
    "    \n",
    "    # Y-price features\n",
    "    'ylog_dayly_ewma_1hour',\n",
    "    'yprice_dayly_ewma_10min',\n",
    "    'yprice_ewma_difpair_10min_4hour',\n",
    "    'yprice_expanding_mean_diff',  # FIXED: renamed from yprice_full_history_diff\n",
    "    'yprice_time_mean_1hour',\n",
    "    'yprice_time_mean_1hour_lag_1workweek',\n",
    "    'yprice_time_mean_10min',\n",
    "    'yprice_time_mean_10min_dayly_ewma_4hour',\n",
    "    'yprice_time_mean_10min_lag_10min',\n",
    "    'yprice_time_mean_10min_rsi_1hour',\n",
    "    'yprice_time_mean_2hours',\n",
    "    'yprice_time_zscore_1hour',\n",
    "    'yprice_time_zscore_2hours',\n",
    "    \n",
    "    # XY combined features\n",
    "    'xy_garmonic_time_std_4hours',\n",
    "    'xy_geom_time_mean_2hours_dayly_ewma_20min',\n",
    "    'xy_square_time_zscore_10min',\n",
    "    \n",
    "    # YX spread features\n",
    "    'yx_spread_ewma_prodpair_1hour_10min',\n",
    "    'yx_spread_time_mean_10min_lag_1hour',\n",
    "    'yx_spread_time_zscore_4hours',\n",
    "]\n",
    "\n",
    "\n",
    "# %% Helper Functions\n",
    "def print_importances(model, selected_cols):\n",
    "    \"\"\"Print feature importances sorted by absolute weight\"\"\"\n",
    "    weigts_sum = sum(map(abs, model.coef_))\n",
    "    for name, weight in sorted(zip(selected_cols, model.coef_), key=lambda x: -abs(x[1])):\n",
    "        percent_weight = abs(weight) / weigts_sum\n",
    "        print('{:40} {:.2%} {:15.2}'.format(name, percent_weight, weight))\n",
    "\n",
    "def rsquared(x, y):\n",
    "    \"\"\"Return R^2 where x and y are array-like.\"\"\"\n",
    "    slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(x, y)\n",
    "    return r_value**2\n",
    "\n",
    "def rsiFunc(prices, n=14):\n",
    "    \"\"\"Calculate RSI (Relative Strength Index)\"\"\"\n",
    "    deltas = np.diff(prices)\n",
    "    seed = deltas[:n+1]\n",
    "    up = seed[seed>=0].sum()/n\n",
    "    down = -seed[seed<0].sum()/n\n",
    "    rs = up/down\n",
    "    rsi = np.zeros_like(prices)\n",
    "    rsi[:n] = 100. - 100./(1.+rs)\n",
    "\n",
    "    for i in range(n, len(prices)):\n",
    "        delta = deltas[i-1]\n",
    "        if delta>0:\n",
    "            upval = delta\n",
    "            downval = 0.\n",
    "        else:\n",
    "            upval = 0.\n",
    "            downval = -delta\n",
    "\n",
    "        up = (up*(n-1) + upval)/n\n",
    "        down = (down*(n-1) + downval)/n\n",
    "        rs = up/down\n",
    "        rsi[i] = 100. - 100./(1.+rs)\n",
    "    return rsi\n",
    "\n",
    "# %% Time Series Validation Functions\n",
    "def time_split(data, valid_ratio, test_ratio):\n",
    "    \"\"\"Split time series data into train, validation, and test sets\"\"\"\n",
    "    n_valid = max(1, int(data.shape[0] * valid_ratio))\n",
    "    n_test = max(1, int(data.shape[0] * test_ratio))\n",
    "    n_train = data.shape[0] - n_valid - n_test\n",
    "    \n",
    "    train = data.iloc[:n_train].reset_index(drop=True).copy()\n",
    "    valid = data.iloc[n_train:-n_test].reset_index(drop=True).copy()\n",
    "    test = data.iloc[-n_test:].reset_index(drop=True).copy()\n",
    "    merged_test = pd.concat([valid, test], ignore_index=True)\n",
    "    return train, valid, test\n",
    "\n",
    "def validate_model_by_pentate(model, source_data, base_cols, droprows=0):\n",
    "    \"\"\"Validate model using 5-fold time series cross-validation\"\"\"\n",
    "    df = source_data.copy()\n",
    "    selected_cols = base_cols.copy()\n",
    "    helper_cols = list(set(selected_cols + ['periods_before_closing', 'returns']))\n",
    "    metrics_dict = {}\n",
    "    \n",
    "    for step in range(5, 10):\n",
    "        n_train = int(df.shape[0] * step // 10)\n",
    "        n_test = int(df.shape[0] * (step + 1) // 10)\n",
    "        train = df.iloc[:n_train].reset_index(drop=True).copy()\n",
    "        test = df.iloc[n_train:n_test].reset_index(drop=True).copy()\n",
    "        train.drop(np.arange(droprows), inplace=True)\n",
    "        train.dropna(inplace=True)\n",
    "\n",
    "        model.fit(train[selected_cols], train.returns)\n",
    "        predicted = model.predict(test[selected_cols])\n",
    "        predicted[test.periods_before_closing == 0] = 0\n",
    "\n",
    "        current_mse = mean_squared_error(test.returns, predicted)\n",
    "        current_r2 = rsquared(test.returns, predicted) * 100\n",
    "        metrics_dict['train_{}_percent'.format(step * 10)] = {\n",
    "            'mse': current_mse,\n",
    "            'r2': current_r2\n",
    "        }\n",
    "    \n",
    "    report = pd.DataFrame(metrics_dict)\n",
    "    report['min_stats'] = report.iloc[:,:5].min(1).astype(np.float32)\n",
    "    report['max_stats'] = report.iloc[:,:5].max(1).astype(np.float32)\n",
    "    report['avg'] = report.mean(1).astype(np.float32)\n",
    "    return report\n",
    "\n",
    "# %% FIXED Feature Engineering Function\n",
    "def create_all_features(data):\n",
    "    \"\"\"\n",
    "    FIXED: Feature engineering without data leakage.\n",
    "    All features only use data available up to time t to predict returns at time t.\n",
    "    \"\"\"\n",
    "    print(\"Creating all features (FIXED - no data leakage)...\")\n",
    "    \n",
    "    # Pre-calculate commonly used values\n",
    "    days = data.day.unique()\n",
    "    \n",
    "    # Opening/Closing differences\n",
    "    print(\"Creating opening/closing difference features...\")\n",
    "    close_price_per_day_y = data.groupby('day').timestamp.max().shift(1).map(\n",
    "        data[['timestamp', 'yprice']].set_index('timestamp').yprice)\n",
    "    data['ydiff_from_closing'] = (data.yprice - data.day.map(close_price_per_day_y)).fillna(0)\n",
    "    \n",
    "    close_price_per_day_x = data.groupby('day').timestamp.max().shift(1).map(\n",
    "        data[['timestamp', 'xprice']].set_index('timestamp').xprice)\n",
    "    data['xdiff_from_closing'] = (data.xprice - data.day.map(close_price_per_day_x)).fillna(0)\n",
    "    \n",
    "    open_price_per_day_x = data.groupby('day').timestamp.min().map(\n",
    "        data[['timestamp', 'xprice']].set_index('timestamp').xprice)\n",
    "    data['xdiff_from_opening'] = data.xprice - data.day.map(open_price_per_day_x)\n",
    "    \n",
    "    # Log features\n",
    "    print(\"Creating log features...\")\n",
    "    data['xlog'] = data.xprice.apply(np.log1p)\n",
    "    data['ylog'] = data.yprice.apply(np.log1p)\n",
    "    \n",
    "    # FIXED: Expanding mean that only uses past data\n",
    "    print(\"Creating expanding mean difference (FIXED)...\")\n",
    "    data['yprice_expanding_mean_diff'] = data['yprice'] - data['yprice'].expanding(min_periods=1).mean()\n",
    "    \n",
    "    # FIXED: Time-based rolling features with backward-looking windows\n",
    "    print(\"Creating time-based rolling features (FIXED)...\")\n",
    "    time_windows = {\n",
    "        6: '1min', 60: '10min', 360: '1hour', 720: '2hours', 1410: '4hours', 2820: '1workweek'\n",
    "    }\n",
    "    \n",
    "    # FIXED: Create all rolling means with backward-looking windows\n",
    "    for window, name in time_windows.items():\n",
    "        if window in [6, 60, 360, 1410]:  # xprice windows\n",
    "            period_repr = '{}s'.format(window * 10)\n",
    "            # FIXED: Use label='left' and closed='left' for backward-looking windows\n",
    "            agg_helper_df = data['xprice'].resample(\n",
    "                period_repr, label='left', closed='left').agg('mean')\n",
    "            # Aggregate with multiple offsets to get more granular averages\n",
    "            for shift in range(10, window * 10, 10):\n",
    "                shifted_df = data['xprice'].resample(\n",
    "                    period_repr, label='left', closed='left', offset='{}s'.format(shift)).agg('mean')\n",
    "                agg_helper_df = pd.concat([agg_helper_df, shifted_df])\n",
    "            colname = f'xprice_time_mean_{name}'\n",
    "            # Forward fill to propagate past values forward\n",
    "            agg_helper_df = agg_helper_df.sort_index()\n",
    "            data[colname] = agg_helper_df.reindex(data.index).ffill()\n",
    "            data[colname] = data['xprice'] - data[colname]\n",
    "        \n",
    "        if window in [60, 360, 720]:  # yprice windows\n",
    "            period_repr = '{}s'.format(window * 10)\n",
    "            # FIXED: Use label='left' and closed='left' for backward-looking windows\n",
    "            agg_helper_df = data['yprice'].resample(\n",
    "                period_repr, label='left', closed='left').agg('mean')\n",
    "            for shift in range(10, window * 10, 10):\n",
    "                shifted_df = data['yprice'].resample(\n",
    "                    period_repr, label='left', closed='left', offset='{}s'.format(shift)).agg('mean')\n",
    "                agg_helper_df = pd.concat([agg_helper_df, shifted_df])\n",
    "            colname = f'yprice_time_mean_{name}'\n",
    "            agg_helper_df = agg_helper_df.sort_index()\n",
    "            data[colname] = agg_helper_df.reindex(data.index).ffill()\n",
    "            data[colname] = data['yprice'] - data[colname]\n",
    "    \n",
    "    # FIXED: Create rolling std for yprice with backward-looking windows\n",
    "    for window in [360, 720]:\n",
    "        name = time_windows[window]\n",
    "        period_repr = '{}s'.format(window * 10)\n",
    "        agg_helper_df = data['yprice'].resample(\n",
    "            period_repr, label='left', closed='left').agg('std')\n",
    "        for shift in range(10, window * 10, 10):\n",
    "            shifted_df = data['yprice'].resample(\n",
    "                period_repr, label='left', closed='left', offset='{}s'.format(shift)).agg('std')\n",
    "            agg_helper_df = pd.concat([agg_helper_df, shifted_df])\n",
    "        colname = f'yprice_time_std_{name}'\n",
    "        agg_helper_df = agg_helper_df.sort_index()\n",
    "        data[colname] = agg_helper_df.reindex(data.index).ffill().fillna(0) + std_reg_const\n",
    "    \n",
    "    # Z-scores for yprice\n",
    "    data['yprice_time_zscore_1hour'] = data.yprice_time_mean_1hour / data.yprice_time_std_1hour\n",
    "    data['yprice_time_zscore_2hours'] = data.yprice_time_mean_2hours / data.yprice_time_std_2hours\n",
    "    \n",
    "    # Intraday EWMA features (backward-looking by nature)\n",
    "    print(\"Creating intraday EWMA features...\")\n",
    "    ewma_configs = [\n",
    "        ('xprice', [24], ['4hour']),\n",
    "        ('xlog', [60], ['10min']),\n",
    "        ('ylog', [360], ['1hour']),\n",
    "        ('yprice', [24, 60], ['4hour', '10min'])\n",
    "    ]\n",
    "    \n",
    "    for col, windows, names in ewma_configs:\n",
    "        for day in days:\n",
    "            df_mask = (data.day == day)\n",
    "            for window, name in zip(windows, names):\n",
    "                colname = f'{col}_dayly_ewma_{name}'\n",
    "                # EWMA is backward-looking by default\n",
    "                ewm = pd.Series.ewm(data.loc[df_mask, col], halflife=window).mean().values\n",
    "                data.loc[df_mask, colname] = ewm\n",
    "                if col in ['xprice', 'yprice']:\n",
    "                    data.loc[df_mask, colname] = data.loc[df_mask, col] - data.loc[df_mask, colname]\n",
    "                elif col in ['xlog', 'ylog']:\n",
    "                    data.loc[df_mask, colname] = data.loc[df_mask, col.replace('log', 'price')] - data.loc[df_mask, colname]\n",
    "    \n",
    "    # EWMA difference pair\n",
    "    data['yprice_ewma_difpair_10min_4hour'] = data.yprice_dayly_ewma_10min - data.yprice_dayly_ewma_4hour\n",
    "    \n",
    "    # Lag features (inherently backward-looking)\n",
    "    print(\"Creating lag features...\")\n",
    "    lag_configs = [\n",
    "        ('xprice', 1410, '4hours'),\n",
    "        ('yprice', 1410, '4hours'),\n",
    "        ('xprice_time_mean_1hour', 1410, '4hours'),\n",
    "        ('yprice_time_mean_1hour', 1410, '4hours'),\n",
    "        ('yprice_time_mean_1hour', 2820, '1workweek'),\n",
    "        ('yprice_time_mean_10min', 60, '10min')\n",
    "    ]\n",
    "    \n",
    "    for col, lag, name in lag_configs:\n",
    "        data[f'{col}_lag_{name}'] = data[col].shift(lag).values\n",
    "    \n",
    "    # RSI features (calculated sequentially, backward-looking)\n",
    "    print(\"Creating RSI features...\")\n",
    "    data['xprice_time_mean_1min_rsi_1min'] = rsiFunc(data['xprice_time_mean_1min'].values, 6)\n",
    "    data['yprice_time_mean_10min_rsi_1hour'] = rsiFunc(data['yprice_time_mean_10min'].values, 360)\n",
    "    \n",
    "    # Additional EWMA on time means\n",
    "    print(\"Creating EWMA on time means...\")\n",
    "    ewma_on_means = [\n",
    "        ('xprice_time_mean_10min', 60, '10min'),\n",
    "        ('yprice_time_mean_10min', 24, '4hour')\n",
    "    ]\n",
    "    \n",
    "    for col, window, name in ewma_on_means:\n",
    "        for day in days:\n",
    "            df_mask = (data.day == day)\n",
    "            colname = f'{col}_dayly_ewma_{name}'\n",
    "            ewm = pd.Series.ewm(data.loc[df_mask, col], halflife=window).mean().values\n",
    "            data.loc[df_mask, colname] = ewm\n",
    "    \n",
    "    # FIXED: XY Combined features with backward-looking windows\n",
    "    print(\"Creating XY combined features (FIXED)...\")\n",
    "    \n",
    "    # XY Harmonic std\n",
    "    agg_helper_df = data['xy_garmonic'].resample(\n",
    "        '14100s', label='left', closed='left').agg('std')\n",
    "    for shift in range(10, 14100, 10):\n",
    "        shifted_df = data['xy_garmonic'].resample(\n",
    "            '14100s', label='left', closed='left', offset='{}s'.format(shift)).agg('std')\n",
    "        agg_helper_df = pd.concat([agg_helper_df, shifted_df])\n",
    "    agg_helper_df = agg_helper_df.sort_index()\n",
    "    data['xy_garmonic_time_std_4hours'] = agg_helper_df.reindex(data.index).ffill().fillna(0) + std_reg_const\n",
    "    \n",
    "    # Harmonic EWMA\n",
    "    for day in days:\n",
    "        df_mask = (data.day == day)\n",
    "        data.loc[df_mask, 'xy_garmonic_dayly_ewma_1hour'] = pd.Series.ewm(\n",
    "            data.loc[df_mask, 'xy_garmonic'], halflife=360).mean().values\n",
    "        data.loc[df_mask, 'xy_garmonic_dayly_ewma_2hours'] = pd.Series.ewm(\n",
    "            data.loc[df_mask, 'xy_garmonic'], halflife=720).mean().values\n",
    "    \n",
    "    data['xy_garmonic_dayly_ewma_1hour'] = data['xy_garmonic'] - data['xy_garmonic_dayly_ewma_1hour']\n",
    "    data['xy_garmonic_dayly_ewma_2hours'] = data['xy_garmonic'] - data['xy_garmonic_dayly_ewma_2hours']\n",
    "    data['xy_garmonic_ewma_prodpair_2hours_1hour'] = (\n",
    "        data.xy_garmonic_dayly_ewma_2hours * data.xy_garmonic_dayly_ewma_1hour)\n",
    "    \n",
    "    # FIXED: XY Geometric features with backward-looking windows\n",
    "    for window, name in [(6, '1min'), (60, '10min'), (360, '1hour'), (720, '2hours')]:\n",
    "        period_repr = '{}s'.format(window * 10)\n",
    "        agg_helper_df = data['xy_geom'].resample(\n",
    "            period_repr, label='left', closed='left').agg('mean')\n",
    "        for shift in range(10, window * 10, 10):\n",
    "            shifted_df = data['xy_geom'].resample(\n",
    "                period_repr, label='left', closed='left', offset='{}s'.format(shift)).agg('mean')\n",
    "            agg_helper_df = pd.concat([agg_helper_df, shifted_df])\n",
    "        colname = f'xy_geom_time_mean_{name}'\n",
    "        agg_helper_df = agg_helper_df.sort_index()\n",
    "        data[colname] = agg_helper_df.reindex(data.index).ffill()\n",
    "        data[colname] = data['xy_geom'] - data[colname]\n",
    "    \n",
    "    # Geometric lags and EWMA\n",
    "    data['xy_geom_time_mean_1hour_lag_20min'] = data['xy_geom_time_mean_1hour'].shift(120).values\n",
    "    \n",
    "    for day in days:\n",
    "        df_mask = (data.day == day)\n",
    "        data.loc[df_mask, 'xy_geom_time_mean_10min_dayly_ewma_1min'] = pd.Series.ewm(\n",
    "            data.loc[df_mask, 'xy_geom_time_mean_10min'], halflife=6).mean().values\n",
    "        data.loc[df_mask, 'xy_geom_time_mean_2hours_dayly_ewma_20min'] = pd.Series.ewm(\n",
    "            data.loc[df_mask, 'xy_geom_time_mean_2hours'], halflife=120).mean().values\n",
    "    \n",
    "    # FIXED: XY Relation std with backward-looking windows\n",
    "    for window, name in [(360, '1hour'), (720, '2hours')]:\n",
    "        period_repr = '{}s'.format(window * 10)\n",
    "        agg_helper_df = data['xy_relation'].resample(\n",
    "            period_repr, label='left', closed='left').agg('std')\n",
    "        for shift in range(10, window * 10, 10):\n",
    "            shifted_df = data['xy_relation'].resample(\n",
    "                period_repr, label='left', closed='left', offset='{}s'.format(shift)).agg('std')\n",
    "            agg_helper_df = pd.concat([agg_helper_df, shifted_df])\n",
    "        colname = f'xy_relation_time_std_{name}'\n",
    "        agg_helper_df = agg_helper_df.sort_index()\n",
    "        data[colname] = agg_helper_df.reindex(data.index).ffill().fillna(0) + std_reg_const\n",
    "    \n",
    "    # FIXED: XY Square zscore with backward-looking windows\n",
    "    agg_helper_df_mean = data['xy_square'].resample(\n",
    "        '600s', label='left', closed='left').agg('mean')\n",
    "    agg_helper_df_std = data['xy_square'].resample(\n",
    "        '600s', label='left', closed='left').agg('std')\n",
    "    \n",
    "    for shift in range(10, 600, 10):\n",
    "        shifted_mean = data['xy_square'].resample(\n",
    "            '600s', label='left', closed='left', offset='{}s'.format(shift)).agg('mean')\n",
    "        shifted_std = data['xy_square'].resample(\n",
    "            '600s', label='left', closed='left', offset='{}s'.format(shift)).agg('std')\n",
    "        agg_helper_df_mean = pd.concat([agg_helper_df_mean, shifted_mean])\n",
    "        agg_helper_df_std = pd.concat([agg_helper_df_std, shifted_std])\n",
    "    \n",
    "    agg_helper_df_mean = agg_helper_df_mean.sort_index()\n",
    "    agg_helper_df_std = agg_helper_df_std.sort_index()\n",
    "    data['xy_square_time_mean_10min'] = data['xy_square'] - agg_helper_df_mean.reindex(data.index).ffill()\n",
    "    data['xy_square_time_std_10min'] = agg_helper_df_std.reindex(data.index).ffill().fillna(0) + std_reg_const\n",
    "    data['xy_square_time_zscore_10min'] = data['xy_square_time_mean_10min'] / data['xy_square_time_std_10min']\n",
    "    \n",
    "    # FIXED: YX Spread features with backward-looking windows\n",
    "    print(\"Creating YX spread features (FIXED)...\")\n",
    "    for window, name in [(60, '10min'), (720, '2hours'), (1410, '4hours')]:\n",
    "        period_repr = '{}s'.format(window * 10)\n",
    "        agg_helper_df = data['yx_spread'].resample(\n",
    "            period_repr, label='left', closed='left').agg('mean')\n",
    "        for shift in range(10, window * 10, 10):\n",
    "            shifted_df = data['yx_spread'].resample(\n",
    "                period_repr, label='left', closed='left', offset='{}s'.format(shift)).agg('mean')\n",
    "            agg_helper_df = pd.concat([agg_helper_df, shifted_df])\n",
    "        colname = f'yx_spread_time_mean_{name}'\n",
    "        agg_helper_df = agg_helper_df.sort_index()\n",
    "        data[colname] = agg_helper_df.reindex(data.index).ffill()\n",
    "        data[colname] = data['yx_spread'] - data[colname]\n",
    "    \n",
    "    # FIXED: Spread std and zscore with backward-looking windows\n",
    "    agg_helper_df = data['yx_spread'].resample(\n",
    "        '14100s', label='left', closed='left').agg('std')\n",
    "    for shift in range(10, 14100, 10):\n",
    "        shifted_df = data['yx_spread'].resample(\n",
    "            '14100s', label='left', closed='left', offset='{}s'.format(shift)).agg('std')\n",
    "        agg_helper_df = pd.concat([agg_helper_df, shifted_df])\n",
    "    agg_helper_df = agg_helper_df.sort_index()\n",
    "    data['yx_spread_time_std_4hours'] = agg_helper_df.reindex(data.index).ffill().fillna(0) + std_reg_const\n",
    "    data['yx_spread_time_zscore_4hours'] = data['yx_spread_time_mean_4hours'] / data['yx_spread_time_std_4hours']\n",
    "    \n",
    "    # Spread lags (inherently backward-looking)\n",
    "    data['yx_spread_time_mean_10min_lag_1hour'] = data['yx_spread_time_mean_10min'].shift(360).values\n",
    "    data['yx_spread_time_mean_2hours_lag_20min'] = data['yx_spread_time_mean_2hours'].shift(120).values\n",
    "    \n",
    "    # Spread EWMA (backward-looking by nature)\n",
    "    for day in days:\n",
    "        df_mask = (data.day == day)\n",
    "        data.loc[df_mask, 'yx_spread_dayly_ewma_10min'] = pd.Series.ewm(\n",
    "            data.loc[df_mask, 'yx_spread'], halflife=60).mean().values\n",
    "        data.loc[df_mask, 'yx_spread_dayly_ewma_1hour'] = pd.Series.ewm(\n",
    "            data.loc[df_mask, 'yx_spread'], halflife=360).mean().values\n",
    "    \n",
    "    data['yx_spread_dayly_ewma_10min'] = data['yx_spread'] - data['yx_spread_dayly_ewma_10min']\n",
    "    data['yx_spread_dayly_ewma_1hour'] = data['yx_spread'] - data['yx_spread_dayly_ewma_1hour']\n",
    "    data['yx_spread_ewma_prodpair_1hour_10min'] = (\n",
    "        data.yx_spread_dayly_ewma_1hour * data.yx_spread_dayly_ewma_10min)\n",
    "    \n",
    "    # Clean up temporary columns\n",
    "    temp_cols = [\n",
    "        'xy_garmonic_dayly_ewma_1hour', 'xy_garmonic_dayly_ewma_2hours',\n",
    "        'xy_square_time_mean_10min', 'xy_square_time_std_10min',\n",
    "        'yx_spread_dayly_ewma_10min', 'yx_spread_dayly_ewma_1hour',\n",
    "        'yprice_time_std_1hour', 'yprice_time_std_2hours',\n",
    "        'yx_spread_time_std_4hours'\n",
    "    ]\n",
    "    \n",
    "    for col in temp_cols:\n",
    "        if col in data.columns:\n",
    "            data.drop(col, axis=1, inplace=True)\n",
    "    \n",
    "    print(\"Feature engineering completed (NO DATA LEAKAGE)!\")\n",
    "    return data\n",
    "\n",
    "# %% Data Initialization and Feature Engineering\n",
    "def init_data(train_fname, test_fname=None):\n",
    "    \"\"\"Initialize and preprocess data\"\"\"\n",
    "    print('loading train file...')\n",
    "    data = pd.read_csv(train_fname)\n",
    "    ntrain = data.shape[0]\n",
    "    if test_fname:\n",
    "        print('loading test file...')\n",
    "        test_data = pd.read_csv(test_fname)\n",
    "        data = pd.concat([data, test_data], ignore_index=True)\n",
    "    data['xprice'] -= 127  # WARNING!\n",
    "    data['yprice'] -= 146  # WARNING!\n",
    "    \n",
    "    data['yx_spread'] = data.yprice - data.xprice\n",
    "    data['yx_relation'] = data.yprice / data.xprice\n",
    "    data['xy_relation'] = data.xprice / data.yprice\n",
    "    data['xy_square'] = np.sqrt(data.xprice ** 2 + data.yprice ** 2) / 2\n",
    "    data['xy_geom'] = np.sqrt(data.xprice * data.yprice)\n",
    "    data['xy_garmonic'] = 2 / (1 / data.xprice + 1 / data.yprice)\n",
    "    \n",
    "    data['timestamp'] = data['timestamp'] // 1000\n",
    "    data['timestamp'] = data['timestamp'].apply(lambda stamp: datetime.fromtimestamp(stamp))\n",
    "    data['timestamp'] = data['timestamp'] - pd.Timedelta(hours=1)\n",
    "    data.index = data['timestamp']\n",
    "    \n",
    "    data['weekday'] = data.timestamp.dt.weekday\n",
    "    data['is_end_of_week'] = (data.timestamp.dt.weekday >= 2).astype(int)\n",
    "    \n",
    "    data['day'] = (data.timestamp.dt.date - data.timestamp.dt.date.min()).apply(lambda x: int(x.days))\n",
    "    day_close_time = data.day.map(data.groupby('day').timestamp.max())\n",
    "    data['periods_before_closing'] = (day_close_time - data.timestamp).apply(lambda x: x.seconds // 10)\n",
    "    day_open_time = data.day.map(data.groupby('day').timestamp.min())\n",
    "    data['periods_after_opening'] = (data.timestamp - day_open_time).apply(lambda x: x.seconds // 10)\n",
    "    return data, ntrain\n",
    "\n",
    "def selected_features_extractor(train_data_path, test_data_path=None):\n",
    "    \"\"\"Main feature extraction pipeline\"\"\"\n",
    "    data, ntrain = init_data(train_data_path, test_data_path)\n",
    "    \n",
    "    # Use the FIXED feature engineering function\n",
    "    data = create_all_features(data)\n",
    "    \n",
    "    usecols = selected_cols + ['returns', 'periods_before_closing']\n",
    "    \n",
    "    train = data[usecols].iloc[droprows:ntrain]\n",
    "    test = data[usecols].iloc[ntrain:]\n",
    "    return train, test\n",
    "\n",
    "# %% Model Training Functions\n",
    "def normalize_train(df):\n",
    "    \"\"\"Normalize training data\"\"\"\n",
    "    extended_cols = selected_cols + ['returns', 'periods_before_closing']\n",
    "    norm_train = df[extended_cols].reset_index(drop=True).copy()\n",
    "    norm_mean = norm_train[selected_cols].mean()\n",
    "    norm_std = norm_train[selected_cols].std() + normalization_std_reg\n",
    "    norm_train.loc[:,selected_cols] = (norm_train[selected_cols] - norm_mean) / norm_std\n",
    "    return norm_train\n",
    "\n",
    "def normalize_train_test(train, test):\n",
    "    \"\"\"Normalize train and test data using training statistics\"\"\"\n",
    "    train_extended_cols = selected_cols + ['returns', 'periods_before_closing']\n",
    "    test_extended_cols = selected_cols + ['periods_before_closing']\n",
    "    \n",
    "    norm_train = train[train_extended_cols].reset_index(drop=True).copy()\n",
    "    norm_test = test[test_extended_cols].reset_index(drop=True).copy()\n",
    "    \n",
    "    norm_mean = norm_train[selected_cols].mean()\n",
    "    norm_std = norm_train[selected_cols].std() + normalization_std_reg\n",
    "    \n",
    "    norm_train.loc[:,selected_cols] = (norm_train[selected_cols] - norm_mean) / norm_std\n",
    "    norm_test.loc[:,selected_cols] = (norm_test[selected_cols] - norm_mean) / norm_std\n",
    "    return norm_train, norm_test\n",
    "\n",
    "\n",
    "# %% Model Estimation Function\n",
    "def modelEstimate(train_data_path):\n",
    "    \"\"\"\n",
    "    Train the model on the training data.\n",
    "    \n",
    "    Parameters:\n",
    "    train_data_path (str): Path to the training CSV file\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary containing the trained model and normalization parameters\n",
    "    \"\"\"\n",
    "    print('--------->Start Estimation....')\n",
    "    \n",
    "    # Extract features\n",
    "    train, _ = selected_features_extractor(train_data_path)\n",
    "    \n",
    "    # Normalize training data\n",
    "    print('----->Normalization....')\n",
    "    norm_train = normalize_train(train)\n",
    "    \n",
    "    # Calculate normalization parameters for later use\n",
    "    norm_mean = train[selected_cols].mean()\n",
    "    norm_std = train[selected_cols].std() + normalization_std_reg\n",
    "    \n",
    "    # Validate model\n",
    "    print('----->Validation....')\n",
    "    model = Ridge(alpha=ridge_alpha)\n",
    "    print(validate_model_by_pentate(model, norm_train, selected_cols, 0))\n",
    "    \n",
    "    # Fit final model\n",
    "    print('----->Fitting....')\n",
    "    model = Ridge(alpha=ridge_alpha)\n",
    "    model.fit(norm_train[selected_cols], norm_train.returns)\n",
    "    \n",
    "    # Print feature importances\n",
    "    print('----->Calculation feature importances...')\n",
    "    print_importances(model, selected_cols)\n",
    "    \n",
    "    # Return model and parameters\n",
    "    return {\n",
    "        'model': model,\n",
    "        'norm_mean': norm_mean,\n",
    "        'norm_std': norm_std\n",
    "    }\n",
    "\n",
    "# %% Model Forecast Function\n",
    "def modelForecast(test_data_path, model_params, train_data_path):\n",
    "    \"\"\"\n",
    "    Make predictions on test data using the trained model.\n",
    "    \n",
    "    Parameters:\n",
    "    test_data_path (str): Path to the test CSV file\n",
    "    model_params (dict): Dictionary containing the trained model and normalization parameters\n",
    "    train_data_path (str): Path to the training CSV file (needed for feature extraction)\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary containing predictions and evaluation metrics (if test data has returns)\n",
    "    \"\"\"\n",
    "    print('--------->Starting Forecasting....')\n",
    "    \n",
    "    # Extract features for both train and test\n",
    "    train, test = selected_features_extractor(train_data_path, test_data_path)\n",
    "    \n",
    "    # Normalize test data using training statistics\n",
    "    print('----->Normalization....')\n",
    "    test_extended_cols = selected_cols + ['periods_before_closing']\n",
    "    \n",
    "    # Check if test data has returns column for evaluation\n",
    "    has_returns = 'returns' in test.columns\n",
    "    if has_returns:\n",
    "        test_extended_cols.append('returns')\n",
    "    \n",
    "    norm_test = test[test_extended_cols].reset_index(drop=True).copy()\n",
    "    norm_test.loc[:,selected_cols] = (norm_test[selected_cols] - model_params['norm_mean']) / model_params['norm_std']\n",
    "    \n",
    "    # Make predictions\n",
    "    print('----->Prediction....')\n",
    "    predicted = model_params['model'].predict(norm_test[selected_cols])\n",
    "    \n",
    "    # Set predictions to 0 at closing periods\n",
    "    predicted[norm_test.periods_before_closing == 0] = 0\n",
    "    \n",
    "    # Calculate evaluation metrics if test data has returns\n",
    "    results = {'predictions': predicted}\n",
    "    \n",
    "    if has_returns:\n",
    "        print('----->Evaluation....')\n",
    "        # Calculate MSE\n",
    "        mse = mean_squared_error(norm_test.returns, predicted)\n",
    "        # Calculate R-squared\n",
    "        r2 = rsquared(norm_test.returns, predicted)\n",
    "        \n",
    "        results['mse'] = mse\n",
    "        results['r2'] = r2\n",
    "        \n",
    "        print(f'Test MSE: {mse:.6f}')\n",
    "        print(f'Test R-squared: {r2:.6f}')\n",
    "        print(f'Test R-squared (x100): {r2*100:.2f}')\n",
    "    else:\n",
    "        print('No returns column in test data - evaluation metrics not calculated')\n",
    "    \n",
    "    return results\n",
    "\n",
    "# %% Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Train the model\n",
    "    train_file_path = '/Users/mazin/Desktop/super prep materials/GSAPred/Two-financial-instruments/train.csv'  # Replace with your actual file path\n",
    "    model_params = modelEstimate(train_file_path)\n",
    "    \n",
    "    # Make predictions\n",
    "    test_file_path = '/Users/mazin/Desktop/super prep materials/GSAPred/Two-financial-instruments/test.csv'  # Replace with your actual file path\n",
    "    results = modelForecast(test_file_path, model_params, train_file_path)\n",
    "    \n",
    "    # Extract predictions\n",
    "    predictions = results['predictions']\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\nFirst 10 predictions: {predictions[:10]}\")\n",
    "    print(f\"Total predictions: {len(predictions)}\")\n",
    "    \n",
    "    # Display evaluation metrics if available\n",
    "    if 'mse' in results:\n",
    "        print(f\"\\nEvaluation Metrics:\")\n",
    "        print(f\"MSE: {results['mse']:.6f}\")\n",
    "        print(f\"R-squared: {results['r2']:.6f}\")\n",
    "        print(f\"R-squared (x100): {results['r2']*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6180900-99b0-4165-aaaf-49b12300d4a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
