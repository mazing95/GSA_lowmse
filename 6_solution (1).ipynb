{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8640b2e-c744-423e-a330-72cd0b57ba06",
   "metadata": {},
   "source": [
    "# Original best performing feature subset with ridge regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "435b7971-9895-4748-93e1-0b176228fe78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------->Start Estimation....\n",
      "Loading file: /Users/mazin/Desktop/super prep materials/GSAPred/Two-financial-instruments/train.csv...\n",
      "Creating all features (NO LEAKAGE - TRAIN)...\n",
      "Creating opening/closing difference features...\n",
      "Creating log features...\n",
      "Creating expanding mean difference...\n",
      "Creating time-based rolling features (TRULY FIXED)...\n",
      "Creating intraday EWMA features...\n",
      "Creating lag features...\n",
      "Creating RSI features...\n",
      "Creating EWMA on time means...\n",
      "Creating XY combined features...\n",
      "Creating YX spread features...\n",
      "Feature engineering completed (NO DATA LEAKAGE - TRAIN)!\n",
      "----->Normalization....\n",
      "----->Validation....\n",
      "     train_50_percent  train_60_percent  train_70_percent  train_80_percent  \\\n",
      "mse          0.031586          0.021226          0.017438          0.017618   \n",
      "r2           0.507232          1.332249          0.024151          2.788663   \n",
      "\n",
      "     train_90_percent  min_stats  max_stats       avg  \n",
      "mse          0.020447   0.017438   0.031586  0.022477  \n",
      "r2           0.711753   0.024151   2.788663  1.168123  \n",
      "----->Fitting....\n",
      "----->Calculation feature importances...\n",
      "yprice_time_zscore_2hours                13.78%           0.025\n",
      "yprice_dayly_ewma_10min                  11.64%          -0.021\n",
      "xprice_lag_4hours                        10.28%          -0.018\n",
      "xprice_dayly_ewma_4hour                  7.84%           0.014\n",
      "yprice_time_mean_10min                   7.73%          -0.014\n",
      "xy_square_time_zscore_10min              6.39%           0.011\n",
      "xy_geom_time_mean_2hours_dayly_ewma_20min 4.72%         -0.0085\n",
      "yprice_time_mean_2hours                  4.35%         -0.0078\n",
      "yprice_time_mean_10min_dayly_ewma_4hour  4.06%          0.0073\n",
      "xprice_time_mean_10min_dayly_ewma_10min  3.97%         -0.0071\n",
      "yprice_time_mean_1hour_lag_1workweek     3.21%         -0.0057\n",
      "yprice_time_mean_1hour                   3.13%         -0.0056\n",
      "xlog                                     3.02%          0.0054\n",
      "xy_garmonic_time_std_4hours              2.74%          0.0049\n",
      "ylog_dayly_ewma_1hour                    2.57%          0.0046\n",
      "xprice_time_mean_1hour_lag_4hours        2.38%         -0.0043\n",
      "xdiff_from_opening                       1.59%          0.0028\n",
      "yprice_ewma_difpair_10min_4hour          1.52%          0.0027\n",
      "yprice_time_mean_10min_rsi_1hour         1.39%          0.0025\n",
      "xlog_dayly_ewma_10min                    1.02%          0.0018\n",
      "yprice_expanding_mean_diff               0.81%         -0.0015\n",
      "yx_spread_ewma_prodpair_1hour_10min      0.73%         -0.0013\n",
      "yx_spread_time_zscore_4hours             0.69%         -0.0012\n",
      "yprice_time_zscore_1hour                 0.24%        -0.00043\n",
      "yx_spread_time_mean_10min_lag_1hour      0.12%         0.00022\n",
      "yprice_time_mean_10min_lag_10min         0.06%        -0.00011\n",
      "Model saved to model_params.pkl\n",
      "--------->Starting Forecasting....\n",
      "Loading file: /Users/mazin/Desktop/super prep materials/GSAPred/Two-financial-instruments/test.csv...\n",
      "Creating all features (NO LEAKAGE - TEST)...\n",
      "Creating opening/closing difference features...\n",
      "Creating log features...\n",
      "Creating expanding mean difference...\n",
      "Creating time-based rolling features (TRULY FIXED)...\n",
      "Creating intraday EWMA features...\n",
      "Creating lag features...\n",
      "Creating RSI features...\n",
      "Creating EWMA on time means...\n",
      "Creating XY combined features...\n",
      "Creating YX spread features...\n",
      "Feature engineering completed (NO DATA LEAKAGE - TEST)!\n",
      "----->Normalization....\n",
      "----->Prediction....\n",
      "----->Evaluation....\n",
      "Test MSE: 0.013038\n",
      "Test R-squared: 0.019115\n",
      "Test R-squared (x100): 1.91\n",
      "\n",
      "First 10 predictions: [0.00280606 0.00309316 0.00439552 0.00566933 0.00254544 0.00078285\n",
      " 0.00110183 0.00091558 0.00591014 0.00713918]\n",
      "Total predictions: 32180\n",
      "\n",
      "Evaluation Metrics:\n",
      "MSE: 0.013038\n",
      "R-squared: 0.019115\n",
      "R-squared (x100): 1.91\n",
      "\n",
      "Predictions saved to predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from copy import copy\n",
    "import scipy\n",
    "import pickle\n",
    "\n",
    "# %% Configuration\n",
    "# Configuration parameters\n",
    "droprows = 7050\n",
    "std_reg_const = 0.1\n",
    "normalization_std_reg = 0.0001\n",
    "ridge_alpha = 1333\n",
    "\n",
    "selected_cols = [\n",
    "    # Opening/Closing difference features\n",
    "    'xdiff_from_opening',\n",
    "    \n",
    "    # X-price features\n",
    "    'xlog',\n",
    "    'xlog_dayly_ewma_10min',\n",
    "    'xprice_dayly_ewma_4hour',\n",
    "    'xprice_lag_4hours',\n",
    "    'xprice_time_mean_1hour_lag_4hours',\n",
    "    'xprice_time_mean_10min_dayly_ewma_10min',\n",
    "    \n",
    "    # Y-price features\n",
    "    'ylog_dayly_ewma_1hour',\n",
    "    'yprice_dayly_ewma_10min',\n",
    "    'yprice_ewma_difpair_10min_4hour',\n",
    "    'yprice_expanding_mean_diff',\n",
    "    'yprice_time_mean_1hour',\n",
    "    'yprice_time_mean_1hour_lag_1workweek',\n",
    "    'yprice_time_mean_10min',\n",
    "    'yprice_time_mean_10min_dayly_ewma_4hour',\n",
    "    'yprice_time_mean_10min_lag_10min',\n",
    "    'yprice_time_mean_10min_rsi_1hour',\n",
    "    'yprice_time_mean_2hours',\n",
    "    'yprice_time_zscore_1hour',\n",
    "    'yprice_time_zscore_2hours',\n",
    "    \n",
    "    # XY combined features\n",
    "    'xy_garmonic_time_std_4hours',\n",
    "    'xy_geom_time_mean_2hours_dayly_ewma_20min',\n",
    "    'xy_square_time_zscore_10min',\n",
    "    \n",
    "    # YX spread features\n",
    "    'yx_spread_ewma_prodpair_1hour_10min',\n",
    "    'yx_spread_time_mean_10min_lag_1hour',\n",
    "    'yx_spread_time_zscore_4hours',\n",
    "]\n",
    "\n",
    "\n",
    "# %% Helper Functions\n",
    "def print_importances(model, selected_cols):\n",
    "    \"\"\"Print feature importances sorted by absolute weight\"\"\"\n",
    "    weigts_sum = sum(map(abs, model.coef_))\n",
    "    for name, weight in sorted(zip(selected_cols, model.coef_), key=lambda x: -abs(x[1])):\n",
    "        percent_weight = abs(weight) / weigts_sum\n",
    "        print('{:40} {:.2%} {:15.2}'.format(name, percent_weight, weight))\n",
    "\n",
    "def rsquared(x, y):\n",
    "    \"\"\"Return R^2 where x and y are array-like.\"\"\"\n",
    "    slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(x, y)\n",
    "    return r_value**2\n",
    "\n",
    "def rsiFunc(prices, n=14):\n",
    "    \"\"\"Calculate RSI (Relative Strength Index)\"\"\"\n",
    "    deltas = np.diff(prices)\n",
    "    seed = deltas[:n+1]\n",
    "    up = seed[seed>=0].sum()/n\n",
    "    down = -seed[seed<0].sum()/n\n",
    "    rs = up/down\n",
    "    rsi = np.zeros_like(prices)\n",
    "    rsi[:n] = 100. - 100./(1.+rs)\n",
    "\n",
    "    for i in range(n, len(prices)):\n",
    "        delta = deltas[i-1]\n",
    "        if delta>0:\n",
    "            upval = delta\n",
    "            downval = 0.\n",
    "        else:\n",
    "            upval = 0.\n",
    "            downval = -delta\n",
    "\n",
    "        up = (up*(n-1) + upval)/n\n",
    "        down = (down*(n-1) + downval)/n\n",
    "        rs = up/down\n",
    "        rsi[i] = 100. - 100./(1.+rs)\n",
    "    return rsi\n",
    "\n",
    "# %% Time Series Validation Functions\n",
    "def time_split(data, valid_ratio, test_ratio):\n",
    "    \"\"\"Split time series data into train, validation, and test sets\"\"\"\n",
    "    n_valid = max(1, int(data.shape[0] * valid_ratio))\n",
    "    n_test = max(1, int(data.shape[0] * test_ratio))\n",
    "    n_train = data.shape[0] - n_valid - n_test\n",
    "    \n",
    "    train = data.iloc[:n_train].reset_index(drop=True).copy()\n",
    "    valid = data.iloc[n_train:-n_test].reset_index(drop=True).copy()\n",
    "    test = data.iloc[-n_test:].reset_index(drop=True).copy()\n",
    "    merged_test = pd.concat([valid, test], ignore_index=True)\n",
    "    return train, valid, test\n",
    "\n",
    "def validate_model_by_pentate(model, source_data, base_cols, droprows=0):\n",
    "    \"\"\"Validate model using 5-fold time series cross-validation\"\"\"\n",
    "    df = source_data.copy()\n",
    "    selected_cols = base_cols.copy()\n",
    "    helper_cols = list(set(selected_cols + ['periods_before_closing', 'returns']))\n",
    "    metrics_dict = {}\n",
    "    \n",
    "    for step in range(5, 10):\n",
    "        n_train = int(df.shape[0] * step // 10)\n",
    "        n_test = int(df.shape[0] * (step + 1) // 10)\n",
    "        train = df.iloc[:n_train].reset_index(drop=True).copy()\n",
    "        test = df.iloc[n_train:n_test].reset_index(drop=True).copy()\n",
    "        train.drop(np.arange(droprows), inplace=True)\n",
    "        train.dropna(inplace=True)\n",
    "\n",
    "        model.fit(train[selected_cols], train.returns)\n",
    "        predicted = model.predict(test[selected_cols])\n",
    "        predicted[test.periods_before_closing == 0] = 0\n",
    "\n",
    "        current_mse = mean_squared_error(test.returns, predicted)\n",
    "        current_r2 = rsquared(test.returns, predicted) * 100\n",
    "        metrics_dict['train_{}_percent'.format(step * 10)] = {\n",
    "            'mse': current_mse,\n",
    "            'r2': current_r2\n",
    "        }\n",
    "    \n",
    "    report = pd.DataFrame(metrics_dict)\n",
    "    report['min_stats'] = report.iloc[:,:5].min(1).astype(np.float32)\n",
    "    report['max_stats'] = report.iloc[:,:5].max(1).astype(np.float32)\n",
    "    report['avg'] = report.mean(1).astype(np.float32)\n",
    "    return report\n",
    "\n",
    "# %% TRULY FIXED Feature Engineering Function\n",
    "def create_all_features(data, is_train=True):\n",
    "    \"\"\"\n",
    "    TRULY FIXED: Feature engineering with NO data leakage.\n",
    "    All features use strictly backward-looking rolling windows.\n",
    "    Current observation at time t IS included (since we predict t+60).\n",
    "    No resample+ffill pattern that causes intra-bin leakage.\n",
    "    \"\"\"\n",
    "    print(f\"Creating all features (NO LEAKAGE - {'TRAIN' if is_train else 'TEST'})...\")\n",
    "    \n",
    "    # Pre-calculate commonly used values\n",
    "    days = data.day.unique()\n",
    "    \n",
    "    # Opening/Closing differences (using previous day's close)\n",
    "    print(\"Creating opening/closing difference features...\")\n",
    "    close_price_per_day_y = data.groupby('day').timestamp.max().shift(1).map(\n",
    "        data[['timestamp', 'yprice']].set_index('timestamp').yprice)\n",
    "    data['ydiff_from_closing'] = (data.yprice - data.day.map(close_price_per_day_y)).fillna(0)\n",
    "    \n",
    "    close_price_per_day_x = data.groupby('day').timestamp.max().shift(1).map(\n",
    "        data[['timestamp', 'xprice']].set_index('timestamp').xprice)\n",
    "    data['xdiff_from_closing'] = (data.xprice - data.day.map(close_price_per_day_x)).fillna(0)\n",
    "    \n",
    "    open_price_per_day_x = data.groupby('day').timestamp.min().map(\n",
    "        data[['timestamp', 'xprice']].set_index('timestamp').xprice)\n",
    "    data['xdiff_from_opening'] = data.xprice - data.day.map(open_price_per_day_x)\n",
    "    \n",
    "    # Log features\n",
    "    print(\"Creating log features...\")\n",
    "    data['xlog'] = data.xprice.apply(np.log1p)\n",
    "    data['ylog'] = data.yprice.apply(np.log1p)\n",
    "    \n",
    "    # Expanding mean that only uses past data (including current)\n",
    "    print(\"Creating expanding mean difference...\")\n",
    "    data['yprice_expanding_mean_diff'] = data['yprice'] - data['yprice'].expanding(min_periods=1).mean()\n",
    "    \n",
    "    # FIXED: Time-based rolling features with backward-looking windows INCLUDING current\n",
    "    print(\"Creating time-based rolling features (TRULY FIXED)...\")\n",
    "    time_windows = {\n",
    "        6: '1min', 60: '10min', 360: '1hour', 720: '2hours', 1410: '4hours', 2820: '1workweek'\n",
    "    }\n",
    "    \n",
    "    # Create rolling means with backward-looking windows INCLUDING current observation\n",
    "    for window, name in time_windows.items():\n",
    "        if window in [6, 60, 360, 1410]:  # xprice windows\n",
    "            colname = f'xprice_time_mean_{name}'\n",
    "            # Rolling includes current observation (no shift)\n",
    "            data[colname] = data['xprice'] - data['xprice'].rolling(\n",
    "                window=window, min_periods=1).mean()\n",
    "        \n",
    "        if window in [60, 360, 720]:  # yprice windows\n",
    "            colname = f'yprice_time_mean_{name}'\n",
    "            # Rolling includes current observation (no shift)\n",
    "            data[colname] = data['yprice'] - data['yprice'].rolling(\n",
    "                window=window, min_periods=1).mean()\n",
    "    \n",
    "    # Create rolling std for yprice with backward-looking windows INCLUDING current\n",
    "    for window in [360, 720]:\n",
    "        name = time_windows[window]\n",
    "        colname = f'yprice_time_std_{name}'\n",
    "        # Rolling includes current observation (no shift)\n",
    "        data[colname] = data['yprice'].rolling(\n",
    "            window=window, min_periods=1).std().fillna(0) + std_reg_const\n",
    "    \n",
    "    # Z-scores for yprice\n",
    "    data['yprice_time_zscore_1hour'] = data.yprice_time_mean_1hour / data.yprice_time_std_1hour\n",
    "    data['yprice_time_zscore_2hours'] = data.yprice_time_mean_2hours / data.yprice_time_std_2hours\n",
    "    \n",
    "    # Intraday EWMA features (backward-looking INCLUDING current)\n",
    "    print(\"Creating intraday EWMA features...\")\n",
    "    ewma_configs = [\n",
    "        ('xprice', [24], ['4hour']),\n",
    "        ('xlog', [60], ['10min']),\n",
    "        ('ylog', [360], ['1hour']),\n",
    "        ('yprice', [24, 60], ['4hour', '10min'])\n",
    "    ]\n",
    "    \n",
    "    for col, windows, names in ewma_configs:\n",
    "        for day in days:\n",
    "            df_mask = (data.day == day)\n",
    "            day_data = data.loc[df_mask].copy()\n",
    "            \n",
    "            for window, name in zip(windows, names):\n",
    "                colname = f'{col}_dayly_ewma_{name}'\n",
    "                # EWMA includes current observation (no shift)\n",
    "                ewm = day_data[col].ewm(halflife=window, adjust=False).mean()\n",
    "                \n",
    "                if col in ['xprice', 'yprice']:\n",
    "                    data.loc[df_mask, colname] = day_data[col] - ewm\n",
    "                elif col in ['xlog', 'ylog']:\n",
    "                    data.loc[df_mask, colname] = day_data[col.replace('log', 'price')] - ewm\n",
    "    \n",
    "    # EWMA difference pair\n",
    "    data['yprice_ewma_difpair_10min_4hour'] = data.yprice_dayly_ewma_10min - data.yprice_dayly_ewma_4hour\n",
    "    \n",
    "    # Lag features (inherently backward-looking, excludes current)\n",
    "    print(\"Creating lag features...\")\n",
    "    lag_configs = [\n",
    "        ('xprice', 1410, '4hours'),\n",
    "        ('yprice', 1410, '4hours'),\n",
    "        ('xprice_time_mean_1hour', 1410, '4hours'),\n",
    "        ('yprice_time_mean_1hour', 1410, '4hours'),\n",
    "        ('yprice_time_mean_1hour', 2820, '1workweek'),\n",
    "        ('yprice_time_mean_10min', 60, '10min')\n",
    "    ]\n",
    "    \n",
    "    for col, lag, name in lag_configs:\n",
    "        data[f'{col}_lag_{name}'] = data[col].shift(lag)\n",
    "    \n",
    "    # RSI features (calculated on backward-looking data including current)\n",
    "    print(\"Creating RSI features...\")\n",
    "    # RSI needs some history, so we calculate it on the rolling means\n",
    "    if 'xprice_time_mean_1min' in data.columns:\n",
    "        data['xprice_time_mean_1min_rsi_1min'] = rsiFunc(data['xprice_time_mean_1min'].fillna(0).values, 6)\n",
    "    data['yprice_time_mean_10min_rsi_1hour'] = rsiFunc(data['yprice_time_mean_10min'].fillna(0).values, 360)\n",
    "    \n",
    "    # Additional EWMA on time means\n",
    "    print(\"Creating EWMA on time means...\")\n",
    "    ewma_on_means = [\n",
    "        ('xprice_time_mean_10min', 60, '10min'),\n",
    "        ('yprice_time_mean_10min', 24, '4hour')\n",
    "    ]\n",
    "    \n",
    "    for col, window, name in ewma_on_means:\n",
    "        for day in days:\n",
    "            df_mask = (data.day == day)\n",
    "            day_data = data.loc[df_mask].copy()\n",
    "            colname = f'{col}_dayly_ewma_{name}'\n",
    "            # EWMA includes current observation (no shift)\n",
    "            ewm = day_data[col].ewm(halflife=window, adjust=False).mean()\n",
    "            data.loc[df_mask, colname] = ewm\n",
    "    \n",
    "    # XY Combined features with backward-looking windows INCLUDING current\n",
    "    print(\"Creating XY combined features...\")\n",
    "    \n",
    "    # XY Harmonic std - use rolling window INCLUDING current\n",
    "    data['xy_garmonic_time_std_4hours'] = (\n",
    "        data['xy_garmonic'].rolling(window=1410, min_periods=1).std().fillna(0) + std_reg_const\n",
    "    )\n",
    "    \n",
    "    # Harmonic EWMA\n",
    "    for day in days:\n",
    "        df_mask = (data.day == day)\n",
    "        day_data = data.loc[df_mask].copy()\n",
    "        \n",
    "        # EWMA includes current observation (no shift)\n",
    "        data.loc[df_mask, 'xy_garmonic_dayly_ewma_1hour'] = (\n",
    "            day_data['xy_garmonic'].ewm(halflife=360, adjust=False).mean()\n",
    "        )\n",
    "        data.loc[df_mask, 'xy_garmonic_dayly_ewma_2hours'] = (\n",
    "            day_data['xy_garmonic'].ewm(halflife=720, adjust=False).mean()\n",
    "        )\n",
    "    \n",
    "    data['xy_garmonic_dayly_ewma_1hour'] = data['xy_garmonic'] - data['xy_garmonic_dayly_ewma_1hour']\n",
    "    data['xy_garmonic_dayly_ewma_2hours'] = data['xy_garmonic'] - data['xy_garmonic_dayly_ewma_2hours']\n",
    "    data['xy_garmonic_ewma_prodpair_2hours_1hour'] = (\n",
    "        data.xy_garmonic_dayly_ewma_2hours * data.xy_garmonic_dayly_ewma_1hour)\n",
    "    \n",
    "    # XY Geometric features with rolling windows INCLUDING current\n",
    "    for window, name in [(6, '1min'), (60, '10min'), (360, '1hour'), (720, '2hours')]:\n",
    "        colname = f'xy_geom_time_mean_{name}'\n",
    "        # Rolling includes current observation (no shift)\n",
    "        data[colname] = data['xy_geom'] - data['xy_geom'].rolling(\n",
    "            window=window, min_periods=1).mean()\n",
    "    \n",
    "    # Geometric lags and EWMA\n",
    "    data['xy_geom_time_mean_1hour_lag_20min'] = data['xy_geom_time_mean_1hour'].shift(120)\n",
    "    \n",
    "    for day in days:\n",
    "        df_mask = (data.day == day)\n",
    "        day_data = data.loc[df_mask].copy()\n",
    "        \n",
    "        # EWMA includes current observation (no shift)\n",
    "        data.loc[df_mask, 'xy_geom_time_mean_10min_dayly_ewma_1min'] = (\n",
    "            day_data['xy_geom_time_mean_10min'].ewm(halflife=6, adjust=False).mean()\n",
    "        )\n",
    "        data.loc[df_mask, 'xy_geom_time_mean_2hours_dayly_ewma_20min'] = (\n",
    "            day_data['xy_geom_time_mean_2hours'].ewm(halflife=120, adjust=False).mean()\n",
    "        )\n",
    "    \n",
    "    # XY Relation std with rolling windows INCLUDING current\n",
    "    for window, name in [(360, '1hour'), (720, '2hours')]:\n",
    "        colname = f'xy_relation_time_std_{name}'\n",
    "        data[colname] = (\n",
    "            data['xy_relation'].rolling(window=window, min_periods=1).std().fillna(0) + std_reg_const\n",
    "        )\n",
    "    \n",
    "    # XY Square zscore with rolling windows INCLUDING current\n",
    "    data['xy_square_time_mean_10min'] = (\n",
    "        data['xy_square'] - data['xy_square'].rolling(window=60, min_periods=1).mean()\n",
    "    )\n",
    "    data['xy_square_time_std_10min'] = (\n",
    "        data['xy_square'].rolling(window=60, min_periods=1).std().fillna(0) + std_reg_const\n",
    "    )\n",
    "    data['xy_square_time_zscore_10min'] = data['xy_square_time_mean_10min'] / data['xy_square_time_std_10min']\n",
    "    \n",
    "    # YX Spread features with rolling windows INCLUDING current\n",
    "    print(\"Creating YX spread features...\")\n",
    "    for window, name in [(60, '10min'), (720, '2hours'), (1410, '4hours')]:\n",
    "        colname = f'yx_spread_time_mean_{name}'\n",
    "        data[colname] = (\n",
    "            data['yx_spread'] - data['yx_spread'].rolling(window=window, min_periods=1).mean()\n",
    "        )\n",
    "    \n",
    "    # Spread std and zscore with rolling windows INCLUDING current\n",
    "    data['yx_spread_time_std_4hours'] = (\n",
    "        data['yx_spread'].rolling(window=1410, min_periods=1).std().fillna(0) + std_reg_const\n",
    "    )\n",
    "    data['yx_spread_time_zscore_4hours'] = data['yx_spread_time_mean_4hours'] / data['yx_spread_time_std_4hours']\n",
    "    \n",
    "    # Spread lags (excluding current)\n",
    "    data['yx_spread_time_mean_10min_lag_1hour'] = data['yx_spread_time_mean_10min'].shift(360)\n",
    "    data['yx_spread_time_mean_2hours_lag_20min'] = data['yx_spread_time_mean_2hours'].shift(120)\n",
    "    \n",
    "    # Spread EWMA INCLUDING current\n",
    "    for day in days:\n",
    "        df_mask = (data.day == day)\n",
    "        day_data = data.loc[df_mask].copy()\n",
    "        \n",
    "        # EWMA includes current observation (no shift)\n",
    "        data.loc[df_mask, 'yx_spread_dayly_ewma_10min'] = (\n",
    "            day_data['yx_spread'].ewm(halflife=60, adjust=False).mean()\n",
    "        )\n",
    "        data.loc[df_mask, 'yx_spread_dayly_ewma_1hour'] = (\n",
    "            day_data['yx_spread'].ewm(halflife=360, adjust=False).mean()\n",
    "        )\n",
    "    \n",
    "    data['yx_spread_dayly_ewma_10min'] = data['yx_spread'] - data['yx_spread_dayly_ewma_10min']\n",
    "    data['yx_spread_dayly_ewma_1hour'] = data['yx_spread'] - data['yx_spread_dayly_ewma_1hour']\n",
    "    data['yx_spread_ewma_prodpair_1hour_10min'] = (\n",
    "        data.yx_spread_dayly_ewma_1hour * data.yx_spread_dayly_ewma_10min)\n",
    "    \n",
    "    # Clean up temporary columns\n",
    "    temp_cols = [\n",
    "        'xy_garmonic_dayly_ewma_1hour', 'xy_garmonic_dayly_ewma_2hours',\n",
    "        'xy_square_time_mean_10min', 'xy_square_time_std_10min',\n",
    "        'yx_spread_dayly_ewma_10min', 'yx_spread_dayly_ewma_1hour',\n",
    "        'yprice_time_std_1hour', 'yprice_time_std_2hours',\n",
    "        'yx_spread_time_std_4hours', 'xy_relation_time_std_1hour',\n",
    "        'xy_relation_time_std_2hours'\n",
    "    ]\n",
    "    \n",
    "    for col in temp_cols:\n",
    "        if col in data.columns:\n",
    "            data.drop(col, axis=1, inplace=True)\n",
    "    \n",
    "    print(f\"Feature engineering completed (NO DATA LEAKAGE - {'TRAIN' if is_train else 'TEST'})!\")\n",
    "    return data\n",
    "\n",
    "# %% Data Initialization\n",
    "def init_data_single(fname):\n",
    "    \"\"\"Initialize and preprocess a single dataset\"\"\"\n",
    "    print(f'Loading file: {fname}...')\n",
    "    data = pd.read_csv(fname)\n",
    "    \n",
    "    data['xprice'] -= 127  # WARNING: Domain-specific adjustment\n",
    "    data['yprice'] -= 146  # WARNING: Domain-specific adjustment\n",
    "    \n",
    "    # Create derived price features\n",
    "    data['yx_spread'] = data.yprice - data.xprice\n",
    "    data['yx_relation'] = data.yprice / data.xprice\n",
    "    data['xy_relation'] = data.xprice / data.yprice\n",
    "    data['xy_square'] = np.sqrt(data.xprice ** 2 + data.yprice ** 2) / 2\n",
    "    data['xy_geom'] = np.sqrt(data.xprice * data.yprice)\n",
    "    data['xy_garmonic'] = 2 / (1 / data.xprice + 1 / data.yprice)\n",
    "    \n",
    "    # Process timestamps\n",
    "    data['timestamp'] = data['timestamp'] // 1000\n",
    "    data['timestamp'] = data['timestamp'].apply(lambda stamp: datetime.fromtimestamp(stamp))\n",
    "    data['timestamp'] = data['timestamp'] - pd.Timedelta(hours=1)\n",
    "    data.index = data['timestamp']\n",
    "    \n",
    "    # Add time-based features\n",
    "    data['weekday'] = data.timestamp.dt.weekday\n",
    "    data['is_end_of_week'] = (data.timestamp.dt.weekday >= 2).astype(int)\n",
    "    \n",
    "    data['day'] = (data.timestamp.dt.date - data.timestamp.dt.date.min()).apply(lambda x: int(x.days))\n",
    "    day_close_time = data.day.map(data.groupby('day').timestamp.max())\n",
    "    data['periods_before_closing'] = (day_close_time - data.timestamp).apply(lambda x: x.seconds // 10)\n",
    "    day_open_time = data.day.map(data.groupby('day').timestamp.min())\n",
    "    data['periods_after_opening'] = (data.timestamp - day_open_time).apply(lambda x: x.seconds // 10)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def selected_features_extractor_separate(train_data_path, test_data_path):\n",
    "    \"\"\"\n",
    "    Extract features separately for train and test to avoid leakage.\n",
    "    This is the SAFE approach - features are computed independently.\n",
    "    \n",
    "    Parameters:\n",
    "    train_data_path: Path to training data (can be None if only processing test)\n",
    "    test_data_path: Path to test data (can be None if only processing train)\n",
    "    \"\"\"\n",
    "    train = None\n",
    "    test = None\n",
    "    \n",
    "    # Process training data if provided\n",
    "    if train_data_path is not None:\n",
    "        train_data = init_data_single(train_data_path)\n",
    "        train_data = create_all_features(train_data, is_train=True)\n",
    "        \n",
    "        usecols = selected_cols + ['returns', 'periods_before_closing']\n",
    "        train = train_data[usecols].iloc[droprows:].copy()\n",
    "    \n",
    "    # Process test data if provided\n",
    "    if test_data_path is not None:\n",
    "        test_data = init_data_single(test_data_path)\n",
    "        test_data = create_all_features(test_data, is_train=False)\n",
    "        \n",
    "        # Test might not have 'returns' column\n",
    "        test_cols = selected_cols + ['periods_before_closing']\n",
    "        if 'returns' in test_data.columns:\n",
    "            test_cols.append('returns')\n",
    "        test = test_data[test_cols].copy()\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "# %% Model Training Functions\n",
    "def normalize_train(df):\n",
    "    \"\"\"Normalize training data\"\"\"\n",
    "    extended_cols = selected_cols + ['returns', 'periods_before_closing']\n",
    "    norm_train = df[extended_cols].reset_index(drop=True).copy()\n",
    "    norm_mean = norm_train[selected_cols].mean()\n",
    "    norm_std = norm_train[selected_cols].std() + normalization_std_reg\n",
    "    norm_train.loc[:,selected_cols] = (norm_train[selected_cols] - norm_mean) / norm_std\n",
    "    return norm_train\n",
    "\n",
    "def normalize_train_test(train, test):\n",
    "    \"\"\"Normalize train and test data using training statistics\"\"\"\n",
    "    train_extended_cols = selected_cols + ['returns', 'periods_before_closing']\n",
    "    test_extended_cols = selected_cols + ['periods_before_closing']\n",
    "    \n",
    "    norm_train = train[train_extended_cols].reset_index(drop=True).copy()\n",
    "    norm_test = test[test_extended_cols].reset_index(drop=True).copy()\n",
    "    \n",
    "    norm_mean = norm_train[selected_cols].mean()\n",
    "    norm_std = norm_train[selected_cols].std() + normalization_std_reg\n",
    "    \n",
    "    norm_train.loc[:,selected_cols] = (norm_train[selected_cols] - norm_mean) / norm_std\n",
    "    norm_test.loc[:,selected_cols] = (norm_test[selected_cols] - norm_mean) / norm_std\n",
    "    return norm_train, norm_test\n",
    "\n",
    "# %% Model Estimation Function\n",
    "def modelEstimate(train_data_path):\n",
    "    \"\"\"\n",
    "    Train the model on the training data.\n",
    "    \n",
    "    Parameters:\n",
    "    train_data_path (str): Path to the training CSV file\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary containing the trained model and normalization parameters\n",
    "    \"\"\"\n",
    "    print('--------->Start Estimation....')\n",
    "    \n",
    "    # Extract features (train only, no test contamination)\n",
    "    train, _ = selected_features_extractor_separate(train_data_path, None)\n",
    "    \n",
    "    # Remove any rows with NaN values\n",
    "    train = train.dropna()\n",
    "    \n",
    "    # Normalize training data\n",
    "    print('----->Normalization....')\n",
    "    norm_train = normalize_train(train)\n",
    "    \n",
    "    # Calculate normalization parameters for later use\n",
    "    norm_mean = train[selected_cols].mean()\n",
    "    norm_std = train[selected_cols].std() + normalization_std_reg\n",
    "    \n",
    "    # Validate model\n",
    "    print('----->Validation....')\n",
    "    model = Ridge(alpha=ridge_alpha)\n",
    "    print(validate_model_by_pentate(model, norm_train, selected_cols, 0))\n",
    "    \n",
    "    # Fit final model\n",
    "    print('----->Fitting....')\n",
    "    model = Ridge(alpha=ridge_alpha)\n",
    "    model.fit(norm_train[selected_cols], norm_train.returns)\n",
    "    \n",
    "    # Print feature importances\n",
    "    print('----->Calculation feature importances...')\n",
    "    print_importances(model, selected_cols)\n",
    "    \n",
    "    # Return model and parameters\n",
    "    return {\n",
    "        'model': model,\n",
    "        'norm_mean': norm_mean,\n",
    "        'norm_std': norm_std\n",
    "    }\n",
    "\n",
    "# %% Model Forecast Function\n",
    "def modelForecast(test_data_path, model_params, train_data_path=None):\n",
    "    \"\"\"\n",
    "    Make predictions on test data using the trained model.\n",
    "    \n",
    "    Parameters:\n",
    "    test_data_path (str): Path to the test CSV file\n",
    "    model_params (dict): Dictionary containing the trained model and normalization parameters\n",
    "    train_data_path (str): Not used in this version (features computed separately)\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary containing predictions and evaluation metrics (if test data has returns)\n",
    "    \"\"\"\n",
    "    print('--------->Starting Forecasting....')\n",
    "    \n",
    "    # Extract features for test only (no train contamination)\n",
    "    _, test = selected_features_extractor_separate(None, test_data_path)\n",
    "    \n",
    "    # Remove any rows with NaN values\n",
    "    test = test.dropna()\n",
    "    \n",
    "    # Normalize test data using training statistics\n",
    "    print('----->Normalization....')\n",
    "    test_extended_cols = selected_cols + ['periods_before_closing']\n",
    "    \n",
    "    # Check if test data has returns column for evaluation\n",
    "    has_returns = 'returns' in test.columns\n",
    "    if has_returns:\n",
    "        test_extended_cols.append('returns')\n",
    "    \n",
    "    norm_test = test[test_extended_cols].reset_index(drop=True).copy()\n",
    "    norm_test.loc[:,selected_cols] = (norm_test[selected_cols] - model_params['norm_mean']) / model_params['norm_std']\n",
    "    \n",
    "    # Make predictions\n",
    "    print('----->Prediction....')\n",
    "    predicted = model_params['model'].predict(norm_test[selected_cols])\n",
    "    \n",
    "    # Set predictions to 0 at closing periods\n",
    "    predicted[norm_test.periods_before_closing == 0] = 0\n",
    "    \n",
    "    # Calculate evaluation metrics if test data has returns\n",
    "    results = {'predictions': predicted}\n",
    "    \n",
    "    if has_returns:\n",
    "        print('----->Evaluation....')\n",
    "        # Calculate MSE\n",
    "        mse = mean_squared_error(norm_test.returns, predicted)\n",
    "        # Calculate R-squared\n",
    "        r2 = rsquared(norm_test.returns, predicted)\n",
    "        \n",
    "        results['mse'] = mse\n",
    "        results['r2'] = r2\n",
    "        \n",
    "        print(f'Test MSE: {mse:.6f}')\n",
    "        print(f'Test R-squared: {r2:.6f}')\n",
    "        print(f'Test R-squared (x100): {r2*100:.2f}')\n",
    "    else:\n",
    "        print('No returns column in test data - evaluation metrics not calculated')\n",
    "    \n",
    "    return results\n",
    "\n",
    "# %% Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Train the model\n",
    "    train_file_path = '/Users/mazin/Desktop/super prep materials/GSAPred/Two-financial-instruments/train.csv'  # Replace with your actual file path\n",
    "    model_params = modelEstimate(train_file_path)\n",
    "    \n",
    "    # Save model for later use\n",
    "    with open('model_params.pkl', 'wb') as f:\n",
    "        pickle.dump(model_params, f)\n",
    "    print(\"Model saved to model_params.pkl\")\n",
    "    \n",
    "    # Make predictions on test data\n",
    "    test_file_path = '/Users/mazin/Desktop/super prep materials/GSAPred/Two-financial-instruments/test.csv'  # Replace with your actual file path\n",
    "    \n",
    "    # Load model if needed\n",
    "    # with open('model_params.pkl', 'rb') as f:\n",
    "    #     model_params = pickle.load(f)\n",
    "    \n",
    "    results = modelForecast(test_file_path, model_params)\n",
    "    \n",
    "    # Extract predictions\n",
    "    predictions = results['predictions']\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\nFirst 10 predictions: {predictions[:10]}\")\n",
    "    print(f\"Total predictions: {len(predictions)}\")\n",
    "    \n",
    "    # Display evaluation metrics if available\n",
    "    if 'mse' in results:\n",
    "        print(f\"\\nEvaluation Metrics:\")\n",
    "        print(f\"MSE: {results['mse']:.6f}\")\n",
    "        print(f\"R-squared: {results['r2']:.6f}\")\n",
    "        print(f\"R-squared (x100): {results['r2']*100:.2f}\")\n",
    "    \n",
    "    # Save predictions to CSV\n",
    "    predictions_df = pd.DataFrame({'predictions': predictions})\n",
    "    predictions_df.to_csv('predictions.csv', index=False)\n",
    "    print(\"\\nPredictions saved to predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410ef305-d208-416b-9d66-ac3eb8d66bdd",
   "metadata": {},
   "source": [
    "# All features (worse performance) (exlcuding xy_relation_time_std_1hour and xy_relation_time_std_2hours as they are no longer calculated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "19427498-fe8d-4bb0-a867-e5d1599eef1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------->Start Estimation....\n",
      "Loading file: /Users/mazin/Desktop/super prep materials/GSAPred/Two-financial-instruments/train.csv...\n",
      "Creating all features (NO LEAKAGE - TRAIN)...\n",
      "Creating opening/closing difference features...\n",
      "Creating log features...\n",
      "Creating expanding mean difference...\n",
      "Creating time-based rolling features (TRULY FIXED)...\n",
      "Creating intraday EWMA features...\n",
      "Creating lag features...\n",
      "Creating RSI features...\n",
      "Creating EWMA on time means...\n",
      "Creating XY combined features...\n",
      "Creating YX spread features...\n",
      "Feature engineering completed (NO DATA LEAKAGE - TRAIN)!\n",
      "----->Normalization....\n",
      "----->Validation....\n",
      "     train_50_percent  train_60_percent  train_70_percent  train_80_percent  \\\n",
      "mse          0.031237          0.021398          0.017521          0.017257   \n",
      "r2           0.593954          0.844598          0.033450          3.592538   \n",
      "\n",
      "     train_90_percent  min_stats  max_stats       avg  \n",
      "mse          0.020141   0.017257   0.031237  0.022293  \n",
      "r2           1.903203   0.033450   3.592538  1.513390  \n",
      "----->Fitting....\n",
      "----->Calculation feature importances...\n",
      "yprice_time_zscore_2hours                8.81%           0.027\n",
      "ylog_dayly_ewma_1hour                    6.89%           0.022\n",
      "yprice_lag_4hours                        6.35%           -0.02\n",
      "xdiff_from_closing                       5.85%          -0.018\n",
      "yprice_time_mean_2hours                  5.59%          -0.017\n",
      "yprice_time_mean_10min                   5.29%          -0.017\n",
      "xprice_time_mean_4hours                  5.14%           0.016\n",
      "yprice_dayly_ewma_10min                  5.11%          -0.016\n",
      "xy_geom_time_mean_2hours_dayly_ewma_20min 4.14%          -0.013\n",
      "xprice_lag_4hours                        4.13%          -0.013\n",
      "xy_square_time_zscore_10min              4.07%           0.013\n",
      "yprice_time_mean_10min_dayly_ewma_4hour  3.35%            0.01\n",
      "xprice_dayly_ewma_4hour                  3.10%          0.0097\n",
      "xy_garmonic_time_std_4hours              2.97%          0.0093\n",
      "is_end_of_week                           2.81%         -0.0088\n",
      "xdiff_from_opening                       2.51%          0.0078\n",
      "xprice_time_mean_10min_dayly_ewma_10min  2.41%         -0.0075\n",
      "xlog                                     1.98%          0.0062\n",
      "xprice_time_mean_1hour_lag_4hours        1.98%         -0.0062\n",
      "yprice_time_mean_1hour_lag_1workweek     1.71%         -0.0053\n",
      "xy_garmonic_ewma_prodpair_2hours_1hour   1.62%          -0.005\n",
      "yx_spread_time_zscore_4hours             1.55%         -0.0048\n",
      "yx_spread_time_mean_2hours_lag_20min     1.39%          0.0043\n",
      "yprice_ewma_difpair_10min_4hour          1.35%         -0.0042\n",
      "xprice_time_mean_1min_rsi_1min           1.35%          0.0042\n",
      "yprice_time_mean_1hour                   1.15%         -0.0036\n",
      "xy_geom_time_mean_1min                   1.06%         -0.0033\n",
      "weekday                                  1.06%          0.0033\n",
      "yprice_time_mean_10min_rsi_1hour         0.83%          0.0026\n",
      "xprice_time_mean_1min                    0.79%          0.0025\n",
      "yprice_time_mean_1hour_lag_4hours        0.73%          0.0023\n",
      "xlog_dayly_ewma_10min                    0.60%         -0.0019\n",
      "ydiff_from_closing                       0.52%          0.0016\n",
      "xy_geom_time_mean_1hour_lag_20min        0.46%          0.0014\n",
      "yx_spread_ewma_prodpair_1hour_10min      0.40%         -0.0012\n",
      "yprice_time_mean_10min_lag_10min         0.36%          0.0011\n",
      "xy_geom_time_mean_10min_dayly_ewma_1min  0.22%         0.00069\n",
      "yprice_time_zscore_1hour                 0.20%         0.00061\n",
      "yprice_expanding_mean_diff               0.13%         0.00039\n",
      "yx_spread_time_mean_10min_lag_1hour      0.03%         0.00011\n",
      "Model saved to model_params.pkl\n",
      "--------->Starting Forecasting....\n",
      "Loading file: /Users/mazin/Desktop/super prep materials/GSAPred/Two-financial-instruments/test.csv...\n",
      "Creating all features (NO LEAKAGE - TEST)...\n",
      "Creating opening/closing difference features...\n",
      "Creating log features...\n",
      "Creating expanding mean difference...\n",
      "Creating time-based rolling features (TRULY FIXED)...\n",
      "Creating intraday EWMA features...\n",
      "Creating lag features...\n",
      "Creating RSI features...\n",
      "Creating EWMA on time means...\n",
      "Creating XY combined features...\n",
      "Creating YX spread features...\n",
      "Feature engineering completed (NO DATA LEAKAGE - TEST)!\n",
      "----->Normalization....\n",
      "----->Prediction....\n",
      "----->Evaluation....\n",
      "Test MSE: 0.013088\n",
      "Test R-squared: 0.017374\n",
      "Test R-squared (x100): 1.74\n",
      "\n",
      "First 10 predictions: [-0.01406675 -0.01568708 -0.01604956 -0.01465867 -0.01944077 -0.02153634\n",
      " -0.02163859 -0.0216259  -0.00765319 -0.00888189]\n",
      "Total predictions: 32180\n",
      "\n",
      "Evaluation Metrics:\n",
      "MSE: 0.013088\n",
      "R-squared: 0.017374\n",
      "R-squared (x100): 1.74\n",
      "\n",
      "Predictions saved to predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from copy import copy\n",
    "import scipy\n",
    "import pickle\n",
    "\n",
    "# %% Configuration\n",
    "# Configuration parameters\n",
    "droprows = 7050\n",
    "std_reg_const = 0.1\n",
    "normalization_std_reg = 0.0001\n",
    "ridge_alpha = 1333\n",
    "\n",
    "selected_cols = [\n",
    "    \n",
    "    # Time-based features\n",
    "    'is_end_of_week',\n",
    "    'weekday',\n",
    "    \n",
    "    # Opening/Closing difference features\n",
    "    'xdiff_from_closing',\n",
    "    'xdiff_from_opening',\n",
    "    'ydiff_from_closing',\n",
    "    \n",
    "    # X-price features\n",
    "    'xlog',\n",
    "    'xlog_dayly_ewma_10min',\n",
    "    'xprice_dayly_ewma_4hour',\n",
    "    'xprice_lag_4hours',\n",
    "    'xprice_time_mean_4hours',\n",
    "    'xprice_time_mean_1hour_lag_4hours',\n",
    "    'xprice_time_mean_1min',\n",
    "    'xprice_time_mean_10min_dayly_ewma_10min',\n",
    "    'xprice_time_mean_1min_rsi_1min',\n",
    "    \n",
    "    # Y-price features\n",
    "    'ylog_dayly_ewma_1hour',\n",
    "    'yprice_dayly_ewma_10min',\n",
    "    'yprice_lag_4hours',\n",
    "    'yprice_ewma_difpair_10min_4hour',\n",
    "    'yprice_expanding_mean_diff', # FIXED: renamed from yprice_full_history_diff\n",
    "    'yprice_time_mean_1hour',\n",
    "    'yprice_time_mean_1hour_lag_4hours',\n",
    "    'yprice_time_mean_1hour_lag_1workweek',\n",
    "    'yprice_time_mean_10min',\n",
    "    'yprice_time_mean_10min_dayly_ewma_4hour',\n",
    "    'yprice_time_mean_10min_lag_10min',\n",
    "    'yprice_time_mean_10min_rsi_1hour',\n",
    "    'yprice_time_mean_2hours',\n",
    "    'yprice_time_zscore_1hour',\n",
    "    'yprice_time_zscore_2hours',\n",
    "    \n",
    "    # XY combined features\n",
    "    'xy_garmonic_ewma_prodpair_2hours_1hour',\n",
    "    'xy_garmonic_time_std_4hours',\n",
    "    'xy_geom_time_mean_1hour_lag_20min',\n",
    "    'xy_geom_time_mean_1min',\n",
    "    'xy_geom_time_mean_10min_dayly_ewma_1min',\n",
    "    'xy_geom_time_mean_2hours_dayly_ewma_20min',\n",
    "    'xy_square_time_zscore_10min',\n",
    "    \n",
    "    # YX spread features\n",
    "    'yx_spread_ewma_prodpair_1hour_10min',\n",
    "    'yx_spread_time_mean_10min_lag_1hour',\n",
    "    'yx_spread_time_mean_2hours_lag_20min',\n",
    "    'yx_spread_time_zscore_4hours',\n",
    "]\n",
    "\n",
    "\n",
    "# %% Helper Functions\n",
    "def print_importances(model, selected_cols):\n",
    "    \"\"\"Print feature importances sorted by absolute weight\"\"\"\n",
    "    weigts_sum = sum(map(abs, model.coef_))\n",
    "    for name, weight in sorted(zip(selected_cols, model.coef_), key=lambda x: -abs(x[1])):\n",
    "        percent_weight = abs(weight) / weigts_sum\n",
    "        print('{:40} {:.2%} {:15.2}'.format(name, percent_weight, weight))\n",
    "\n",
    "def rsquared(x, y):\n",
    "    \"\"\"Return R^2 where x and y are array-like.\"\"\"\n",
    "    slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(x, y)\n",
    "    return r_value**2\n",
    "\n",
    "def rsiFunc(prices, n=14):\n",
    "    \"\"\"Calculate RSI (Relative Strength Index)\"\"\"\n",
    "    deltas = np.diff(prices)\n",
    "    seed = deltas[:n+1]\n",
    "    up = seed[seed>=0].sum()/n\n",
    "    down = -seed[seed<0].sum()/n\n",
    "    rs = up/down\n",
    "    rsi = np.zeros_like(prices)\n",
    "    rsi[:n] = 100. - 100./(1.+rs)\n",
    "\n",
    "    for i in range(n, len(prices)):\n",
    "        delta = deltas[i-1]\n",
    "        if delta>0:\n",
    "            upval = delta\n",
    "            downval = 0.\n",
    "        else:\n",
    "            upval = 0.\n",
    "            downval = -delta\n",
    "\n",
    "        up = (up*(n-1) + upval)/n\n",
    "        down = (down*(n-1) + downval)/n\n",
    "        rs = up/down\n",
    "        rsi[i] = 100. - 100./(1.+rs)\n",
    "    return rsi\n",
    "\n",
    "# %% Time Series Validation Functions\n",
    "def time_split(data, valid_ratio, test_ratio):\n",
    "    \"\"\"Split time series data into train, validation, and test sets\"\"\"\n",
    "    n_valid = max(1, int(data.shape[0] * valid_ratio))\n",
    "    n_test = max(1, int(data.shape[0] * test_ratio))\n",
    "    n_train = data.shape[0] - n_valid - n_test\n",
    "    \n",
    "    train = data.iloc[:n_train].reset_index(drop=True).copy()\n",
    "    valid = data.iloc[n_train:-n_test].reset_index(drop=True).copy()\n",
    "    test = data.iloc[-n_test:].reset_index(drop=True).copy()\n",
    "    merged_test = pd.concat([valid, test], ignore_index=True)\n",
    "    return train, valid, test\n",
    "\n",
    "def validate_model_by_pentate(model, source_data, base_cols, droprows=0):\n",
    "    \"\"\"Validate model using 5-fold time series cross-validation\"\"\"\n",
    "    df = source_data.copy()\n",
    "    selected_cols = base_cols.copy()\n",
    "    helper_cols = list(set(selected_cols + ['periods_before_closing', 'returns']))\n",
    "    metrics_dict = {}\n",
    "    \n",
    "    for step in range(5, 10):\n",
    "        n_train = int(df.shape[0] * step // 10)\n",
    "        n_test = int(df.shape[0] * (step + 1) // 10)\n",
    "        train = df.iloc[:n_train].reset_index(drop=True).copy()\n",
    "        test = df.iloc[n_train:n_test].reset_index(drop=True).copy()\n",
    "        train.drop(np.arange(droprows), inplace=True)\n",
    "        train.dropna(inplace=True)\n",
    "\n",
    "        model.fit(train[selected_cols], train.returns)\n",
    "        predicted = model.predict(test[selected_cols])\n",
    "        predicted[test.periods_before_closing == 0] = 0\n",
    "\n",
    "        current_mse = mean_squared_error(test.returns, predicted)\n",
    "        current_r2 = rsquared(test.returns, predicted) * 100\n",
    "        metrics_dict['train_{}_percent'.format(step * 10)] = {\n",
    "            'mse': current_mse,\n",
    "            'r2': current_r2\n",
    "        }\n",
    "    \n",
    "    report = pd.DataFrame(metrics_dict)\n",
    "    report['min_stats'] = report.iloc[:,:5].min(1).astype(np.float32)\n",
    "    report['max_stats'] = report.iloc[:,:5].max(1).astype(np.float32)\n",
    "    report['avg'] = report.mean(1).astype(np.float32)\n",
    "    return report\n",
    "\n",
    "# %% TRULY FIXED Feature Engineering Function\n",
    "def create_all_features(data, is_train=True):\n",
    "    \"\"\"\n",
    "    TRULY FIXED: Feature engineering with NO data leakage.\n",
    "    All features use strictly backward-looking rolling windows.\n",
    "    Current observation at time t IS included (since we predict t+60).\n",
    "    No resample+ffill pattern that causes intra-bin leakage.\n",
    "    \"\"\"\n",
    "    print(f\"Creating all features (NO LEAKAGE - {'TRAIN' if is_train else 'TEST'})...\")\n",
    "    \n",
    "    # Pre-calculate commonly used values\n",
    "    days = data.day.unique()\n",
    "    \n",
    "    # Opening/Closing differences (using previous day's close)\n",
    "    print(\"Creating opening/closing difference features...\")\n",
    "    close_price_per_day_y = data.groupby('day').timestamp.max().shift(1).map(\n",
    "        data[['timestamp', 'yprice']].set_index('timestamp').yprice)\n",
    "    data['ydiff_from_closing'] = (data.yprice - data.day.map(close_price_per_day_y)).fillna(0)\n",
    "    \n",
    "    close_price_per_day_x = data.groupby('day').timestamp.max().shift(1).map(\n",
    "        data[['timestamp', 'xprice']].set_index('timestamp').xprice)\n",
    "    data['xdiff_from_closing'] = (data.xprice - data.day.map(close_price_per_day_x)).fillna(0)\n",
    "    \n",
    "    open_price_per_day_x = data.groupby('day').timestamp.min().map(\n",
    "        data[['timestamp', 'xprice']].set_index('timestamp').xprice)\n",
    "    data['xdiff_from_opening'] = data.xprice - data.day.map(open_price_per_day_x)\n",
    "    \n",
    "    # Log features\n",
    "    print(\"Creating log features...\")\n",
    "    data['xlog'] = data.xprice.apply(np.log1p)\n",
    "    data['ylog'] = data.yprice.apply(np.log1p)\n",
    "    \n",
    "    # Expanding mean that only uses past data (including current)\n",
    "    print(\"Creating expanding mean difference...\")\n",
    "    data['yprice_expanding_mean_diff'] = data['yprice'] - data['yprice'].expanding(min_periods=1).mean()\n",
    "    \n",
    "    # FIXED: Time-based rolling features with backward-looking windows INCLUDING current\n",
    "    print(\"Creating time-based rolling features (TRULY FIXED)...\")\n",
    "    time_windows = {\n",
    "        6: '1min', 60: '10min', 360: '1hour', 720: '2hours', 1410: '4hours', 2820: '1workweek'\n",
    "    }\n",
    "    \n",
    "    # Create rolling means with backward-looking windows INCLUDING current observation\n",
    "    for window, name in time_windows.items():\n",
    "        if window in [6, 60, 360, 1410]:  # xprice windows\n",
    "            colname = f'xprice_time_mean_{name}'\n",
    "            # Rolling includes current observation (no shift)\n",
    "            data[colname] = data['xprice'] - data['xprice'].rolling(\n",
    "                window=window, min_periods=1).mean()\n",
    "        \n",
    "        if window in [60, 360, 720]:  # yprice windows\n",
    "            colname = f'yprice_time_mean_{name}'\n",
    "            # Rolling includes current observation (no shift)\n",
    "            data[colname] = data['yprice'] - data['yprice'].rolling(\n",
    "                window=window, min_periods=1).mean()\n",
    "    \n",
    "    # Create rolling std for yprice with backward-looking windows INCLUDING current\n",
    "    for window in [360, 720]:\n",
    "        name = time_windows[window]\n",
    "        colname = f'yprice_time_std_{name}'\n",
    "        # Rolling includes current observation (no shift)\n",
    "        data[colname] = data['yprice'].rolling(\n",
    "            window=window, min_periods=1).std().fillna(0) + std_reg_const\n",
    "    \n",
    "    # Z-scores for yprice\n",
    "    data['yprice_time_zscore_1hour'] = data.yprice_time_mean_1hour / data.yprice_time_std_1hour\n",
    "    data['yprice_time_zscore_2hours'] = data.yprice_time_mean_2hours / data.yprice_time_std_2hours\n",
    "    \n",
    "    # Intraday EWMA features (backward-looking INCLUDING current)\n",
    "    print(\"Creating intraday EWMA features...\")\n",
    "    ewma_configs = [\n",
    "        ('xprice', [24], ['4hour']),\n",
    "        ('xlog', [60], ['10min']),\n",
    "        ('ylog', [360], ['1hour']),\n",
    "        ('yprice', [24, 60], ['4hour', '10min'])\n",
    "    ]\n",
    "    \n",
    "    for col, windows, names in ewma_configs:\n",
    "        for day in days:\n",
    "            df_mask = (data.day == day)\n",
    "            day_data = data.loc[df_mask].copy()\n",
    "            \n",
    "            for window, name in zip(windows, names):\n",
    "                colname = f'{col}_dayly_ewma_{name}'\n",
    "                # EWMA includes current observation (no shift)\n",
    "                ewm = day_data[col].ewm(halflife=window, adjust=False).mean()\n",
    "                \n",
    "                if col in ['xprice', 'yprice']:\n",
    "                    data.loc[df_mask, colname] = day_data[col] - ewm\n",
    "                elif col in ['xlog', 'ylog']:\n",
    "                    data.loc[df_mask, colname] = day_data[col.replace('log', 'price')] - ewm\n",
    "    \n",
    "    # EWMA difference pair\n",
    "    data['yprice_ewma_difpair_10min_4hour'] = data.yprice_dayly_ewma_10min - data.yprice_dayly_ewma_4hour\n",
    "    \n",
    "    # Lag features (inherently backward-looking, excludes current)\n",
    "    print(\"Creating lag features...\")\n",
    "    lag_configs = [\n",
    "        ('xprice', 1410, '4hours'),\n",
    "        ('yprice', 1410, '4hours'),\n",
    "        ('xprice_time_mean_1hour', 1410, '4hours'),\n",
    "        ('yprice_time_mean_1hour', 1410, '4hours'),\n",
    "        ('yprice_time_mean_1hour', 2820, '1workweek'),\n",
    "        ('yprice_time_mean_10min', 60, '10min')\n",
    "    ]\n",
    "    \n",
    "    for col, lag, name in lag_configs:\n",
    "        data[f'{col}_lag_{name}'] = data[col].shift(lag)\n",
    "    \n",
    "    # RSI features (calculated on backward-looking data including current)\n",
    "    print(\"Creating RSI features...\")\n",
    "    # RSI needs some history, so we calculate it on the rolling means\n",
    "    if 'xprice_time_mean_1min' in data.columns:\n",
    "        data['xprice_time_mean_1min_rsi_1min'] = rsiFunc(data['xprice_time_mean_1min'].fillna(0).values, 6)\n",
    "    data['yprice_time_mean_10min_rsi_1hour'] = rsiFunc(data['yprice_time_mean_10min'].fillna(0).values, 360)\n",
    "    \n",
    "    # Additional EWMA on time means\n",
    "    print(\"Creating EWMA on time means...\")\n",
    "    ewma_on_means = [\n",
    "        ('xprice_time_mean_10min', 60, '10min'),\n",
    "        ('yprice_time_mean_10min', 24, '4hour')\n",
    "    ]\n",
    "    \n",
    "    for col, window, name in ewma_on_means:\n",
    "        for day in days:\n",
    "            df_mask = (data.day == day)\n",
    "            day_data = data.loc[df_mask].copy()\n",
    "            colname = f'{col}_dayly_ewma_{name}'\n",
    "            # EWMA includes current observation (no shift)\n",
    "            ewm = day_data[col].ewm(halflife=window, adjust=False).mean()\n",
    "            data.loc[df_mask, colname] = ewm\n",
    "    \n",
    "    # XY Combined features with backward-looking windows INCLUDING current\n",
    "    print(\"Creating XY combined features...\")\n",
    "    \n",
    "    # XY Harmonic std - use rolling window INCLUDING current\n",
    "    data['xy_garmonic_time_std_4hours'] = (\n",
    "        data['xy_garmonic'].rolling(window=1410, min_periods=1).std().fillna(0) + std_reg_const\n",
    "    )\n",
    "    \n",
    "    # Harmonic EWMA\n",
    "    for day in days:\n",
    "        df_mask = (data.day == day)\n",
    "        day_data = data.loc[df_mask].copy()\n",
    "        \n",
    "        # EWMA includes current observation (no shift)\n",
    "        data.loc[df_mask, 'xy_garmonic_dayly_ewma_1hour'] = (\n",
    "            day_data['xy_garmonic'].ewm(halflife=360, adjust=False).mean()\n",
    "        )\n",
    "        data.loc[df_mask, 'xy_garmonic_dayly_ewma_2hours'] = (\n",
    "            day_data['xy_garmonic'].ewm(halflife=720, adjust=False).mean()\n",
    "        )\n",
    "    \n",
    "    data['xy_garmonic_dayly_ewma_1hour'] = data['xy_garmonic'] - data['xy_garmonic_dayly_ewma_1hour']\n",
    "    data['xy_garmonic_dayly_ewma_2hours'] = data['xy_garmonic'] - data['xy_garmonic_dayly_ewma_2hours']\n",
    "    data['xy_garmonic_ewma_prodpair_2hours_1hour'] = (\n",
    "        data.xy_garmonic_dayly_ewma_2hours * data.xy_garmonic_dayly_ewma_1hour)\n",
    "    \n",
    "    # XY Geometric features with rolling windows INCLUDING current\n",
    "    for window, name in [(6, '1min'), (60, '10min'), (360, '1hour'), (720, '2hours')]:\n",
    "        colname = f'xy_geom_time_mean_{name}'\n",
    "        # Rolling includes current observation (no shift)\n",
    "        data[colname] = data['xy_geom'] - data['xy_geom'].rolling(\n",
    "            window=window, min_periods=1).mean()\n",
    "    \n",
    "    # Geometric lags and EWMA\n",
    "    data['xy_geom_time_mean_1hour_lag_20min'] = data['xy_geom_time_mean_1hour'].shift(120)\n",
    "    \n",
    "    for day in days:\n",
    "        df_mask = (data.day == day)\n",
    "        day_data = data.loc[df_mask].copy()\n",
    "        \n",
    "        # EWMA includes current observation (no shift)\n",
    "        data.loc[df_mask, 'xy_geom_time_mean_10min_dayly_ewma_1min'] = (\n",
    "            day_data['xy_geom_time_mean_10min'].ewm(halflife=6, adjust=False).mean()\n",
    "        )\n",
    "        data.loc[df_mask, 'xy_geom_time_mean_2hours_dayly_ewma_20min'] = (\n",
    "            day_data['xy_geom_time_mean_2hours'].ewm(halflife=120, adjust=False).mean()\n",
    "        )\n",
    "    \n",
    "    # XY Relation std with rolling windows INCLUDING current\n",
    "    for window, name in [(360, '1hour'), (720, '2hours')]:\n",
    "        colname = f'xy_relation_time_std_{name}'\n",
    "        data[colname] = (\n",
    "            data['xy_relation'].rolling(window=window, min_periods=1).std().fillna(0) + std_reg_const\n",
    "        )\n",
    "    \n",
    "    # XY Square zscore with rolling windows INCLUDING current\n",
    "    data['xy_square_time_mean_10min'] = (\n",
    "        data['xy_square'] - data['xy_square'].rolling(window=60, min_periods=1).mean()\n",
    "    )\n",
    "    data['xy_square_time_std_10min'] = (\n",
    "        data['xy_square'].rolling(window=60, min_periods=1).std().fillna(0) + std_reg_const\n",
    "    )\n",
    "    data['xy_square_time_zscore_10min'] = data['xy_square_time_mean_10min'] / data['xy_square_time_std_10min']\n",
    "    \n",
    "    # YX Spread features with rolling windows INCLUDING current\n",
    "    print(\"Creating YX spread features...\")\n",
    "    for window, name in [(60, '10min'), (720, '2hours'), (1410, '4hours')]:\n",
    "        colname = f'yx_spread_time_mean_{name}'\n",
    "        data[colname] = (\n",
    "            data['yx_spread'] - data['yx_spread'].rolling(window=window, min_periods=1).mean()\n",
    "        )\n",
    "    \n",
    "    # Spread std and zscore with rolling windows INCLUDING current\n",
    "    data['yx_spread_time_std_4hours'] = (\n",
    "        data['yx_spread'].rolling(window=1410, min_periods=1).std().fillna(0) + std_reg_const\n",
    "    )\n",
    "    data['yx_spread_time_zscore_4hours'] = data['yx_spread_time_mean_4hours'] / data['yx_spread_time_std_4hours']\n",
    "    \n",
    "    # Spread lags (excluding current)\n",
    "    data['yx_spread_time_mean_10min_lag_1hour'] = data['yx_spread_time_mean_10min'].shift(360)\n",
    "    data['yx_spread_time_mean_2hours_lag_20min'] = data['yx_spread_time_mean_2hours'].shift(120)\n",
    "    \n",
    "    # Spread EWMA INCLUDING current\n",
    "    for day in days:\n",
    "        df_mask = (data.day == day)\n",
    "        day_data = data.loc[df_mask].copy()\n",
    "        \n",
    "        # EWMA includes current observation (no shift)\n",
    "        data.loc[df_mask, 'yx_spread_dayly_ewma_10min'] = (\n",
    "            day_data['yx_spread'].ewm(halflife=60, adjust=False).mean()\n",
    "        )\n",
    "        data.loc[df_mask, 'yx_spread_dayly_ewma_1hour'] = (\n",
    "            day_data['yx_spread'].ewm(halflife=360, adjust=False).mean()\n",
    "        )\n",
    "    \n",
    "    data['yx_spread_dayly_ewma_10min'] = data['yx_spread'] - data['yx_spread_dayly_ewma_10min']\n",
    "    data['yx_spread_dayly_ewma_1hour'] = data['yx_spread'] - data['yx_spread_dayly_ewma_1hour']\n",
    "    data['yx_spread_ewma_prodpair_1hour_10min'] = (\n",
    "        data.yx_spread_dayly_ewma_1hour * data.yx_spread_dayly_ewma_10min)\n",
    "    \n",
    "    # Clean up temporary columns\n",
    "    temp_cols = [\n",
    "        'xy_garmonic_dayly_ewma_1hour', 'xy_garmonic_dayly_ewma_2hours',\n",
    "        'xy_square_time_mean_10min', 'xy_square_time_std_10min',\n",
    "        'yx_spread_dayly_ewma_10min', 'yx_spread_dayly_ewma_1hour',\n",
    "        'yprice_time_std_1hour', 'yprice_time_std_2hours',\n",
    "        'yx_spread_time_std_4hours', 'xy_relation_time_std_1hour',\n",
    "        'xy_relation_time_std_2hours'\n",
    "    ]\n",
    "    \n",
    "    for col in temp_cols:\n",
    "        if col in data.columns:\n",
    "            data.drop(col, axis=1, inplace=True)\n",
    "    \n",
    "    print(f\"Feature engineering completed (NO DATA LEAKAGE - {'TRAIN' if is_train else 'TEST'})!\")\n",
    "    return data\n",
    "\n",
    "# %% Data Initialization\n",
    "def init_data_single(fname):\n",
    "    \"\"\"Initialize and preprocess a single dataset\"\"\"\n",
    "    print(f'Loading file: {fname}...')\n",
    "    data = pd.read_csv(fname)\n",
    "    \n",
    "    data['xprice'] -= 127  # WARNING: Domain-specific adjustment\n",
    "    data['yprice'] -= 146  # WARNING: Domain-specific adjustment\n",
    "    \n",
    "    # Create derived price features\n",
    "    data['yx_spread'] = data.yprice - data.xprice\n",
    "    data['yx_relation'] = data.yprice / data.xprice\n",
    "    data['xy_relation'] = data.xprice / data.yprice\n",
    "    data['xy_square'] = np.sqrt(data.xprice ** 2 + data.yprice ** 2) / 2\n",
    "    data['xy_geom'] = np.sqrt(data.xprice * data.yprice)\n",
    "    data['xy_garmonic'] = 2 / (1 / data.xprice + 1 / data.yprice)\n",
    "    \n",
    "    # Process timestamps\n",
    "    data['timestamp'] = data['timestamp'] // 1000\n",
    "    data['timestamp'] = data['timestamp'].apply(lambda stamp: datetime.fromtimestamp(stamp))\n",
    "    data['timestamp'] = data['timestamp'] - pd.Timedelta(hours=1)\n",
    "    data.index = data['timestamp']\n",
    "    \n",
    "    # Add time-based features\n",
    "    data['weekday'] = data.timestamp.dt.weekday\n",
    "    data['is_end_of_week'] = (data.timestamp.dt.weekday >= 2).astype(int)\n",
    "    \n",
    "    data['day'] = (data.timestamp.dt.date - data.timestamp.dt.date.min()).apply(lambda x: int(x.days))\n",
    "    day_close_time = data.day.map(data.groupby('day').timestamp.max())\n",
    "    data['periods_before_closing'] = (day_close_time - data.timestamp).apply(lambda x: x.seconds // 10)\n",
    "    day_open_time = data.day.map(data.groupby('day').timestamp.min())\n",
    "    data['periods_after_opening'] = (data.timestamp - day_open_time).apply(lambda x: x.seconds // 10)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def selected_features_extractor_separate(train_data_path, test_data_path):\n",
    "    \"\"\"\n",
    "    Extract features separately for train and test to avoid leakage.\n",
    "    This is the SAFE approach - features are computed independently.\n",
    "    \n",
    "    Parameters:\n",
    "    train_data_path: Path to training data (can be None if only processing test)\n",
    "    test_data_path: Path to test data (can be None if only processing train)\n",
    "    \"\"\"\n",
    "    train = None\n",
    "    test = None\n",
    "    \n",
    "    # Process training data if provided\n",
    "    if train_data_path is not None:\n",
    "        train_data = init_data_single(train_data_path)\n",
    "        train_data = create_all_features(train_data, is_train=True)\n",
    "        \n",
    "        usecols = selected_cols + ['returns', 'periods_before_closing']\n",
    "        train = train_data[usecols].iloc[droprows:].copy()\n",
    "    \n",
    "    # Process test data if provided\n",
    "    if test_data_path is not None:\n",
    "        test_data = init_data_single(test_data_path)\n",
    "        test_data = create_all_features(test_data, is_train=False)\n",
    "        \n",
    "        # Test might not have 'returns' column\n",
    "        test_cols = selected_cols + ['periods_before_closing']\n",
    "        if 'returns' in test_data.columns:\n",
    "            test_cols.append('returns')\n",
    "        test = test_data[test_cols].copy()\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "# %% Model Training Functions\n",
    "def normalize_train(df):\n",
    "    \"\"\"Normalize training data\"\"\"\n",
    "    extended_cols = selected_cols + ['returns', 'periods_before_closing']\n",
    "    norm_train = df[extended_cols].reset_index(drop=True).copy()\n",
    "    norm_mean = norm_train[selected_cols].mean()\n",
    "    norm_std = norm_train[selected_cols].std() + normalization_std_reg\n",
    "    norm_train.loc[:,selected_cols] = (norm_train[selected_cols] - norm_mean) / norm_std\n",
    "    return norm_train\n",
    "\n",
    "def normalize_train_test(train, test):\n",
    "    \"\"\"Normalize train and test data using training statistics\"\"\"\n",
    "    train_extended_cols = selected_cols + ['returns', 'periods_before_closing']\n",
    "    test_extended_cols = selected_cols + ['periods_before_closing']\n",
    "    \n",
    "    norm_train = train[train_extended_cols].reset_index(drop=True).copy()\n",
    "    norm_test = test[test_extended_cols].reset_index(drop=True).copy()\n",
    "    \n",
    "    norm_mean = norm_train[selected_cols].mean()\n",
    "    norm_std = norm_train[selected_cols].std() + normalization_std_reg\n",
    "    \n",
    "    norm_train.loc[:,selected_cols] = (norm_train[selected_cols] - norm_mean) / norm_std\n",
    "    norm_test.loc[:,selected_cols] = (norm_test[selected_cols] - norm_mean) / norm_std\n",
    "    return norm_train, norm_test\n",
    "\n",
    "# %% Model Estimation Function\n",
    "def modelEstimate(train_data_path):\n",
    "    \"\"\"\n",
    "    Train the model on the training data.\n",
    "    \n",
    "    Parameters:\n",
    "    train_data_path (str): Path to the training CSV file\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary containing the trained model and normalization parameters\n",
    "    \"\"\"\n",
    "    print('--------->Start Estimation....')\n",
    "    \n",
    "    # Extract features (train only, no test contamination)\n",
    "    train, _ = selected_features_extractor_separate(train_data_path, None)\n",
    "    \n",
    "    # Remove any rows with NaN values\n",
    "    train = train.dropna()\n",
    "    \n",
    "    # Normalize training data\n",
    "    print('----->Normalization....')\n",
    "    norm_train = normalize_train(train)\n",
    "    \n",
    "    # Calculate normalization parameters for later use\n",
    "    norm_mean = train[selected_cols].mean()\n",
    "    norm_std = train[selected_cols].std() + normalization_std_reg\n",
    "    \n",
    "    # Validate model\n",
    "    print('----->Validation....')\n",
    "    model = Ridge(alpha=ridge_alpha)\n",
    "    print(validate_model_by_pentate(model, norm_train, selected_cols, 0))\n",
    "    \n",
    "    # Fit final model\n",
    "    print('----->Fitting....')\n",
    "    model = Ridge(alpha=ridge_alpha)\n",
    "    model.fit(norm_train[selected_cols], norm_train.returns)\n",
    "    \n",
    "    # Print feature importances\n",
    "    print('----->Calculation feature importances...')\n",
    "    print_importances(model, selected_cols)\n",
    "    \n",
    "    # Return model and parameters\n",
    "    return {\n",
    "        'model': model,\n",
    "        'norm_mean': norm_mean,\n",
    "        'norm_std': norm_std\n",
    "    }\n",
    "\n",
    "# %% Model Forecast Function\n",
    "def modelForecast(test_data_path, model_params, train_data_path=None):\n",
    "    \"\"\"\n",
    "    Make predictions on test data using the trained model.\n",
    "    \n",
    "    Parameters:\n",
    "    test_data_path (str): Path to the test CSV file\n",
    "    model_params (dict): Dictionary containing the trained model and normalization parameters\n",
    "    train_data_path (str): Not used in this version (features computed separately)\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary containing predictions and evaluation metrics (if test data has returns)\n",
    "    \"\"\"\n",
    "    print('--------->Starting Forecasting....')\n",
    "    \n",
    "    # Extract features for test only (no train contamination)\n",
    "    _, test = selected_features_extractor_separate(None, test_data_path)\n",
    "    \n",
    "    # Remove any rows with NaN values\n",
    "    test = test.dropna()\n",
    "    \n",
    "    # Normalize test data using training statistics\n",
    "    print('----->Normalization....')\n",
    "    test_extended_cols = selected_cols + ['periods_before_closing']\n",
    "    \n",
    "    # Check if test data has returns column for evaluation\n",
    "    has_returns = 'returns' in test.columns\n",
    "    if has_returns:\n",
    "        test_extended_cols.append('returns')\n",
    "    \n",
    "    norm_test = test[test_extended_cols].reset_index(drop=True).copy()\n",
    "    norm_test.loc[:,selected_cols] = (norm_test[selected_cols] - model_params['norm_mean']) / model_params['norm_std']\n",
    "    \n",
    "    # Make predictions\n",
    "    print('----->Prediction....')\n",
    "    predicted = model_params['model'].predict(norm_test[selected_cols])\n",
    "    \n",
    "    # Set predictions to 0 at closing periods\n",
    "    predicted[norm_test.periods_before_closing == 0] = 0\n",
    "    \n",
    "    # Calculate evaluation metrics if test data has returns\n",
    "    results = {'predictions': predicted}\n",
    "    \n",
    "    if has_returns:\n",
    "        print('----->Evaluation....')\n",
    "        # Calculate MSE\n",
    "        mse = mean_squared_error(norm_test.returns, predicted)\n",
    "        # Calculate R-squared\n",
    "        r2 = rsquared(norm_test.returns, predicted)\n",
    "        \n",
    "        results['mse'] = mse\n",
    "        results['r2'] = r2\n",
    "        \n",
    "        print(f'Test MSE: {mse:.6f}')\n",
    "        print(f'Test R-squared: {r2:.6f}')\n",
    "        print(f'Test R-squared (x100): {r2*100:.2f}')\n",
    "    else:\n",
    "        print('No returns column in test data - evaluation metrics not calculated')\n",
    "    \n",
    "    return results\n",
    "\n",
    "# %% Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Train the model\n",
    "    train_file_path = '/Users/mazin/Desktop/super prep materials/GSAPred/Two-financial-instruments/train.csv'  # Replace with your actual file path\n",
    "    model_params = modelEstimate(train_file_path)\n",
    "    \n",
    "    # Save model for later use\n",
    "    with open('model_params.pkl', 'wb') as f:\n",
    "        pickle.dump(model_params, f)\n",
    "    print(\"Model saved to model_params.pkl\")\n",
    "    \n",
    "    # Make predictions on test data\n",
    "    test_file_path = '/Users/mazin/Desktop/super prep materials/GSAPred/Two-financial-instruments/test.csv'  # Replace with your actual file path\n",
    "    \n",
    "    # Load model if needed\n",
    "    # with open('model_params.pkl', 'rb') as f:\n",
    "    #     model_params = pickle.load(f)\n",
    "    \n",
    "    results = modelForecast(test_file_path, model_params)\n",
    "    \n",
    "    # Extract predictions\n",
    "    predictions = results['predictions']\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\nFirst 10 predictions: {predictions[:10]}\")\n",
    "    print(f\"Total predictions: {len(predictions)}\")\n",
    "    \n",
    "    # Display evaluation metrics if available\n",
    "    if 'mse' in results:\n",
    "        print(f\"\\nEvaluation Metrics:\")\n",
    "        print(f\"MSE: {results['mse']:.6f}\")\n",
    "        print(f\"R-squared: {results['r2']:.6f}\")\n",
    "        print(f\"R-squared (x100): {results['r2']*100:.2f}\")\n",
    "    \n",
    "    # Save predictions to CSV\n",
    "    predictions_df = pd.DataFrame({'predictions': predictions})\n",
    "    predictions_df.to_csv('predictions.csv', index=False)\n",
    "    print(\"\\nPredictions saved to predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab50b9f-af56-461f-8302-9e0480f8442a",
   "metadata": {},
   "source": [
    "# XGBoost with all features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8cf862fc-37ff-427b-bbba-c5e5cf37a82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Training XGBoost Model for Financial Time Series Prediction\n",
      "======================================================================\n",
      "--------->Start Estimation with XGBoost....\n",
      "Loading file: /Users/mazin/Desktop/super prep materials/GSAPred/Two-financial-instruments/train.csv...\n",
      "Creating all features (NO LEAKAGE - TRAIN)...\n",
      "Creating opening/closing difference features...\n",
      "Creating log features...\n",
      "Creating expanding mean difference...\n",
      "Creating time-based rolling features (TRULY FIXED)...\n",
      "Creating intraday EWMA features...\n",
      "Creating lag features...\n",
      "Creating RSI features...\n",
      "Creating EWMA on time means...\n",
      "Creating XY combined features...\n",
      "Creating YX spread features...\n",
      "Feature engineering completed (NO DATA LEAKAGE - TRAIN)!\n",
      "----->Normalization....\n",
      "----->Validation with XGBoost....\n",
      "     train_50_percent  train_60_percent  train_70_percent  train_80_percent  \\\n",
      "mse          0.037720          0.023819          0.019280          0.035759   \n",
      "r2           1.266047          1.117368          0.000252          0.174544   \n",
      "\n",
      "     train_90_percent  min_stats  max_stats       avg  \n",
      "mse          0.020948   0.019280   0.037720  0.027790  \n",
      "r2           1.866351   0.000252   1.866351  0.898738  \n",
      "----->Fitting XGBoost model....\n",
      "Using XGBoost parameters: {'objective': 'reg:squarederror', 'learning_rate': 0.05, 'n_estimators': 500, 'max_depth': 5, 'min_child_weight': 6, 'gamma': 0.1, 'subsample': 0.8, 'colsample_bytree': 0.8, 'reg_alpha': 0.1, 'reg_lambda': 1.0, 'random_state': 42}\n",
      "Best iteration: 136\n",
      "Best score: 0.142278\n",
      "----->Calculating feature importances (XGBoost)...\n",
      "\n",
      "Feature Importances (XGBoost):\n",
      "----------------------------------------------------------------------\n",
      "yprice_expanding_mean_diff               4.19%        0.041879\n",
      "yprice_lag_4hours                        3.89%        0.038933\n",
      "xprice_dayly_ewma_4hour                  3.50%        0.035006\n",
      "xprice_lag_4hours                        3.48%        0.034839\n",
      "xprice_time_mean_10min_dayly_ewma_10min  3.35%        0.033538\n",
      "xy_garmonic_time_std_4hours              3.30%        0.033018\n",
      "xprice_time_mean_4hours                  3.22%        0.032213\n",
      "ylog_dayly_ewma_1hour                    3.18%        0.031755\n",
      "xlog                                     3.11%        0.031060\n",
      "yprice_time_mean_2hours                  3.03%        0.030284\n",
      "xlog_dayly_ewma_10min                    2.99%        0.029920\n",
      "yx_spread_time_mean_2hours_lag_20min     2.94%        0.029422\n",
      "yprice_time_zscore_2hours                2.91%        0.029148\n",
      "xy_geom_time_mean_1hour_lag_20min        2.79%        0.027910\n",
      "xdiff_from_opening                       2.79%        0.027908\n",
      "xy_geom_time_mean_2hours_dayly_ewma_20min 2.75%        0.027504\n",
      "xdiff_from_closing                       2.73%        0.027309\n",
      "yx_spread_time_zscore_4hours             2.67%        0.026707\n",
      "yprice_time_mean_10min_dayly_ewma_4hour  2.63%        0.026295\n",
      "yprice_time_mean_1hour_lag_4hours        2.63%        0.026270\n",
      "is_end_of_week                           2.61%        0.026069\n",
      "yprice_ewma_difpair_10min_4hour          2.61%        0.026056\n",
      "yprice_time_mean_1hour_lag_1workweek     2.54%        0.025432\n",
      "xy_garmonic_ewma_prodpair_2hours_1hour   2.51%        0.025092\n",
      "yprice_time_mean_1hour                   2.42%        0.024175\n",
      "yprice_dayly_ewma_10min                  2.39%        0.023926\n",
      "xprice_time_mean_1hour_lag_4hours        2.35%        0.023532\n",
      "ydiff_from_closing                       2.34%        0.023442\n",
      "weekday                                  2.29%        0.022858\n",
      "yprice_time_zscore_1hour                 2.13%        0.021297\n",
      "yx_spread_ewma_prodpair_1hour_10min      2.06%        0.020647\n",
      "xy_geom_time_mean_10min_dayly_ewma_1min  1.87%        0.018693\n",
      "yx_spread_time_mean_10min_lag_1hour      1.85%        0.018499\n",
      "yprice_time_mean_10min                   1.58%        0.015761\n",
      "yprice_time_mean_10min_lag_10min         1.31%        0.013089\n",
      "xy_geom_time_mean_1min                   1.28%        0.012848\n",
      "yprice_time_mean_10min_rsi_1hour         1.19%        0.011908\n",
      "xprice_time_mean_1min_rsi_1min           1.09%        0.010877\n",
      "xy_square_time_zscore_10min              1.08%        0.010765\n",
      "xprice_time_mean_1min                    0.41%        0.004118\n",
      "\n",
      "Model saved to xgboost_model_params.pkl\n",
      "\n",
      "======================================================================\n",
      "Making Predictions on Test Data\n",
      "======================================================================\n",
      "--------->Starting Forecasting with XGBoost....\n",
      "Loading file: /Users/mazin/Desktop/super prep materials/GSAPred/Two-financial-instruments/test.csv...\n",
      "Creating all features (NO LEAKAGE - TEST)...\n",
      "Creating opening/closing difference features...\n",
      "Creating log features...\n",
      "Creating expanding mean difference...\n",
      "Creating time-based rolling features (TRULY FIXED)...\n",
      "Creating intraday EWMA features...\n",
      "Creating lag features...\n",
      "Creating RSI features...\n",
      "Creating EWMA on time means...\n",
      "Creating XY combined features...\n",
      "Creating YX spread features...\n",
      "Feature engineering completed (NO DATA LEAKAGE - TEST)!\n",
      "----->Normalization....\n",
      "----->Prediction with XGBoost....\n",
      "----->Evaluation....\n",
      "Test MSE: 0.013206\n",
      "Test R-squared: 0.010321\n",
      "Test R-squared (x100): 1.03\n",
      "\n",
      "First 10 predictions: [0.00568232 0.00581567 0.00547574 0.00547574 0.00954241 0.00850167\n",
      " 0.00879716 0.00879716 0.01549012 0.01549012]\n",
      "Total predictions: 32180\n",
      "\n",
      "Evaluation Metrics:\n",
      "MSE: 0.013206\n",
      "R-squared: 0.010321\n",
      "R-squared (x100): 1.03\n",
      "\n",
      "Predictions saved to xgboost_predictions.csv\n",
      "\n",
      "======================================================================\n",
      "Top 10 Most Important Features:\n",
      "======================================================================\n",
      " 1. yprice_expanding_mean_diff               0.041879\n",
      " 2. yprice_lag_4hours                        0.038933\n",
      " 3. xprice_dayly_ewma_4hour                  0.035006\n",
      " 4. xprice_lag_4hours                        0.034839\n",
      " 5. xprice_time_mean_10min_dayly_ewma_10min  0.033538\n",
      " 6. xy_garmonic_time_std_4hours              0.033018\n",
      " 7. xprice_time_mean_4hours                  0.032213\n",
      " 8. ylog_dayly_ewma_1hour                    0.031755\n",
      " 9. xlog                                     0.031060\n",
      "10. yprice_time_mean_2hours                  0.030284\n",
      "\n",
      "======================================================================\n",
      "XGBoost Model Training and Prediction Complete!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from copy import copy\n",
    "import scipy\n",
    "import pickle\n",
    "\n",
    "# %% Configuration\n",
    "# Configuration parameters\n",
    "droprows = 7050\n",
    "std_reg_const = 0.1\n",
    "normalization_std_reg = 0.0001\n",
    "\n",
    "# XGBoost parameters\n",
    "xgb_params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'learning_rate': 0.05,\n",
    "    'n_estimators': 500,\n",
    "    'max_depth': 5,\n",
    "    'min_child_weight': 6,\n",
    "    'gamma': 0.1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 1.0,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "selected_cols = [\n",
    "    \n",
    "    # Time-based features\n",
    "    'is_end_of_week',\n",
    "    'weekday',\n",
    "    \n",
    "    # Opening/Closing difference features\n",
    "    'xdiff_from_closing',\n",
    "    'xdiff_from_opening',\n",
    "    'ydiff_from_closing',\n",
    "    \n",
    "    # X-price features\n",
    "    'xlog',\n",
    "    'xlog_dayly_ewma_10min',\n",
    "    'xprice_dayly_ewma_4hour',\n",
    "    'xprice_lag_4hours',\n",
    "    'xprice_time_mean_4hours',\n",
    "    'xprice_time_mean_1hour_lag_4hours',\n",
    "    'xprice_time_mean_1min',\n",
    "    'xprice_time_mean_10min_dayly_ewma_10min',\n",
    "    'xprice_time_mean_1min_rsi_1min',\n",
    "    \n",
    "    # Y-price features\n",
    "    'ylog_dayly_ewma_1hour',\n",
    "    'yprice_dayly_ewma_10min',\n",
    "    'yprice_lag_4hours',\n",
    "    'yprice_ewma_difpair_10min_4hour',\n",
    "    'yprice_expanding_mean_diff', # FIXED: renamed from yprice_full_history_diff\n",
    "    'yprice_time_mean_1hour',\n",
    "    'yprice_time_mean_1hour_lag_4hours',\n",
    "    'yprice_time_mean_1hour_lag_1workweek',\n",
    "    'yprice_time_mean_10min',\n",
    "    'yprice_time_mean_10min_dayly_ewma_4hour',\n",
    "    'yprice_time_mean_10min_lag_10min',\n",
    "    'yprice_time_mean_10min_rsi_1hour',\n",
    "    'yprice_time_mean_2hours',\n",
    "    'yprice_time_zscore_1hour',\n",
    "    'yprice_time_zscore_2hours',\n",
    "    \n",
    "    # XY combined features\n",
    "    'xy_garmonic_ewma_prodpair_2hours_1hour',\n",
    "    'xy_garmonic_time_std_4hours',\n",
    "    'xy_geom_time_mean_1hour_lag_20min',\n",
    "    'xy_geom_time_mean_1min',\n",
    "    'xy_geom_time_mean_10min_dayly_ewma_1min',\n",
    "    'xy_geom_time_mean_2hours_dayly_ewma_20min',\n",
    "    'xy_square_time_zscore_10min',\n",
    "    \n",
    "    # YX spread features\n",
    "    'yx_spread_ewma_prodpair_1hour_10min',\n",
    "    'yx_spread_time_mean_10min_lag_1hour',\n",
    "    'yx_spread_time_mean_2hours_lag_20min',\n",
    "    'yx_spread_time_zscore_4hours',\n",
    "]\n",
    "\n",
    "\n",
    "# %% Helper Functions\n",
    "def print_importances_xgb(model, selected_cols):\n",
    "    \"\"\"Print feature importances for XGBoost model sorted by importance\"\"\"\n",
    "    feature_importance = model.feature_importances_\n",
    "    importance_dict = dict(zip(selected_cols, feature_importance))\n",
    "    \n",
    "    # Sort by importance\n",
    "    sorted_features = sorted(importance_dict.items(), key=lambda x: -x[1])\n",
    "    \n",
    "    # Calculate percentage weights\n",
    "    total_importance = sum(feature_importance)\n",
    "    \n",
    "    print(\"\\nFeature Importances (XGBoost):\")\n",
    "    print(\"-\" * 70)\n",
    "    for name, importance in sorted_features:\n",
    "        percent_importance = importance / total_importance if total_importance > 0 else 0\n",
    "        print('{:40} {:.2%} {:15.6f}'.format(name, percent_importance, importance))\n",
    "\n",
    "def rsquared(x, y):\n",
    "    \"\"\"Return R^2 where x and y are array-like.\"\"\"\n",
    "    slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(x, y)\n",
    "    return r_value**2\n",
    "\n",
    "def rsiFunc(prices, n=14):\n",
    "    \"\"\"Calculate RSI (Relative Strength Index)\"\"\"\n",
    "    deltas = np.diff(prices)\n",
    "    seed = deltas[:n+1]\n",
    "    up = seed[seed>=0].sum()/n\n",
    "    down = -seed[seed<0].sum()/n\n",
    "    rs = up/down\n",
    "    rsi = np.zeros_like(prices)\n",
    "    rsi[:n] = 100. - 100./(1.+rs)\n",
    "\n",
    "    for i in range(n, len(prices)):\n",
    "        delta = deltas[i-1]\n",
    "        if delta>0:\n",
    "            upval = delta\n",
    "            downval = 0.\n",
    "        else:\n",
    "            upval = 0.\n",
    "            downval = -delta\n",
    "\n",
    "        up = (up*(n-1) + upval)/n\n",
    "        down = (down*(n-1) + downval)/n\n",
    "        rs = up/down\n",
    "        rsi[i] = 100. - 100./(1.+rs)\n",
    "    return rsi\n",
    "\n",
    "# %% Time Series Validation Functions\n",
    "def time_split(data, valid_ratio, test_ratio):\n",
    "    \"\"\"Split time series data into train, validation, and test sets\"\"\"\n",
    "    n_valid = max(1, int(data.shape[0] * valid_ratio))\n",
    "    n_test = max(1, int(data.shape[0] * test_ratio))\n",
    "    n_train = data.shape[0] - n_valid - n_test\n",
    "    \n",
    "    train = data.iloc[:n_train].reset_index(drop=True).copy()\n",
    "    valid = data.iloc[n_train:-n_test].reset_index(drop=True).copy()\n",
    "    test = data.iloc[-n_test:].reset_index(drop=True).copy()\n",
    "    merged_test = pd.concat([valid, test], ignore_index=True)\n",
    "    return train, valid, test\n",
    "\n",
    "def validate_model_by_pentate(model, source_data, base_cols, droprows=0):\n",
    "    \"\"\"Validate model using 5-fold time series cross-validation\"\"\"\n",
    "    df = source_data.copy()\n",
    "    selected_cols = base_cols.copy()\n",
    "    helper_cols = list(set(selected_cols + ['periods_before_closing', 'returns']))\n",
    "    metrics_dict = {}\n",
    "    \n",
    "    for step in range(5, 10):\n",
    "        n_train = int(df.shape[0] * step // 10)\n",
    "        n_test = int(df.shape[0] * (step + 1) // 10)\n",
    "        train = df.iloc[:n_train].reset_index(drop=True).copy()\n",
    "        test = df.iloc[n_train:n_test].reset_index(drop=True).copy()\n",
    "        train.drop(np.arange(droprows), inplace=True)\n",
    "        train.dropna(inplace=True)\n",
    "\n",
    "        # For XGBoost, we need to create a new model instance for each fold\n",
    "        fold_model = XGBRegressor(**xgb_params)\n",
    "        fold_model.fit(train[selected_cols], train.returns)\n",
    "        predicted = fold_model.predict(test[selected_cols])\n",
    "        predicted[test.periods_before_closing == 0] = 0\n",
    "\n",
    "        current_mse = mean_squared_error(test.returns, predicted)\n",
    "        current_r2 = rsquared(test.returns, predicted) * 100\n",
    "        metrics_dict['train_{}_percent'.format(step * 10)] = {\n",
    "            'mse': current_mse,\n",
    "            'r2': current_r2\n",
    "        }\n",
    "    \n",
    "    report = pd.DataFrame(metrics_dict)\n",
    "    report['min_stats'] = report.iloc[:,:5].min(1).astype(np.float32)\n",
    "    report['max_stats'] = report.iloc[:,:5].max(1).astype(np.float32)\n",
    "    report['avg'] = report.mean(1).astype(np.float32)\n",
    "    return report\n",
    "\n",
    "# %% TRULY FIXED Feature Engineering Function\n",
    "def create_all_features(data, is_train=True):\n",
    "    \"\"\"\n",
    "    TRULY FIXED: Feature engineering with NO data leakage.\n",
    "    All features use strictly backward-looking rolling windows.\n",
    "    Current observation at time t IS included (since we predict t+60).\n",
    "    No resample+ffill pattern that causes intra-bin leakage.\n",
    "    \"\"\"\n",
    "    print(f\"Creating all features (NO LEAKAGE - {'TRAIN' if is_train else 'TEST'})...\")\n",
    "    \n",
    "    # Pre-calculate commonly used values\n",
    "    days = data.day.unique()\n",
    "    \n",
    "    # Opening/Closing differences (using previous day's close)\n",
    "    print(\"Creating opening/closing difference features...\")\n",
    "    close_price_per_day_y = data.groupby('day').timestamp.max().shift(1).map(\n",
    "        data[['timestamp', 'yprice']].set_index('timestamp').yprice)\n",
    "    data['ydiff_from_closing'] = (data.yprice - data.day.map(close_price_per_day_y)).fillna(0)\n",
    "    \n",
    "    close_price_per_day_x = data.groupby('day').timestamp.max().shift(1).map(\n",
    "        data[['timestamp', 'xprice']].set_index('timestamp').xprice)\n",
    "    data['xdiff_from_closing'] = (data.xprice - data.day.map(close_price_per_day_x)).fillna(0)\n",
    "    \n",
    "    open_price_per_day_x = data.groupby('day').timestamp.min().map(\n",
    "        data[['timestamp', 'xprice']].set_index('timestamp').xprice)\n",
    "    data['xdiff_from_opening'] = data.xprice - data.day.map(open_price_per_day_x)\n",
    "    \n",
    "    # Log features\n",
    "    print(\"Creating log features...\")\n",
    "    data['xlog'] = data.xprice.apply(np.log1p)\n",
    "    data['ylog'] = data.yprice.apply(np.log1p)\n",
    "    \n",
    "    # Expanding mean that only uses past data (including current)\n",
    "    print(\"Creating expanding mean difference...\")\n",
    "    data['yprice_expanding_mean_diff'] = data['yprice'] - data['yprice'].expanding(min_periods=1).mean()\n",
    "    \n",
    "    # FIXED: Time-based rolling features with backward-looking windows INCLUDING current\n",
    "    print(\"Creating time-based rolling features (TRULY FIXED)...\")\n",
    "    time_windows = {\n",
    "        6: '1min', 60: '10min', 360: '1hour', 720: '2hours', 1410: '4hours', 2820: '1workweek'\n",
    "    }\n",
    "    \n",
    "    # Create rolling means with backward-looking windows INCLUDING current observation\n",
    "    for window, name in time_windows.items():\n",
    "        if window in [6, 60, 360, 1410]:  # xprice windows\n",
    "            colname = f'xprice_time_mean_{name}'\n",
    "            # Rolling includes current observation (no shift)\n",
    "            data[colname] = data['xprice'] - data['xprice'].rolling(\n",
    "                window=window, min_periods=1).mean()\n",
    "        \n",
    "        if window in [60, 360, 720]:  # yprice windows\n",
    "            colname = f'yprice_time_mean_{name}'\n",
    "            # Rolling includes current observation (no shift)\n",
    "            data[colname] = data['yprice'] - data['yprice'].rolling(\n",
    "                window=window, min_periods=1).mean()\n",
    "    \n",
    "    # Create rolling std for yprice with backward-looking windows INCLUDING current\n",
    "    for window in [360, 720]:\n",
    "        name = time_windows[window]\n",
    "        colname = f'yprice_time_std_{name}'\n",
    "        # Rolling includes current observation (no shift)\n",
    "        data[colname] = data['yprice'].rolling(\n",
    "            window=window, min_periods=1).std().fillna(0) + std_reg_const\n",
    "    \n",
    "    # Z-scores for yprice\n",
    "    data['yprice_time_zscore_1hour'] = data.yprice_time_mean_1hour / data.yprice_time_std_1hour\n",
    "    data['yprice_time_zscore_2hours'] = data.yprice_time_mean_2hours / data.yprice_time_std_2hours\n",
    "    \n",
    "    # Intraday EWMA features (backward-looking INCLUDING current)\n",
    "    print(\"Creating intraday EWMA features...\")\n",
    "    ewma_configs = [\n",
    "        ('xprice', [24], ['4hour']),\n",
    "        ('xlog', [60], ['10min']),\n",
    "        ('ylog', [360], ['1hour']),\n",
    "        ('yprice', [24, 60], ['4hour', '10min'])\n",
    "    ]\n",
    "    \n",
    "    for col, windows, names in ewma_configs:\n",
    "        for day in days:\n",
    "            df_mask = (data.day == day)\n",
    "            day_data = data.loc[df_mask].copy()\n",
    "            \n",
    "            for window, name in zip(windows, names):\n",
    "                colname = f'{col}_dayly_ewma_{name}'\n",
    "                # EWMA includes current observation (no shift)\n",
    "                ewm = day_data[col].ewm(halflife=window, adjust=False).mean()\n",
    "                \n",
    "                if col in ['xprice', 'yprice']:\n",
    "                    data.loc[df_mask, colname] = day_data[col] - ewm\n",
    "                elif col in ['xlog', 'ylog']:\n",
    "                    data.loc[df_mask, colname] = day_data[col.replace('log', 'price')] - ewm\n",
    "    \n",
    "    # EWMA difference pair\n",
    "    data['yprice_ewma_difpair_10min_4hour'] = data.yprice_dayly_ewma_10min - data.yprice_dayly_ewma_4hour\n",
    "    \n",
    "    # Lag features (inherently backward-looking, excludes current)\n",
    "    print(\"Creating lag features...\")\n",
    "    lag_configs = [\n",
    "        ('xprice', 1410, '4hours'),\n",
    "        ('yprice', 1410, '4hours'),\n",
    "        ('xprice_time_mean_1hour', 1410, '4hours'),\n",
    "        ('yprice_time_mean_1hour', 1410, '4hours'),\n",
    "        ('yprice_time_mean_1hour', 2820, '1workweek'),\n",
    "        ('yprice_time_mean_10min', 60, '10min')\n",
    "    ]\n",
    "    \n",
    "    for col, lag, name in lag_configs:\n",
    "        data[f'{col}_lag_{name}'] = data[col].shift(lag)\n",
    "    \n",
    "    # RSI features (calculated on backward-looking data including current)\n",
    "    print(\"Creating RSI features...\")\n",
    "    # RSI needs some history, so we calculate it on the rolling means\n",
    "    if 'xprice_time_mean_1min' in data.columns:\n",
    "        data['xprice_time_mean_1min_rsi_1min'] = rsiFunc(data['xprice_time_mean_1min'].fillna(0).values, 6)\n",
    "    data['yprice_time_mean_10min_rsi_1hour'] = rsiFunc(data['yprice_time_mean_10min'].fillna(0).values, 360)\n",
    "    \n",
    "    # Additional EWMA on time means\n",
    "    print(\"Creating EWMA on time means...\")\n",
    "    ewma_on_means = [\n",
    "        ('xprice_time_mean_10min', 60, '10min'),\n",
    "        ('yprice_time_mean_10min', 24, '4hour')\n",
    "    ]\n",
    "    \n",
    "    for col, window, name in ewma_on_means:\n",
    "        for day in days:\n",
    "            df_mask = (data.day == day)\n",
    "            day_data = data.loc[df_mask].copy()\n",
    "            colname = f'{col}_dayly_ewma_{name}'\n",
    "            # EWMA includes current observation (no shift)\n",
    "            ewm = day_data[col].ewm(halflife=window, adjust=False).mean()\n",
    "            data.loc[df_mask, colname] = ewm\n",
    "    \n",
    "    # XY Combined features with backward-looking windows INCLUDING current\n",
    "    print(\"Creating XY combined features...\")\n",
    "    \n",
    "    # XY Harmonic std - use rolling window INCLUDING current\n",
    "    data['xy_garmonic_time_std_4hours'] = (\n",
    "        data['xy_garmonic'].rolling(window=1410, min_periods=1).std().fillna(0) + std_reg_const\n",
    "    )\n",
    "    \n",
    "    # Harmonic EWMA\n",
    "    for day in days:\n",
    "        df_mask = (data.day == day)\n",
    "        day_data = data.loc[df_mask].copy()\n",
    "        \n",
    "        # EWMA includes current observation (no shift)\n",
    "        data.loc[df_mask, 'xy_garmonic_dayly_ewma_1hour'] = (\n",
    "            day_data['xy_garmonic'].ewm(halflife=360, adjust=False).mean()\n",
    "        )\n",
    "        data.loc[df_mask, 'xy_garmonic_dayly_ewma_2hours'] = (\n",
    "            day_data['xy_garmonic'].ewm(halflife=720, adjust=False).mean()\n",
    "        )\n",
    "    \n",
    "    data['xy_garmonic_dayly_ewma_1hour'] = data['xy_garmonic'] - data['xy_garmonic_dayly_ewma_1hour']\n",
    "    data['xy_garmonic_dayly_ewma_2hours'] = data['xy_garmonic'] - data['xy_garmonic_dayly_ewma_2hours']\n",
    "    data['xy_garmonic_ewma_prodpair_2hours_1hour'] = (\n",
    "        data.xy_garmonic_dayly_ewma_2hours * data.xy_garmonic_dayly_ewma_1hour)\n",
    "    \n",
    "    # XY Geometric features with rolling windows INCLUDING current\n",
    "    for window, name in [(6, '1min'), (60, '10min'), (360, '1hour'), (720, '2hours')]:\n",
    "        colname = f'xy_geom_time_mean_{name}'\n",
    "        # Rolling includes current observation (no shift)\n",
    "        data[colname] = data['xy_geom'] - data['xy_geom'].rolling(\n",
    "            window=window, min_periods=1).mean()\n",
    "    \n",
    "    # Geometric lags and EWMA\n",
    "    data['xy_geom_time_mean_1hour_lag_20min'] = data['xy_geom_time_mean_1hour'].shift(120)\n",
    "    \n",
    "    for day in days:\n",
    "        df_mask = (data.day == day)\n",
    "        day_data = data.loc[df_mask].copy()\n",
    "        \n",
    "        # EWMA includes current observation (no shift)\n",
    "        data.loc[df_mask, 'xy_geom_time_mean_10min_dayly_ewma_1min'] = (\n",
    "            day_data['xy_geom_time_mean_10min'].ewm(halflife=6, adjust=False).mean()\n",
    "        )\n",
    "        data.loc[df_mask, 'xy_geom_time_mean_2hours_dayly_ewma_20min'] = (\n",
    "            day_data['xy_geom_time_mean_2hours'].ewm(halflife=120, adjust=False).mean()\n",
    "        )\n",
    "    \n",
    "    # XY Relation std with rolling windows INCLUDING current\n",
    "    for window, name in [(360, '1hour'), (720, '2hours')]:\n",
    "        colname = f'xy_relation_time_std_{name}'\n",
    "        data[colname] = (\n",
    "            data['xy_relation'].rolling(window=window, min_periods=1).std().fillna(0) + std_reg_const\n",
    "        )\n",
    "    \n",
    "    # XY Square zscore with rolling windows INCLUDING current\n",
    "    data['xy_square_time_mean_10min'] = (\n",
    "        data['xy_square'] - data['xy_square'].rolling(window=60, min_periods=1).mean()\n",
    "    )\n",
    "    data['xy_square_time_std_10min'] = (\n",
    "        data['xy_square'].rolling(window=60, min_periods=1).std().fillna(0) + std_reg_const\n",
    "    )\n",
    "    data['xy_square_time_zscore_10min'] = data['xy_square_time_mean_10min'] / data['xy_square_time_std_10min']\n",
    "    \n",
    "    # YX Spread features with rolling windows INCLUDING current\n",
    "    print(\"Creating YX spread features...\")\n",
    "    for window, name in [(60, '10min'), (720, '2hours'), (1410, '4hours')]:\n",
    "        colname = f'yx_spread_time_mean_{name}'\n",
    "        data[colname] = (\n",
    "            data['yx_spread'] - data['yx_spread'].rolling(window=window, min_periods=1).mean()\n",
    "        )\n",
    "    \n",
    "    # Spread std and zscore with rolling windows INCLUDING current\n",
    "    data['yx_spread_time_std_4hours'] = (\n",
    "        data['yx_spread'].rolling(window=1410, min_periods=1).std().fillna(0) + std_reg_const\n",
    "    )\n",
    "    data['yx_spread_time_zscore_4hours'] = data['yx_spread_time_mean_4hours'] / data['yx_spread_time_std_4hours']\n",
    "    \n",
    "    # Spread lags (excluding current)\n",
    "    data['yx_spread_time_mean_10min_lag_1hour'] = data['yx_spread_time_mean_10min'].shift(360)\n",
    "    data['yx_spread_time_mean_2hours_lag_20min'] = data['yx_spread_time_mean_2hours'].shift(120)\n",
    "    \n",
    "    # Spread EWMA INCLUDING current\n",
    "    for day in days:\n",
    "        df_mask = (data.day == day)\n",
    "        day_data = data.loc[df_mask].copy()\n",
    "        \n",
    "        # EWMA includes current observation (no shift)\n",
    "        data.loc[df_mask, 'yx_spread_dayly_ewma_10min'] = (\n",
    "            day_data['yx_spread'].ewm(halflife=60, adjust=False).mean()\n",
    "        )\n",
    "        data.loc[df_mask, 'yx_spread_dayly_ewma_1hour'] = (\n",
    "            day_data['yx_spread'].ewm(halflife=360, adjust=False).mean()\n",
    "        )\n",
    "    \n",
    "    data['yx_spread_dayly_ewma_10min'] = data['yx_spread'] - data['yx_spread_dayly_ewma_10min']\n",
    "    data['yx_spread_dayly_ewma_1hour'] = data['yx_spread'] - data['yx_spread_dayly_ewma_1hour']\n",
    "    data['yx_spread_ewma_prodpair_1hour_10min'] = (\n",
    "        data.yx_spread_dayly_ewma_1hour * data.yx_spread_dayly_ewma_10min)\n",
    "    \n",
    "    # Clean up temporary columns\n",
    "    temp_cols = [\n",
    "        'xy_garmonic_dayly_ewma_1hour', 'xy_garmonic_dayly_ewma_2hours',\n",
    "        'xy_square_time_mean_10min', 'xy_square_time_std_10min',\n",
    "        'yx_spread_dayly_ewma_10min', 'yx_spread_dayly_ewma_1hour',\n",
    "        'yprice_time_std_1hour', 'yprice_time_std_2hours',\n",
    "        'yx_spread_time_std_4hours', 'xy_relation_time_std_1hour',\n",
    "        'xy_relation_time_std_2hours'\n",
    "    ]\n",
    "    \n",
    "    for col in temp_cols:\n",
    "        if col in data.columns:\n",
    "            data.drop(col, axis=1, inplace=True)\n",
    "    \n",
    "    print(f\"Feature engineering completed (NO DATA LEAKAGE - {'TRAIN' if is_train else 'TEST'})!\")\n",
    "    return data\n",
    "\n",
    "# %% Data Initialization\n",
    "def init_data_single(fname):\n",
    "    \"\"\"Initialize and preprocess a single dataset\"\"\"\n",
    "    print(f'Loading file: {fname}...')\n",
    "    data = pd.read_csv(fname)\n",
    "    \n",
    "    data['xprice'] -= 127  # WARNING: Domain-specific adjustment\n",
    "    data['yprice'] -= 146  # WARNING: Domain-specific adjustment\n",
    "    \n",
    "    # Create derived price features\n",
    "    data['yx_spread'] = data.yprice - data.xprice\n",
    "    data['yx_relation'] = data.yprice / data.xprice\n",
    "    data['xy_relation'] = data.xprice / data.yprice\n",
    "    data['xy_square'] = np.sqrt(data.xprice ** 2 + data.yprice ** 2) / 2\n",
    "    data['xy_geom'] = np.sqrt(data.xprice * data.yprice)\n",
    "    data['xy_garmonic'] = 2 / (1 / data.xprice + 1 / data.yprice)\n",
    "    \n",
    "    # Process timestamps\n",
    "    data['timestamp'] = data['timestamp'] // 1000\n",
    "    data['timestamp'] = data['timestamp'].apply(lambda stamp: datetime.fromtimestamp(stamp))\n",
    "    data['timestamp'] = data['timestamp'] - pd.Timedelta(hours=1)\n",
    "    data.index = data['timestamp']\n",
    "    \n",
    "    # Add time-based features\n",
    "    data['weekday'] = data.timestamp.dt.weekday\n",
    "    data['is_end_of_week'] = (data.timestamp.dt.weekday >= 2).astype(int)\n",
    "    \n",
    "    data['day'] = (data.timestamp.dt.date - data.timestamp.dt.date.min()).apply(lambda x: int(x.days))\n",
    "    day_close_time = data.day.map(data.groupby('day').timestamp.max())\n",
    "    data['periods_before_closing'] = (day_close_time - data.timestamp).apply(lambda x: x.seconds // 10)\n",
    "    day_open_time = data.day.map(data.groupby('day').timestamp.min())\n",
    "    data['periods_after_opening'] = (data.timestamp - day_open_time).apply(lambda x: x.seconds // 10)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def selected_features_extractor_separate(train_data_path, test_data_path):\n",
    "    \"\"\"\n",
    "    Extract features separately for train and test to avoid leakage.\n",
    "    This is the SAFE approach - features are computed independently.\n",
    "    \n",
    "    Parameters:\n",
    "    train_data_path: Path to training data (can be None if only processing test)\n",
    "    test_data_path: Path to test data (can be None if only processing train)\n",
    "    \"\"\"\n",
    "    train = None\n",
    "    test = None\n",
    "    \n",
    "    # Process training data if provided\n",
    "    if train_data_path is not None:\n",
    "        train_data = init_data_single(train_data_path)\n",
    "        train_data = create_all_features(train_data, is_train=True)\n",
    "        \n",
    "        usecols = selected_cols + ['returns', 'periods_before_closing']\n",
    "        train = train_data[usecols].iloc[droprows:].copy()\n",
    "    \n",
    "    # Process test data if provided\n",
    "    if test_data_path is not None:\n",
    "        test_data = init_data_single(test_data_path)\n",
    "        test_data = create_all_features(test_data, is_train=False)\n",
    "        \n",
    "        # Test might not have 'returns' column\n",
    "        test_cols = selected_cols + ['periods_before_closing']\n",
    "        if 'returns' in test_data.columns:\n",
    "            test_cols.append('returns')\n",
    "        test = test_data[test_cols].copy()\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "# %% Model Training Functions\n",
    "def normalize_train(df):\n",
    "    \"\"\"Normalize training data\"\"\"\n",
    "    extended_cols = selected_cols + ['returns', 'periods_before_closing']\n",
    "    norm_train = df[extended_cols].reset_index(drop=True).copy()\n",
    "    norm_mean = norm_train[selected_cols].mean()\n",
    "    norm_std = norm_train[selected_cols].std() + normalization_std_reg\n",
    "    norm_train.loc[:,selected_cols] = (norm_train[selected_cols] - norm_mean) / norm_std\n",
    "    return norm_train\n",
    "\n",
    "def normalize_train_test(train, test):\n",
    "    \"\"\"Normalize train and test data using training statistics\"\"\"\n",
    "    train_extended_cols = selected_cols + ['returns', 'periods_before_closing']\n",
    "    test_extended_cols = selected_cols + ['periods_before_closing']\n",
    "    \n",
    "    norm_train = train[train_extended_cols].reset_index(drop=True).copy()\n",
    "    norm_test = test[test_extended_cols].reset_index(drop=True).copy()\n",
    "    \n",
    "    norm_mean = norm_train[selected_cols].mean()\n",
    "    norm_std = norm_train[selected_cols].std() + normalization_std_reg\n",
    "    \n",
    "    norm_train.loc[:,selected_cols] = (norm_train[selected_cols] - norm_mean) / norm_std\n",
    "    norm_test.loc[:,selected_cols] = (norm_test[selected_cols] - norm_mean) / norm_std\n",
    "    return norm_train, norm_test\n",
    "\n",
    "# %% Model Estimation Function\n",
    "def modelEstimate(train_data_path):\n",
    "    \"\"\"\n",
    "    Train the XGBoost model on the training data.\n",
    "    \n",
    "    Parameters:\n",
    "    train_data_path (str): Path to the training CSV file\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary containing the trained model and normalization parameters\n",
    "    \"\"\"\n",
    "    print('--------->Start Estimation with XGBoost....')\n",
    "    \n",
    "    # Extract features (train only, no test contamination)\n",
    "    train, _ = selected_features_extractor_separate(train_data_path, None)\n",
    "    \n",
    "    # Remove any rows with NaN values\n",
    "    train = train.dropna()\n",
    "    \n",
    "    # Normalize training data\n",
    "    print('----->Normalization....')\n",
    "    norm_train = normalize_train(train)\n",
    "    \n",
    "    # Calculate normalization parameters for later use\n",
    "    norm_mean = train[selected_cols].mean()\n",
    "    norm_std = train[selected_cols].std() + normalization_std_reg\n",
    "    \n",
    "    # Validate model\n",
    "    print('----->Validation with XGBoost....')\n",
    "    model = XGBRegressor(**xgb_params)\n",
    "    print(validate_model_by_pentate(model, norm_train, selected_cols, 0))\n",
    "    \n",
    "    # Fit final model\n",
    "    print('----->Fitting XGBoost model....')\n",
    "    print(f'Using XGBoost parameters: {xgb_params}')\n",
    "    \n",
    "    # Create model with early stopping included in parameters\n",
    "    xgb_params_with_early_stopping = xgb_params.copy()\n",
    "    xgb_params_with_early_stopping['early_stopping_rounds'] = 50\n",
    "    xgb_params_with_early_stopping['eval_metric'] = 'rmse'\n",
    "    \n",
    "    model = XGBRegressor(**xgb_params_with_early_stopping)\n",
    "    \n",
    "    # Optional: Use early stopping with validation set\n",
    "    # Split off a validation set for early stopping\n",
    "    val_size = int(len(norm_train) * 0.1)\n",
    "    train_X = norm_train[selected_cols].iloc[:-val_size]\n",
    "    train_y = norm_train.returns.iloc[:-val_size]\n",
    "    val_X = norm_train[selected_cols].iloc[-val_size:]\n",
    "    val_y = norm_train.returns.iloc[-val_size:]\n",
    "    \n",
    "    # Fit with validation set\n",
    "    model.fit(\n",
    "        train_X, train_y,\n",
    "        eval_set=[(val_X, val_y)],\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Check if model has best_iteration attribute (depends on XGBoost version)\n",
    "    if hasattr(model, 'best_iteration'):\n",
    "        print(f'Best iteration: {model.best_iteration}')\n",
    "    if hasattr(model, 'best_score'):\n",
    "        print(f'Best score: {model.best_score:.6f}')\n",
    "    \n",
    "    # Print feature importances\n",
    "    print('----->Calculating feature importances (XGBoost)...')\n",
    "    print_importances_xgb(model, selected_cols)\n",
    "    \n",
    "    # Return model and parameters\n",
    "    return {\n",
    "        'model': model,\n",
    "        'norm_mean': norm_mean,\n",
    "        'norm_std': norm_std,\n",
    "        'xgb_params': xgb_params\n",
    "    }\n",
    "\n",
    "# %% Model Forecast Function\n",
    "def modelForecast(test_data_path, model_params, train_data_path=None):\n",
    "    \"\"\"\n",
    "    Make predictions on test data using the trained XGBoost model.\n",
    "    \n",
    "    Parameters:\n",
    "    test_data_path (str): Path to the test CSV file\n",
    "    model_params (dict): Dictionary containing the trained model and normalization parameters\n",
    "    train_data_path (str): Not used in this version (features computed separately)\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary containing predictions and evaluation metrics (if test data has returns)\n",
    "    \"\"\"\n",
    "    print('--------->Starting Forecasting with XGBoost....')\n",
    "    \n",
    "    # Extract features for test only (no train contamination)\n",
    "    _, test = selected_features_extractor_separate(None, test_data_path)\n",
    "    \n",
    "    # Remove any rows with NaN values\n",
    "    test = test.dropna()\n",
    "    \n",
    "    # Normalize test data using training statistics\n",
    "    print('----->Normalization....')\n",
    "    test_extended_cols = selected_cols + ['periods_before_closing']\n",
    "    \n",
    "    # Check if test data has returns column for evaluation\n",
    "    has_returns = 'returns' in test.columns\n",
    "    if has_returns:\n",
    "        test_extended_cols.append('returns')\n",
    "    \n",
    "    norm_test = test[test_extended_cols].reset_index(drop=True).copy()\n",
    "    norm_test.loc[:,selected_cols] = (norm_test[selected_cols] - model_params['norm_mean']) / model_params['norm_std']\n",
    "    \n",
    "    # Make predictions\n",
    "    print('----->Prediction with XGBoost....')\n",
    "    predicted = model_params['model'].predict(norm_test[selected_cols])\n",
    "    \n",
    "    # Set predictions to 0 at closing periods\n",
    "    predicted[norm_test.periods_before_closing == 0] = 0\n",
    "    \n",
    "    # Calculate evaluation metrics if test data has returns\n",
    "    results = {'predictions': predicted}\n",
    "    \n",
    "    if has_returns:\n",
    "        print('----->Evaluation....')\n",
    "        # Calculate MSE\n",
    "        mse = mean_squared_error(norm_test.returns, predicted)\n",
    "        # Calculate R-squared\n",
    "        r2 = rsquared(norm_test.returns, predicted)\n",
    "        \n",
    "        results['mse'] = mse\n",
    "        results['r2'] = r2\n",
    "        \n",
    "        print(f'Test MSE: {mse:.6f}')\n",
    "        print(f'Test R-squared: {r2:.6f}')\n",
    "        print(f'Test R-squared (x100): {r2*100:.2f}')\n",
    "    else:\n",
    "        print('No returns column in test data - evaluation metrics not calculated')\n",
    "    \n",
    "    return results\n",
    "\n",
    "# %% Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Train the model\n",
    "    train_file_path = '/Users/mazin/Desktop/super prep materials/GSAPred/Two-financial-instruments/train.csv'  # Replace with your actual file path\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"Training XGBoost Model for Financial Time Series Prediction\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    model_params = modelEstimate(train_file_path)\n",
    "    \n",
    "    # Save model for later use\n",
    "    with open('xgboost_model_params.pkl', 'wb') as f:\n",
    "        pickle.dump(model_params, f)\n",
    "    print(\"\\nModel saved to xgboost_model_params.pkl\")\n",
    "    \n",
    "    # Make predictions on test data\n",
    "    test_file_path = '/Users/mazin/Desktop/super prep materials/GSAPred/Two-financial-instruments/test.csv'  # Replace with your actual file path\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Making Predictions on Test Data\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Load model if needed\n",
    "    # with open('xgboost_model_params.pkl', 'rb') as f:\n",
    "    #     model_params = pickle.load(f)\n",
    "    \n",
    "    results = modelForecast(test_file_path, model_params)\n",
    "    \n",
    "    # Extract predictions\n",
    "    predictions = results['predictions']\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\nFirst 10 predictions: {predictions[:10]}\")\n",
    "    print(f\"Total predictions: {len(predictions)}\")\n",
    "    \n",
    "    # Display evaluation metrics if available\n",
    "    if 'mse' in results:\n",
    "        print(f\"\\nEvaluation Metrics:\")\n",
    "        print(f\"MSE: {results['mse']:.6f}\")\n",
    "        print(f\"R-squared: {results['r2']:.6f}\")\n",
    "        print(f\"R-squared (x100): {results['r2']*100:.2f}\")\n",
    "    \n",
    "    # Save predictions to CSV\n",
    "    predictions_df = pd.DataFrame({'predictions': predictions})\n",
    "    predictions_df.to_csv('xgboost_predictions.csv', index=False)\n",
    "    print(\"\\nPredictions saved to xgboost_predictions.csv\")\n",
    "    \n",
    "    # Additional analysis: Get feature importance plot data\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Top 10 Most Important Features:\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    feature_importance = model_params['model'].feature_importances_\n",
    "    importance_dict = dict(zip(selected_cols, feature_importance))\n",
    "    sorted_features = sorted(importance_dict.items(), key=lambda x: -x[1])[:10]\n",
    "    \n",
    "    for i, (name, importance) in enumerate(sorted_features, 1):\n",
    "        print(f\"{i:2}. {name:40} {importance:.6f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"XGBoost Model Training and Prediction Complete!\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eed98fb-11f0-4d46-8fe9-e9828880fd3c",
   "metadata": {},
   "source": [
    "# Elastic Net CV with all features (better r squared than ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "dfef2bdf-35df-4f8d-b663-b396bcacc26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Training ElasticNetCV Model for Financial Time Series Prediction\n",
      "======================================================================\n",
      "--------->Start Estimation with ElasticNetCV....\n",
      "Loading file: /Users/mazin/Desktop/super prep materials/GSAPred/Two-financial-instruments/train.csv...\n",
      "Creating all features (NO LEAKAGE - TRAIN)...\n",
      "Creating opening/closing difference features...\n",
      "Creating log features...\n",
      "Creating expanding mean difference...\n",
      "Creating time-based rolling features (TRULY FIXED)...\n",
      "Creating intraday EWMA features...\n",
      "Creating lag features...\n",
      "Creating RSI features...\n",
      "Creating EWMA on time means...\n",
      "Creating XY combined features...\n",
      "Creating YX spread features...\n",
      "Feature engineering completed (NO DATA LEAKAGE - TRAIN)!\n",
      "----->Normalization....\n",
      "----->Validation with ElasticNetCV....\n",
      "     train_50_percent  train_60_percent  train_70_percent  train_80_percent  \\\n",
      "mse          0.030347          0.021255          0.017238          0.017891   \n",
      "r2           1.167415          0.839517          0.001305          1.112515   \n",
      "\n",
      "     train_90_percent  min_stats  max_stats       avg  \n",
      "mse          0.020179   0.017238   0.030347  0.022071  \n",
      "r2           1.842684   0.001305   1.842684  0.972489  \n",
      "----->Fitting ElasticNetCV model with cross-validation....\n",
      "Testing l1_ratios: [0.1, 0.3, 0.5, 0.7, 0.9, 0.95, 0.99]\n",
      "Testing 100 alpha values from 0.000100 to 100.000000\n",
      "Using 5-fold cross-validation\n",
      "\n",
      "----->Optimal hyperparameters found:\n",
      "Best alpha (regularization strength): 0.001417\n",
      "Best l1_ratio (L1 vs L2 mix): 0.950\n",
      "  - L1 penalty weight: 95.0%\n",
      "  - L2 penalty weight: 5.0%\n",
      "\n",
      "----->Calculating feature importances (ElasticNet)...\n",
      "\n",
      "Feature Importances (ElasticNet Coefficients):\n",
      "----------------------------------------------------------------------\n",
      "Non-zero features: 17/40\n",
      "Selected alpha: 0.001417\n",
      "Selected l1_ratio: 0.950\n",
      "----------------------------------------------------------------------\n",
      "xprice_dayly_ewma_4hour                  17.12%        0.013631\n",
      "yprice_dayly_ewma_10min                  13.88%       -0.011054\n",
      "yprice_time_zscore_2hours                11.24%        0.008948\n",
      "xprice_lag_4hours                        8.61%       -0.006853\n",
      "xdiff_from_closing                       7.06%       -0.005618\n",
      "xy_garmonic_time_std_4hours              6.19%        0.004925\n",
      "yprice_time_mean_1hour_lag_1workweek     5.73%       -0.004561\n",
      "is_end_of_week                           5.36%       -0.004266\n",
      "xdiff_from_opening                       4.69%        0.003736\n",
      "xprice_time_mean_1hour_lag_4hours        4.56%       -0.003627\n",
      "xprice_time_mean_10min_dayly_ewma_10min  4.53%       -0.003608\n",
      "yprice_time_mean_2hours                  4.20%       -0.003343\n",
      "xprice_time_mean_1min_rsi_1min           3.38%        0.002691\n",
      "xy_garmonic_ewma_prodpair_2hours_1hour   2.08%       -0.001657\n",
      "yx_spread_ewma_prodpair_1hour_10min      0.60%       -0.000479\n",
      "xy_geom_time_mean_1hour_lag_20min        0.44%       -0.000351\n",
      "yprice_time_mean_1hour                   0.35%       -0.000275\n",
      "\n",
      "Model saved to elasticnet_model_params.pkl\n",
      "\n",
      "======================================================================\n",
      "Making Predictions on Test Data\n",
      "======================================================================\n",
      "--------->Starting Forecasting with ElasticNetCV....\n",
      "Loading file: /Users/mazin/Desktop/super prep materials/GSAPred/Two-financial-instruments/test.csv...\n",
      "Creating all features (NO LEAKAGE - TEST)...\n",
      "Creating opening/closing difference features...\n",
      "Creating log features...\n",
      "Creating expanding mean difference...\n",
      "Creating time-based rolling features (TRULY FIXED)...\n",
      "Creating intraday EWMA features...\n",
      "Creating lag features...\n",
      "Creating RSI features...\n",
      "Creating EWMA on time means...\n",
      "Creating XY combined features...\n",
      "Creating YX spread features...\n",
      "Feature engineering completed (NO DATA LEAKAGE - TEST)!\n",
      "----->Normalization....\n",
      "----->Prediction with ElasticNetCV....\n",
      "----->Evaluation....\n",
      "Test MSE: 0.013091\n",
      "Test R-squared: 0.021939\n",
      "Test R-squared (x100): 2.19\n",
      "\n",
      "First 10 predictions: [-0.00251764 -0.00371736 -0.00431929 -0.00371463 -0.00621533 -0.00739297\n",
      " -0.00743756 -0.00777405  0.00240088  0.00103201]\n",
      "Total predictions: 32180\n",
      "\n",
      "Evaluation Metrics:\n",
      "MSE: 0.013091\n",
      "R-squared: 0.021939\n",
      "R-squared (x100): 2.19\n",
      "\n",
      "Predictions saved to elasticnet_predictions.csv\n",
      "\n",
      "======================================================================\n",
      "Model Sparsity Analysis:\n",
      "======================================================================\n",
      "Total features: 40\n",
      "Non-zero coefficients: 17\n",
      "Zero coefficients: 23\n",
      "Sparsity: 57.5%\n",
      "\n",
      "Selected alpha: 0.001417\n",
      "Selected l1_ratio: 0.950\n",
      "\n",
      "======================================================================\n",
      "Top 10 Most Important Features (by absolute coefficient):\n",
      "======================================================================\n",
      " 1. xprice_dayly_ewma_4hour                         0.013631\n",
      " 2. yprice_dayly_ewma_10min                        -0.011054\n",
      " 3. yprice_time_zscore_2hours                       0.008948\n",
      " 4. xprice_lag_4hours                              -0.006853\n",
      " 5. xdiff_from_closing                             -0.005618\n",
      " 6. xy_garmonic_time_std_4hours                     0.004925\n",
      " 7. yprice_time_mean_1hour_lag_1workweek           -0.004561\n",
      " 8. is_end_of_week                                 -0.004266\n",
      " 9. xdiff_from_opening                              0.003736\n",
      "10. xprice_time_mean_1hour_lag_4hours              -0.003627\n",
      "\n",
      "======================================================================\n",
      "ElasticNetCV Model Training and Prediction Complete!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from copy import copy\n",
    "import scipy\n",
    "import pickle\n",
    "\n",
    "# %% Configuration\n",
    "# Configuration parameters\n",
    "droprows = 7050\n",
    "std_reg_const = 0.1\n",
    "normalization_std_reg = 0.0001\n",
    "\n",
    "# ElasticNetCV parameters\n",
    "elasticnet_params = {\n",
    "    'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9, 0.95, 0.99],  # Mix of L1 and L2 penalties\n",
    "    'alphas': np.logspace(-4, 2, 100),  # Range of regularization strengths to try\n",
    "    'cv': 5,  # 5-fold cross-validation\n",
    "    'max_iter': 10000,  # Maximum iterations\n",
    "    'tol': 0.0001,  # Tolerance for optimization\n",
    "    'random_state': 42,\n",
    "    'selection': 'cyclic',  # Feature selection method\n",
    "    'fit_intercept': True\n",
    "}\n",
    "\n",
    "selected_cols = [\n",
    "    \n",
    "    # Time-based features\n",
    "    'is_end_of_week',\n",
    "    'weekday',\n",
    "    \n",
    "    # Opening/Closing difference features\n",
    "    'xdiff_from_closing',\n",
    "    'xdiff_from_opening',\n",
    "    'ydiff_from_closing',\n",
    "    \n",
    "    # X-price features\n",
    "    'xlog',\n",
    "    'xlog_dayly_ewma_10min',\n",
    "    'xprice_dayly_ewma_4hour',\n",
    "    'xprice_lag_4hours',\n",
    "    'xprice_time_mean_4hours',\n",
    "    'xprice_time_mean_1hour_lag_4hours',\n",
    "    'xprice_time_mean_1min',\n",
    "    'xprice_time_mean_10min_dayly_ewma_10min',\n",
    "    'xprice_time_mean_1min_rsi_1min',\n",
    "    \n",
    "    # Y-price features\n",
    "    'ylog_dayly_ewma_1hour',\n",
    "    'yprice_dayly_ewma_10min',\n",
    "    'yprice_lag_4hours',\n",
    "    'yprice_ewma_difpair_10min_4hour',\n",
    "    'yprice_expanding_mean_diff', # FIXED: renamed from yprice_full_history_diff\n",
    "    'yprice_time_mean_1hour',\n",
    "    'yprice_time_mean_1hour_lag_4hours',\n",
    "    'yprice_time_mean_1hour_lag_1workweek',\n",
    "    'yprice_time_mean_10min',\n",
    "    'yprice_time_mean_10min_dayly_ewma_4hour',\n",
    "    'yprice_time_mean_10min_lag_10min',\n",
    "    'yprice_time_mean_10min_rsi_1hour',\n",
    "    'yprice_time_mean_2hours',\n",
    "    'yprice_time_zscore_1hour',\n",
    "    'yprice_time_zscore_2hours',\n",
    "    \n",
    "    # XY combined features\n",
    "    'xy_garmonic_ewma_prodpair_2hours_1hour',\n",
    "    'xy_garmonic_time_std_4hours',\n",
    "    'xy_geom_time_mean_1hour_lag_20min',\n",
    "    'xy_geom_time_mean_1min',\n",
    "    'xy_geom_time_mean_10min_dayly_ewma_1min',\n",
    "    'xy_geom_time_mean_2hours_dayly_ewma_20min',\n",
    "    'xy_square_time_zscore_10min',\n",
    "    \n",
    "    # YX spread features\n",
    "    'yx_spread_ewma_prodpair_1hour_10min',\n",
    "    'yx_spread_time_mean_10min_lag_1hour',\n",
    "    'yx_spread_time_mean_2hours_lag_20min',\n",
    "    'yx_spread_time_zscore_4hours',\n",
    "]\n",
    "\n",
    "\n",
    "# %% Helper Functions\n",
    "def print_importances_elasticnet(model, selected_cols):\n",
    "    \"\"\"Print feature importances for ElasticNet model sorted by absolute coefficient\"\"\"\n",
    "    weights_sum = sum(map(abs, model.coef_))\n",
    "    \n",
    "    print(\"\\nFeature Importances (ElasticNet Coefficients):\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Create a list of (name, weight) tuples and sort by absolute weight\n",
    "    feature_weights = list(zip(selected_cols, model.coef_))\n",
    "    sorted_features = sorted(feature_weights, key=lambda x: -abs(x[1]))\n",
    "    \n",
    "    # Print non-zero features\n",
    "    non_zero_count = sum(1 for _, weight in feature_weights if abs(weight) > 1e-10)\n",
    "    print(f\"Non-zero features: {non_zero_count}/{len(selected_cols)}\")\n",
    "    print(f\"Selected alpha: {model.alpha_:.6f}\")\n",
    "    print(f\"Selected l1_ratio: {model.l1_ratio_:.3f}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for name, weight in sorted_features:\n",
    "        if abs(weight) > 1e-10:  # Only show non-zero weights\n",
    "            percent_weight = abs(weight) / weights_sum if weights_sum > 0 else 0\n",
    "            print('{:40} {:.2%} {:15.6f}'.format(name, percent_weight, weight))\n",
    "\n",
    "def rsquared(x, y):\n",
    "    \"\"\"Return R^2 where x and y are array-like.\"\"\"\n",
    "    slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(x, y)\n",
    "    return r_value**2\n",
    "\n",
    "def rsiFunc(prices, n=14):\n",
    "    \"\"\"Calculate RSI (Relative Strength Index)\"\"\"\n",
    "    deltas = np.diff(prices)\n",
    "    seed = deltas[:n+1]\n",
    "    up = seed[seed>=0].sum()/n\n",
    "    down = -seed[seed<0].sum()/n\n",
    "    rs = up/down\n",
    "    rsi = np.zeros_like(prices)\n",
    "    rsi[:n] = 100. - 100./(1.+rs)\n",
    "\n",
    "    for i in range(n, len(prices)):\n",
    "        delta = deltas[i-1]\n",
    "        if delta>0:\n",
    "            upval = delta\n",
    "            downval = 0.\n",
    "        else:\n",
    "            upval = 0.\n",
    "            downval = -delta\n",
    "\n",
    "        up = (up*(n-1) + upval)/n\n",
    "        down = (down*(n-1) + downval)/n\n",
    "        rs = up/down\n",
    "        rsi[i] = 100. - 100./(1.+rs)\n",
    "    return rsi\n",
    "\n",
    "# %% Time Series Validation Functions\n",
    "def time_split(data, valid_ratio, test_ratio):\n",
    "    \"\"\"Split time series data into train, validation, and test sets\"\"\"\n",
    "    n_valid = max(1, int(data.shape[0] * valid_ratio))\n",
    "    n_test = max(1, int(data.shape[0] * test_ratio))\n",
    "    n_train = data.shape[0] - n_valid - n_test\n",
    "    \n",
    "    train = data.iloc[:n_train].reset_index(drop=True).copy()\n",
    "    valid = data.iloc[n_train:-n_test].reset_index(drop=True).copy()\n",
    "    test = data.iloc[-n_test:].reset_index(drop=True).copy()\n",
    "    merged_test = pd.concat([valid, test], ignore_index=True)\n",
    "    return train, valid, test\n",
    "\n",
    "def validate_model_by_pentate(model_class, model_params, source_data, base_cols, droprows=0):\n",
    "    \"\"\"Validate model using 5-fold time series cross-validation\"\"\"\n",
    "    df = source_data.copy()\n",
    "    selected_cols = base_cols.copy()\n",
    "    helper_cols = list(set(selected_cols + ['periods_before_closing', 'returns']))\n",
    "    metrics_dict = {}\n",
    "    \n",
    "    for step in range(5, 10):\n",
    "        n_train = int(df.shape[0] * step // 10)\n",
    "        n_test = int(df.shape[0] * (step + 1) // 10)\n",
    "        train = df.iloc[:n_train].reset_index(drop=True).copy()\n",
    "        test = df.iloc[n_train:n_test].reset_index(drop=True).copy()\n",
    "        train.drop(np.arange(droprows), inplace=True)\n",
    "        train.dropna(inplace=True)\n",
    "\n",
    "        # Create and fit a new model for each fold\n",
    "        fold_model = model_class(**model_params)\n",
    "        fold_model.fit(train[selected_cols], train.returns)\n",
    "        predicted = fold_model.predict(test[selected_cols])\n",
    "        predicted[test.periods_before_closing == 0] = 0\n",
    "\n",
    "        current_mse = mean_squared_error(test.returns, predicted)\n",
    "        current_r2 = rsquared(test.returns, predicted) * 100\n",
    "        metrics_dict['train_{}_percent'.format(step * 10)] = {\n",
    "            'mse': current_mse,\n",
    "            'r2': current_r2\n",
    "        }\n",
    "    \n",
    "    report = pd.DataFrame(metrics_dict)\n",
    "    report['min_stats'] = report.iloc[:,:5].min(1).astype(np.float32)\n",
    "    report['max_stats'] = report.iloc[:,:5].max(1).astype(np.float32)\n",
    "    report['avg'] = report.mean(1).astype(np.float32)\n",
    "    return report\n",
    "\n",
    "# %% TRULY FIXED Feature Engineering Function\n",
    "def create_all_features(data, is_train=True):\n",
    "    \"\"\"\n",
    "    TRULY FIXED: Feature engineering with NO data leakage.\n",
    "    All features use strictly backward-looking rolling windows.\n",
    "    Current observation at time t IS included (since we predict t+60).\n",
    "    No resample+ffill pattern that causes intra-bin leakage.\n",
    "    \"\"\"\n",
    "    print(f\"Creating all features (NO LEAKAGE - {'TRAIN' if is_train else 'TEST'})...\")\n",
    "    \n",
    "    # Pre-calculate commonly used values\n",
    "    days = data.day.unique()\n",
    "    \n",
    "    # Opening/Closing differences (using previous day's close)\n",
    "    print(\"Creating opening/closing difference features...\")\n",
    "    close_price_per_day_y = data.groupby('day').timestamp.max().shift(1).map(\n",
    "        data[['timestamp', 'yprice']].set_index('timestamp').yprice)\n",
    "    data['ydiff_from_closing'] = (data.yprice - data.day.map(close_price_per_day_y)).fillna(0)\n",
    "    \n",
    "    close_price_per_day_x = data.groupby('day').timestamp.max().shift(1).map(\n",
    "        data[['timestamp', 'xprice']].set_index('timestamp').xprice)\n",
    "    data['xdiff_from_closing'] = (data.xprice - data.day.map(close_price_per_day_x)).fillna(0)\n",
    "    \n",
    "    open_price_per_day_x = data.groupby('day').timestamp.min().map(\n",
    "        data[['timestamp', 'xprice']].set_index('timestamp').xprice)\n",
    "    data['xdiff_from_opening'] = data.xprice - data.day.map(open_price_per_day_x)\n",
    "    \n",
    "    # Log features\n",
    "    print(\"Creating log features...\")\n",
    "    data['xlog'] = data.xprice.apply(np.log1p)\n",
    "    data['ylog'] = data.yprice.apply(np.log1p)\n",
    "    \n",
    "    # Expanding mean that only uses past data (including current)\n",
    "    print(\"Creating expanding mean difference...\")\n",
    "    data['yprice_expanding_mean_diff'] = data['yprice'] - data['yprice'].expanding(min_periods=1).mean()\n",
    "    \n",
    "    # FIXED: Time-based rolling features with backward-looking windows INCLUDING current\n",
    "    print(\"Creating time-based rolling features (TRULY FIXED)...\")\n",
    "    time_windows = {\n",
    "        6: '1min', 60: '10min', 360: '1hour', 720: '2hours', 1410: '4hours', 2820: '1workweek'\n",
    "    }\n",
    "    \n",
    "    # Create rolling means with backward-looking windows INCLUDING current observation\n",
    "    for window, name in time_windows.items():\n",
    "        if window in [6, 60, 360, 1410]:  # xprice windows\n",
    "            colname = f'xprice_time_mean_{name}'\n",
    "            # Rolling includes current observation (no shift)\n",
    "            data[colname] = data['xprice'] - data['xprice'].rolling(\n",
    "                window=window, min_periods=1).mean()\n",
    "        \n",
    "        if window in [60, 360, 720]:  # yprice windows\n",
    "            colname = f'yprice_time_mean_{name}'\n",
    "            # Rolling includes current observation (no shift)\n",
    "            data[colname] = data['yprice'] - data['yprice'].rolling(\n",
    "                window=window, min_periods=1).mean()\n",
    "    \n",
    "    # Create rolling std for yprice with backward-looking windows INCLUDING current\n",
    "    for window in [360, 720]:\n",
    "        name = time_windows[window]\n",
    "        colname = f'yprice_time_std_{name}'\n",
    "        # Rolling includes current observation (no shift)\n",
    "        data[colname] = data['yprice'].rolling(\n",
    "            window=window, min_periods=1).std().fillna(0) + std_reg_const\n",
    "    \n",
    "    # Z-scores for yprice\n",
    "    data['yprice_time_zscore_1hour'] = data.yprice_time_mean_1hour / data.yprice_time_std_1hour\n",
    "    data['yprice_time_zscore_2hours'] = data.yprice_time_mean_2hours / data.yprice_time_std_2hours\n",
    "    \n",
    "    # Intraday EWMA features (backward-looking INCLUDING current)\n",
    "    print(\"Creating intraday EWMA features...\")\n",
    "    ewma_configs = [\n",
    "        ('xprice', [24], ['4hour']),\n",
    "        ('xlog', [60], ['10min']),\n",
    "        ('ylog', [360], ['1hour']),\n",
    "        ('yprice', [24, 60], ['4hour', '10min'])\n",
    "    ]\n",
    "    \n",
    "    for col, windows, names in ewma_configs:\n",
    "        for day in days:\n",
    "            df_mask = (data.day == day)\n",
    "            day_data = data.loc[df_mask].copy()\n",
    "            \n",
    "            for window, name in zip(windows, names):\n",
    "                colname = f'{col}_dayly_ewma_{name}'\n",
    "                # EWMA includes current observation (no shift)\n",
    "                ewm = day_data[col].ewm(halflife=window, adjust=False).mean()\n",
    "                \n",
    "                if col in ['xprice', 'yprice']:\n",
    "                    data.loc[df_mask, colname] = day_data[col] - ewm\n",
    "                elif col in ['xlog', 'ylog']:\n",
    "                    data.loc[df_mask, colname] = day_data[col.replace('log', 'price')] - ewm\n",
    "    \n",
    "    # EWMA difference pair\n",
    "    data['yprice_ewma_difpair_10min_4hour'] = data.yprice_dayly_ewma_10min - data.yprice_dayly_ewma_4hour\n",
    "    \n",
    "    # Lag features (inherently backward-looking, excludes current)\n",
    "    print(\"Creating lag features...\")\n",
    "    lag_configs = [\n",
    "        ('xprice', 1410, '4hours'),\n",
    "        ('yprice', 1410, '4hours'),\n",
    "        ('xprice_time_mean_1hour', 1410, '4hours'),\n",
    "        ('yprice_time_mean_1hour', 1410, '4hours'),\n",
    "        ('yprice_time_mean_1hour', 2820, '1workweek'),\n",
    "        ('yprice_time_mean_10min', 60, '10min')\n",
    "    ]\n",
    "    \n",
    "    for col, lag, name in lag_configs:\n",
    "        data[f'{col}_lag_{name}'] = data[col].shift(lag)\n",
    "    \n",
    "    # RSI features (calculated on backward-looking data including current)\n",
    "    print(\"Creating RSI features...\")\n",
    "    # RSI needs some history, so we calculate it on the rolling means\n",
    "    if 'xprice_time_mean_1min' in data.columns:\n",
    "        data['xprice_time_mean_1min_rsi_1min'] = rsiFunc(data['xprice_time_mean_1min'].fillna(0).values, 6)\n",
    "    data['yprice_time_mean_10min_rsi_1hour'] = rsiFunc(data['yprice_time_mean_10min'].fillna(0).values, 360)\n",
    "    \n",
    "    # Additional EWMA on time means\n",
    "    print(\"Creating EWMA on time means...\")\n",
    "    ewma_on_means = [\n",
    "        ('xprice_time_mean_10min', 60, '10min'),\n",
    "        ('yprice_time_mean_10min', 24, '4hour')\n",
    "    ]\n",
    "    \n",
    "    for col, window, name in ewma_on_means:\n",
    "        for day in days:\n",
    "            df_mask = (data.day == day)\n",
    "            day_data = data.loc[df_mask].copy()\n",
    "            colname = f'{col}_dayly_ewma_{name}'\n",
    "            # EWMA includes current observation (no shift)\n",
    "            ewm = day_data[col].ewm(halflife=window, adjust=False).mean()\n",
    "            data.loc[df_mask, colname] = ewm\n",
    "    \n",
    "    # XY Combined features with backward-looking windows INCLUDING current\n",
    "    print(\"Creating XY combined features...\")\n",
    "    \n",
    "    # XY Harmonic std - use rolling window INCLUDING current\n",
    "    data['xy_garmonic_time_std_4hours'] = (\n",
    "        data['xy_garmonic'].rolling(window=1410, min_periods=1).std().fillna(0) + std_reg_const\n",
    "    )\n",
    "    \n",
    "    # Harmonic EWMA\n",
    "    for day in days:\n",
    "        df_mask = (data.day == day)\n",
    "        day_data = data.loc[df_mask].copy()\n",
    "        \n",
    "        # EWMA includes current observation (no shift)\n",
    "        data.loc[df_mask, 'xy_garmonic_dayly_ewma_1hour'] = (\n",
    "            day_data['xy_garmonic'].ewm(halflife=360, adjust=False).mean()\n",
    "        )\n",
    "        data.loc[df_mask, 'xy_garmonic_dayly_ewma_2hours'] = (\n",
    "            day_data['xy_garmonic'].ewm(halflife=720, adjust=False).mean()\n",
    "        )\n",
    "    \n",
    "    data['xy_garmonic_dayly_ewma_1hour'] = data['xy_garmonic'] - data['xy_garmonic_dayly_ewma_1hour']\n",
    "    data['xy_garmonic_dayly_ewma_2hours'] = data['xy_garmonic'] - data['xy_garmonic_dayly_ewma_2hours']\n",
    "    data['xy_garmonic_ewma_prodpair_2hours_1hour'] = (\n",
    "        data.xy_garmonic_dayly_ewma_2hours * data.xy_garmonic_dayly_ewma_1hour)\n",
    "    \n",
    "    # XY Geometric features with rolling windows INCLUDING current\n",
    "    for window, name in [(6, '1min'), (60, '10min'), (360, '1hour'), (720, '2hours')]:\n",
    "        colname = f'xy_geom_time_mean_{name}'\n",
    "        # Rolling includes current observation (no shift)\n",
    "        data[colname] = data['xy_geom'] - data['xy_geom'].rolling(\n",
    "            window=window, min_periods=1).mean()\n",
    "    \n",
    "    # Geometric lags and EWMA\n",
    "    data['xy_geom_time_mean_1hour_lag_20min'] = data['xy_geom_time_mean_1hour'].shift(120)\n",
    "    \n",
    "    for day in days:\n",
    "        df_mask = (data.day == day)\n",
    "        day_data = data.loc[df_mask].copy()\n",
    "        \n",
    "        # EWMA includes current observation (no shift)\n",
    "        data.loc[df_mask, 'xy_geom_time_mean_10min_dayly_ewma_1min'] = (\n",
    "            day_data['xy_geom_time_mean_10min'].ewm(halflife=6, adjust=False).mean()\n",
    "        )\n",
    "        data.loc[df_mask, 'xy_geom_time_mean_2hours_dayly_ewma_20min'] = (\n",
    "            day_data['xy_geom_time_mean_2hours'].ewm(halflife=120, adjust=False).mean()\n",
    "        )\n",
    "    \n",
    "    # XY Relation std with rolling windows INCLUDING current\n",
    "    for window, name in [(360, '1hour'), (720, '2hours')]:\n",
    "        colname = f'xy_relation_time_std_{name}'\n",
    "        data[colname] = (\n",
    "            data['xy_relation'].rolling(window=window, min_periods=1).std().fillna(0) + std_reg_const\n",
    "        )\n",
    "    \n",
    "    # XY Square zscore with rolling windows INCLUDING current\n",
    "    data['xy_square_time_mean_10min'] = (\n",
    "        data['xy_square'] - data['xy_square'].rolling(window=60, min_periods=1).mean()\n",
    "    )\n",
    "    data['xy_square_time_std_10min'] = (\n",
    "        data['xy_square'].rolling(window=60, min_periods=1).std().fillna(0) + std_reg_const\n",
    "    )\n",
    "    data['xy_square_time_zscore_10min'] = data['xy_square_time_mean_10min'] / data['xy_square_time_std_10min']\n",
    "    \n",
    "    # YX Spread features with rolling windows INCLUDING current\n",
    "    print(\"Creating YX spread features...\")\n",
    "    for window, name in [(60, '10min'), (720, '2hours'), (1410, '4hours')]:\n",
    "        colname = f'yx_spread_time_mean_{name}'\n",
    "        data[colname] = (\n",
    "            data['yx_spread'] - data['yx_spread'].rolling(window=window, min_periods=1).mean()\n",
    "        )\n",
    "    \n",
    "    # Spread std and zscore with rolling windows INCLUDING current\n",
    "    data['yx_spread_time_std_4hours'] = (\n",
    "        data['yx_spread'].rolling(window=1410, min_periods=1).std().fillna(0) + std_reg_const\n",
    "    )\n",
    "    data['yx_spread_time_zscore_4hours'] = data['yx_spread_time_mean_4hours'] / data['yx_spread_time_std_4hours']\n",
    "    \n",
    "    # Spread lags (excluding current)\n",
    "    data['yx_spread_time_mean_10min_lag_1hour'] = data['yx_spread_time_mean_10min'].shift(360)\n",
    "    data['yx_spread_time_mean_2hours_lag_20min'] = data['yx_spread_time_mean_2hours'].shift(120)\n",
    "    \n",
    "    # Spread EWMA INCLUDING current\n",
    "    for day in days:\n",
    "        df_mask = (data.day == day)\n",
    "        day_data = data.loc[df_mask].copy()\n",
    "        \n",
    "        # EWMA includes current observation (no shift)\n",
    "        data.loc[df_mask, 'yx_spread_dayly_ewma_10min'] = (\n",
    "            day_data['yx_spread'].ewm(halflife=60, adjust=False).mean()\n",
    "        )\n",
    "        data.loc[df_mask, 'yx_spread_dayly_ewma_1hour'] = (\n",
    "            day_data['yx_spread'].ewm(halflife=360, adjust=False).mean()\n",
    "        )\n",
    "    \n",
    "    data['yx_spread_dayly_ewma_10min'] = data['yx_spread'] - data['yx_spread_dayly_ewma_10min']\n",
    "    data['yx_spread_dayly_ewma_1hour'] = data['yx_spread'] - data['yx_spread_dayly_ewma_1hour']\n",
    "    data['yx_spread_ewma_prodpair_1hour_10min'] = (\n",
    "        data.yx_spread_dayly_ewma_1hour * data.yx_spread_dayly_ewma_10min)\n",
    "    \n",
    "    # Clean up temporary columns\n",
    "    temp_cols = [\n",
    "        'xy_garmonic_dayly_ewma_1hour', 'xy_garmonic_dayly_ewma_2hours',\n",
    "        'xy_square_time_mean_10min', 'xy_square_time_std_10min',\n",
    "        'yx_spread_dayly_ewma_10min', 'yx_spread_dayly_ewma_1hour',\n",
    "        'yprice_time_std_1hour', 'yprice_time_std_2hours',\n",
    "        'yx_spread_time_std_4hours', 'xy_relation_time_std_1hour',\n",
    "        'xy_relation_time_std_2hours'\n",
    "    ]\n",
    "    \n",
    "    for col in temp_cols:\n",
    "        if col in data.columns:\n",
    "            data.drop(col, axis=1, inplace=True)\n",
    "    \n",
    "    print(f\"Feature engineering completed (NO DATA LEAKAGE - {'TRAIN' if is_train else 'TEST'})!\")\n",
    "    return data\n",
    "\n",
    "# %% Data Initialization\n",
    "def init_data_single(fname):\n",
    "    \"\"\"Initialize and preprocess a single dataset\"\"\"\n",
    "    print(f'Loading file: {fname}...')\n",
    "    data = pd.read_csv(fname)\n",
    "    \n",
    "    data['xprice'] -= 127  # WARNING: Domain-specific adjustment\n",
    "    data['yprice'] -= 146  # WARNING: Domain-specific adjustment\n",
    "    \n",
    "    # Create derived price features\n",
    "    data['yx_spread'] = data.yprice - data.xprice\n",
    "    data['yx_relation'] = data.yprice / data.xprice\n",
    "    data['xy_relation'] = data.xprice / data.yprice\n",
    "    data['xy_square'] = np.sqrt(data.xprice ** 2 + data.yprice ** 2) / 2\n",
    "    data['xy_geom'] = np.sqrt(data.xprice * data.yprice)\n",
    "    data['xy_garmonic'] = 2 / (1 / data.xprice + 1 / data.yprice)\n",
    "    \n",
    "    # Process timestamps\n",
    "    data['timestamp'] = data['timestamp'] // 1000\n",
    "    data['timestamp'] = data['timestamp'].apply(lambda stamp: datetime.fromtimestamp(stamp))\n",
    "    data['timestamp'] = data['timestamp'] - pd.Timedelta(hours=1)\n",
    "    data.index = data['timestamp']\n",
    "    \n",
    "    # Add time-based features\n",
    "    data['weekday'] = data.timestamp.dt.weekday\n",
    "    data['is_end_of_week'] = (data.timestamp.dt.weekday >= 2).astype(int)\n",
    "    \n",
    "    data['day'] = (data.timestamp.dt.date - data.timestamp.dt.date.min()).apply(lambda x: int(x.days))\n",
    "    day_close_time = data.day.map(data.groupby('day').timestamp.max())\n",
    "    data['periods_before_closing'] = (day_close_time - data.timestamp).apply(lambda x: x.seconds // 10)\n",
    "    day_open_time = data.day.map(data.groupby('day').timestamp.min())\n",
    "    data['periods_after_opening'] = (data.timestamp - day_open_time).apply(lambda x: x.seconds // 10)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def selected_features_extractor_separate(train_data_path, test_data_path):\n",
    "    \"\"\"\n",
    "    Extract features separately for train and test to avoid leakage.\n",
    "    This is the SAFE approach - features are computed independently.\n",
    "    \n",
    "    Parameters:\n",
    "    train_data_path: Path to training data (can be None if only processing test)\n",
    "    test_data_path: Path to test data (can be None if only processing train)\n",
    "    \"\"\"\n",
    "    train = None\n",
    "    test = None\n",
    "    \n",
    "    # Process training data if provided\n",
    "    if train_data_path is not None:\n",
    "        train_data = init_data_single(train_data_path)\n",
    "        train_data = create_all_features(train_data, is_train=True)\n",
    "        \n",
    "        usecols = selected_cols + ['returns', 'periods_before_closing']\n",
    "        train = train_data[usecols].iloc[droprows:].copy()\n",
    "    \n",
    "    # Process test data if provided\n",
    "    if test_data_path is not None:\n",
    "        test_data = init_data_single(test_data_path)\n",
    "        test_data = create_all_features(test_data, is_train=False)\n",
    "        \n",
    "        # Test might not have 'returns' column\n",
    "        test_cols = selected_cols + ['periods_before_closing']\n",
    "        if 'returns' in test_data.columns:\n",
    "            test_cols.append('returns')\n",
    "        test = test_data[test_cols].copy()\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "# %% Model Training Functions\n",
    "def normalize_train(df):\n",
    "    \"\"\"Normalize training data\"\"\"\n",
    "    extended_cols = selected_cols + ['returns', 'periods_before_closing']\n",
    "    norm_train = df[extended_cols].reset_index(drop=True).copy()\n",
    "    norm_mean = norm_train[selected_cols].mean()\n",
    "    norm_std = norm_train[selected_cols].std() + normalization_std_reg\n",
    "    norm_train.loc[:,selected_cols] = (norm_train[selected_cols] - norm_mean) / norm_std\n",
    "    return norm_train\n",
    "\n",
    "def normalize_train_test(train, test):\n",
    "    \"\"\"Normalize train and test data using training statistics\"\"\"\n",
    "    train_extended_cols = selected_cols + ['returns', 'periods_before_closing']\n",
    "    test_extended_cols = selected_cols + ['periods_before_closing']\n",
    "    \n",
    "    norm_train = train[train_extended_cols].reset_index(drop=True).copy()\n",
    "    norm_test = test[test_extended_cols].reset_index(drop=True).copy()\n",
    "    \n",
    "    norm_mean = norm_train[selected_cols].mean()\n",
    "    norm_std = norm_train[selected_cols].std() + normalization_std_reg\n",
    "    \n",
    "    norm_train.loc[:,selected_cols] = (norm_train[selected_cols] - norm_mean) / norm_std\n",
    "    norm_test.loc[:,selected_cols] = (norm_test[selected_cols] - norm_mean) / norm_std\n",
    "    return norm_train, norm_test\n",
    "\n",
    "# %% Model Estimation Function\n",
    "def modelEstimate(train_data_path):\n",
    "    \"\"\"\n",
    "    Train the ElasticNetCV model on the training data.\n",
    "    \n",
    "    Parameters:\n",
    "    train_data_path (str): Path to the training CSV file\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary containing the trained model and normalization parameters\n",
    "    \"\"\"\n",
    "    print('--------->Start Estimation with ElasticNetCV....')\n",
    "    \n",
    "    # Extract features (train only, no test contamination)\n",
    "    train, _ = selected_features_extractor_separate(train_data_path, None)\n",
    "    \n",
    "    # Remove any rows with NaN values\n",
    "    train = train.dropna()\n",
    "    \n",
    "    # Normalize training data\n",
    "    print('----->Normalization....')\n",
    "    norm_train = normalize_train(train)\n",
    "    \n",
    "    # Calculate normalization parameters for later use\n",
    "    norm_mean = train[selected_cols].mean()\n",
    "    norm_std = train[selected_cols].std() + normalization_std_reg\n",
    "    \n",
    "    # Validate model (using simpler parameters for validation)\n",
    "    print('----->Validation with ElasticNetCV....')\n",
    "    simple_params = {\n",
    "        'l1_ratio': 0.5,  # Use single value for validation\n",
    "        'cv': 5,\n",
    "        'max_iter': 10000,\n",
    "        'random_state': 42\n",
    "    }\n",
    "    print(validate_model_by_pentate(ElasticNetCV, simple_params, norm_train, selected_cols, 0))\n",
    "    \n",
    "    # Fit final model with full parameter search\n",
    "    print('----->Fitting ElasticNetCV model with cross-validation....')\n",
    "    print(f'Testing l1_ratios: {elasticnet_params[\"l1_ratio\"]}')\n",
    "    print(f'Testing {len(elasticnet_params[\"alphas\"])} alpha values from {elasticnet_params[\"alphas\"].min():.6f} to {elasticnet_params[\"alphas\"].max():.6f}')\n",
    "    print(f'Using {elasticnet_params[\"cv\"]}-fold cross-validation')\n",
    "    \n",
    "    model = ElasticNetCV(\n",
    "        l1_ratio=elasticnet_params['l1_ratio'],\n",
    "        alphas=elasticnet_params['alphas'],\n",
    "        cv=elasticnet_params['cv'],\n",
    "        max_iter=elasticnet_params['max_iter'],\n",
    "        tol=elasticnet_params['tol'],\n",
    "        random_state=elasticnet_params['random_state'],\n",
    "        selection=elasticnet_params['selection'],\n",
    "        fit_intercept=elasticnet_params['fit_intercept']\n",
    "    )\n",
    "    \n",
    "    # Fit the model\n",
    "    model.fit(norm_train[selected_cols], norm_train.returns)\n",
    "    \n",
    "    # Print selected hyperparameters\n",
    "    print(f'\\n----->Optimal hyperparameters found:')\n",
    "    print(f'Best alpha (regularization strength): {model.alpha_:.6f}')\n",
    "    print(f'Best l1_ratio (L1 vs L2 mix): {model.l1_ratio_:.3f}')\n",
    "    print(f'  - L1 penalty weight: {model.l1_ratio_:.1%}')\n",
    "    print(f'  - L2 penalty weight: {(1-model.l1_ratio_):.1%}')\n",
    "    \n",
    "    # Print feature importances\n",
    "    print('\\n----->Calculating feature importances (ElasticNet)...')\n",
    "    print_importances_elasticnet(model, selected_cols)\n",
    "    \n",
    "    # Return model and parameters\n",
    "    return {\n",
    "        'model': model,\n",
    "        'norm_mean': norm_mean,\n",
    "        'norm_std': norm_std,\n",
    "        'elasticnet_params': elasticnet_params,\n",
    "        'selected_alpha': model.alpha_,\n",
    "        'selected_l1_ratio': model.l1_ratio_\n",
    "    }\n",
    "\n",
    "# %% Model Forecast Function\n",
    "def modelForecast(test_data_path, model_params, train_data_path=None):\n",
    "    \"\"\"\n",
    "    Make predictions on test data using the trained ElasticNetCV model.\n",
    "    \n",
    "    Parameters:\n",
    "    test_data_path (str): Path to the test CSV file\n",
    "    model_params (dict): Dictionary containing the trained model and normalization parameters\n",
    "    train_data_path (str): Not used in this version (features computed separately)\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary containing predictions and evaluation metrics (if test data has returns)\n",
    "    \"\"\"\n",
    "    print('--------->Starting Forecasting with ElasticNetCV....')\n",
    "    \n",
    "    # Extract features for test only (no train contamination)\n",
    "    _, test = selected_features_extractor_separate(None, test_data_path)\n",
    "    \n",
    "    # Remove any rows with NaN values\n",
    "    test = test.dropna()\n",
    "    \n",
    "    # Normalize test data using training statistics\n",
    "    print('----->Normalization....')\n",
    "    test_extended_cols = selected_cols + ['periods_before_closing']\n",
    "    \n",
    "    # Check if test data has returns column for evaluation\n",
    "    has_returns = 'returns' in test.columns\n",
    "    if has_returns:\n",
    "        test_extended_cols.append('returns')\n",
    "    \n",
    "    norm_test = test[test_extended_cols].reset_index(drop=True).copy()\n",
    "    norm_test.loc[:,selected_cols] = (norm_test[selected_cols] - model_params['norm_mean']) / model_params['norm_std']\n",
    "    \n",
    "    # Make predictions\n",
    "    print('----->Prediction with ElasticNetCV....')\n",
    "    predicted = model_params['model'].predict(norm_test[selected_cols])\n",
    "    \n",
    "    # Set predictions to 0 at closing periods\n",
    "    predicted[norm_test.periods_before_closing == 0] = 0\n",
    "    \n",
    "    # Calculate evaluation metrics if test data has returns\n",
    "    results = {'predictions': predicted}\n",
    "    \n",
    "    if has_returns:\n",
    "        print('----->Evaluation....')\n",
    "        # Calculate MSE\n",
    "        mse = mean_squared_error(norm_test.returns, predicted)\n",
    "        # Calculate R-squared\n",
    "        r2 = rsquared(norm_test.returns, predicted)\n",
    "        \n",
    "        results['mse'] = mse\n",
    "        results['r2'] = r2\n",
    "        \n",
    "        print(f'Test MSE: {mse:.6f}')\n",
    "        print(f'Test R-squared: {r2:.6f}')\n",
    "        print(f'Test R-squared (x100): {r2*100:.2f}')\n",
    "    else:\n",
    "        print('No returns column in test data - evaluation metrics not calculated')\n",
    "    \n",
    "    return results\n",
    "\n",
    "# %% Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Train the model\n",
    "    train_file_path = '/Users/mazin/Desktop/super prep materials/GSAPred/Two-financial-instruments/train.csv'  # Replace with your actual file path\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"Training ElasticNetCV Model for Financial Time Series Prediction\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    model_params = modelEstimate(train_file_path)\n",
    "    \n",
    "    # Save model for later use\n",
    "    with open('elasticnet_model_params.pkl', 'wb') as f:\n",
    "        pickle.dump(model_params, f)\n",
    "    print(\"\\nModel saved to elasticnet_model_params.pkl\")\n",
    "    \n",
    "    # Make predictions on test data\n",
    "    test_file_path = '/Users/mazin/Desktop/super prep materials/GSAPred/Two-financial-instruments/test.csv'  # Replace with your actual file path\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Making Predictions on Test Data\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Load model if needed\n",
    "    # with open('elasticnet_model_params.pkl', 'rb') as f:\n",
    "    #     model_params = pickle.load(f)\n",
    "    \n",
    "    results = modelForecast(test_file_path, model_params)\n",
    "    \n",
    "    # Extract predictions\n",
    "    predictions = results['predictions']\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\nFirst 10 predictions: {predictions[:10]}\")\n",
    "    print(f\"Total predictions: {len(predictions)}\")\n",
    "    \n",
    "    # Display evaluation metrics if available\n",
    "    if 'mse' in results:\n",
    "        print(f\"\\nEvaluation Metrics:\")\n",
    "        print(f\"MSE: {results['mse']:.6f}\")\n",
    "        print(f\"R-squared: {results['r2']:.6f}\")\n",
    "        print(f\"R-squared (x100): {results['r2']*100:.2f}\")\n",
    "    \n",
    "    # Save predictions to CSV\n",
    "    predictions_df = pd.DataFrame({'predictions': predictions})\n",
    "    predictions_df.to_csv('elasticnet_predictions.csv', index=False)\n",
    "    print(\"\\nPredictions saved to elasticnet_predictions.csv\")\n",
    "    \n",
    "    # Additional analysis: Show sparsity and feature selection\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Model Sparsity Analysis:\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    coefficients = model_params['model'].coef_\n",
    "    non_zero_coefs = np.sum(np.abs(coefficients) > 1e-10)\n",
    "    sparsity = 1 - (non_zero_coefs / len(coefficients))\n",
    "    \n",
    "    print(f\"Total features: {len(coefficients)}\")\n",
    "    print(f\"Non-zero coefficients: {non_zero_coefs}\")\n",
    "    print(f\"Zero coefficients: {len(coefficients) - non_zero_coefs}\")\n",
    "    print(f\"Sparsity: {sparsity:.1%}\")\n",
    "    print(f\"\\nSelected alpha: {model_params['selected_alpha']:.6f}\")\n",
    "    print(f\"Selected l1_ratio: {model_params['selected_l1_ratio']:.3f}\")\n",
    "    \n",
    "    # Show top 10 most important features\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Top 10 Most Important Features (by absolute coefficient):\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    feature_importance = list(zip(selected_cols, coefficients))\n",
    "    sorted_features = sorted(feature_importance, key=lambda x: -abs(x[1]))[:10]\n",
    "    \n",
    "    for i, (name, coef) in enumerate(sorted_features, 1):\n",
    "        print(f\"{i:2}. {name:40} {coef:15.6f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ElasticNetCV Model Training and Prediction Complete!\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb326e45-fc02-4328-b089-cb15fc5314a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
